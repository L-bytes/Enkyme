{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\marle\\anaconda3\\envs\\py37\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os.path import join\n",
    "import os\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import MACCSkeys\n",
    "from ete3 import NCBITaxa\n",
    "import random\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
    "random.seed(10)\n",
    "import torch\n",
    "import esm\n",
    "from bioservices import *\n",
    "from data_preprocessing import *\n",
    "from functions_and_dicts_data_preprocessing_GNN import *\n",
    "from build_GNN import *\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "datasets_dir = \"../../data\"\n",
    "\n",
    "CURRENT_DIR = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading in Sabio data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Sabio data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data points: 3550\n",
      "Number of UniProt IDs: 833\n"
     ]
    }
   ],
   "source": [
    "organism = \"Seed plants\"\n",
    "\n",
    "df_Sabio = pd.read_table(join(\"..\", \"..\", \"data\", \"Km_model_\" + organism + \".tsv\"))\n",
    "\n",
    "df_Sabio[\"Km\"] = df_Sabio[\"Km\"].astype('float')\n",
    "df_Sabio[\"PMID\"] = df_Sabio[\"PMID\"].astype('Int64')\n",
    "\n",
    "df_Sabio[\"substrate_IDs\"] = df_Sabio[\"substrate_IDs\"].str.split('#')\n",
    "df_Sabio[\"product_IDs\"] = df_Sabio[\"product_IDs\"].str.split('#')\n",
    "\n",
    "print(\"Number of data points: %s\" % len(df_Sabio))\n",
    "print(\"Number of UniProt IDs: %s\" % len(set(df_Sabio[\"Uniprot IDs\"])))\n",
    "\n",
    "df_Km = df_Sabio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "droplist = []\n",
    "\n",
    "for ind in df_Km.index:\n",
    "    UID, Km = df_Km[\"Uniprot IDs\"][ind], df_Km[\"Km\"][ind]\n",
    "    help_df = df_Km.loc[df_Km[\"Uniprot IDs\"] == UID].loc[df_Km[\"Km\"] == Km]\n",
    "    \n",
    "    if len(help_df) > 1:\n",
    "        droplist = droplist + list(help_df.index)[1:]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Km.drop(list(set(droplist)), inplace = True)\n",
    "print(\"Dropping %s data points, because they are duplicated.\" % len(set(droplist)))\n",
    "df_Km.reset_index(inplace = True, drop = True)\n",
    "df_Km"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing top and bottom 3% of Km values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_outliers_IQR(df):\n",
    "\n",
    "   q1=df.quantile(0.25)\n",
    "\n",
    "   q3=df.quantile(0.75)\n",
    "\n",
    "   IQR=q3-q1\n",
    "\n",
    "   outliers = df[((df<(q1-1.5*IQR)) | (df>(q3+1.5*IQR)))]\n",
    "\n",
    "   return outliers\n",
    "\n",
    "find_outliers_IQR(df_Km[\"Km\"])\n",
    "\n",
    "print(df_Km['Km'].quantile(0.03),  df_Km['Km'].quantile(0.97))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Km = df_Km[(df_Km['Km'] > df_Km['Km'].quantile(0.03)) & (df_Km['Km'] < df_Km['Km'].quantile(0.97))]\n",
    "df_Km.reset_index(inplace = True, drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "todrop= []\n",
    "\n",
    "for ind in df_Km.index:\n",
    "    UID = df_Km[\"Uniprot IDs\"][ind]\n",
    "    if len(UID.split(';')) > 1:\n",
    "        todrop.append(ind)\n",
    "        print(df_Km[\"Uniprot IDs\"][ind])\n",
    "        print(todrop)\n",
    "        \n",
    "df_Km.drop(todrop, inplace=True)\n",
    "df_Km.reset_index(inplace = True, drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Km[\"substrate_IDs\"] = df_Km[\"substrate_IDs\"].apply(lambda x: (set(x)))\n",
    "df_Km[\"product_IDs\"] = df_Km[\"product_IDs\"].apply(lambda x: (set(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_Km.to_pickle(join(\"..\", \"..\", \"data\", \"Km_data_merged.pkl\"))\n",
    "df_Km = pd.read_pickle(join(\"..\", \"..\", \"data\", \"Km_data_merged.pkl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Assigning IDs to every unique sequence and to every unique reaction in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating DataFrames for all sequences and for all reactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reactions = pd.DataFrame({\"substrates\": df_Km[\"substrate_IDs\"],\n",
    "                            \"products\" : df_Km[\"product_IDs\"]})\n",
    "\n",
    "df_reactions = df_reactions.loc[df_reactions[\"substrates\"] != set([])]\n",
    "df_reactions = df_reactions.loc[df_reactions[\"products\"] != set([])]\n",
    "\n",
    "\n",
    "droplist = []\n",
    "for ind in df_reactions.index:\n",
    "    sub_IDs, pro_IDs = df_reactions[\"substrates\"][ind], df_reactions[\"products\"][ind]\n",
    "    help_df = df_reactions.loc[df_reactions[\"substrates\"] == sub_IDs].loc[df_reactions[\"products\"] == pro_IDs]\n",
    "    if len(help_df):\n",
    "        for ind in list(help_df.index)[1:]:\n",
    "            droplist.append(ind)\n",
    "            \n",
    "df_reactions.drop(list(set(droplist)), inplace = True)\n",
    "df_reactions.reset_index(inplace = True, drop =True)\n",
    "\n",
    "df_reactions[\"Reaction ID\"] = [\"Reaction_\" + str(ind) for ind in df_reactions.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sequence</th>\n",
       "      <th>Sequence ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MTTGKGKILILGATGYLGKYMVKASISLGHPTYAYVMPLKKNSDDS...</td>\n",
       "      <td>Sequence_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MEENGMKSKILIFGGTGYIGNHMVKGSLKLGHPTYVFTRPNSSKTT...</td>\n",
       "      <td>Sequence_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MGKGGNSEDAVSGKEHGEENMAAWLLGIKTLKIQPYILPSLGPYDV...</td>\n",
       "      <td>Sequence_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MANLRESSRDKSRWSLEGMTALVTGGSKGIGEAVVEELAMLGARVH...</td>\n",
       "      <td>Sequence_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MAKEGGLGENSRWSLGGMTALVTGGSKGIGEAVVEELAMLGAKVHT...</td>\n",
       "      <td>Sequence_4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>977</th>\n",
       "      <td>MSSLEDIKNETVDLEKIPIEEVFQQLKCSREGLTTQEGEDRIQIFG...</td>\n",
       "      <td>Sequence_977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>978</th>\n",
       "      <td>MNARALLCSSNIHSLYTSNRPPEKTSSSRSLRNLKPSPKSLRVWIY...</td>\n",
       "      <td>Sequence_978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>979</th>\n",
       "      <td>MKSFNTEGHNHSTAESGDAYTVSDPTKNVDEDGREKRTGTWLTASA...</td>\n",
       "      <td>Sequence_979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>980</th>\n",
       "      <td>MDAYNNPSAVESGDAAVKSVDDDGREKRTGTFWTASAHIITAVIGS...</td>\n",
       "      <td>Sequence_980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981</th>\n",
       "      <td>MEKKKSMFVEQSFPEHEIGDTNKNFDEDGRDKRTGTWMTGSAHIIT...</td>\n",
       "      <td>Sequence_981</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>982 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Sequence   Sequence ID\n",
       "0    MTTGKGKILILGATGYLGKYMVKASISLGHPTYAYVMPLKKNSDDS...    Sequence_0\n",
       "1    MEENGMKSKILIFGGTGYIGNHMVKGSLKLGHPTYVFTRPNSSKTT...    Sequence_1\n",
       "2    MGKGGNSEDAVSGKEHGEENMAAWLLGIKTLKIQPYILPSLGPYDV...    Sequence_2\n",
       "3    MANLRESSRDKSRWSLEGMTALVTGGSKGIGEAVVEELAMLGARVH...    Sequence_3\n",
       "4    MAKEGGLGENSRWSLGGMTALVTGGSKGIGEAVVEELAMLGAKVHT...    Sequence_4\n",
       "..                                                 ...           ...\n",
       "977  MSSLEDIKNETVDLEKIPIEEVFQQLKCSREGLTTQEGEDRIQIFG...  Sequence_977\n",
       "978  MNARALLCSSNIHSLYTSNRPPEKTSSSRSLRNLKPSPKSLRVWIY...  Sequence_978\n",
       "979  MKSFNTEGHNHSTAESGDAYTVSDPTKNVDEDGREKRTGTWLTASA...  Sequence_979\n",
       "980  MDAYNNPSAVESGDAAVKSVDDDGREKRTGTFWTASAHIITAVIGS...  Sequence_980\n",
       "981  MEKKKSMFVEQSFPEHEIGDTNKNFDEDGRDKRTGTWMTGSAHIIT...  Sequence_981\n",
       "\n",
       "[982 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sequences = pd.DataFrame(data = {\"Sequence\" : df_Km[\"Sequence\"].unique()})\n",
    "df_sequences = df_sequences.loc[~pd.isnull(df_sequences[\"Sequence\"])]\n",
    "df_sequences.reset_index(inplace = True, drop = True)\n",
    "df_sequences[\"Sequence ID\"] = [\"Sequence_\" + str(ind) for ind in df_sequences.index]\n",
    "\n",
    "df_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating minimal Km value for each reaction and sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reactions[\"min_Km_for_RID\"] = np.nan\n",
    "for ind in df_reactions.index:\n",
    "    df_reactions[\"min_Km_for_RID\"][ind] = min(df_Km.loc[df_Km[\"substrate_IDs\"] == df_reactions[\"substrates\"][ind]].loc[df_Km[\"product_IDs\"] == df_reactions[\"products\"][ind]][\"Km\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sequences[\"min_Km_for_UID\"] = np.nan\n",
    "for ind in df_sequences.index:\n",
    "    df_sequences[\"min_Km_for_UID\"][ind] = min(df_Km.loc[df_Km[\"Sequence\"] == df_sequences['Sequence'][ind]][\"Km\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating the sum of the molecular weights of all substrates and of all products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reactions[\"MW_frac\"] = np.nan\n",
    "\n",
    "for ind in df_reactions.index:\n",
    "    substrates = list(df_reactions[\"substrates\"][ind])\n",
    "    products = list(df_reactions[\"products\"][ind])\n",
    "    \n",
    "    mw_subs = mw_mets(metabolites = substrates)\n",
    "    mw_pros = mw_mets(metabolites = products)\n",
    "    \n",
    "    if mw_subs == np.nan or mw_pros == np.nan:\n",
    "        df_reactions[\"MW_frac\"][ind] = np.inf\n",
    "    if mw_pros != 0:\n",
    "        df_reactions[\"MW_frac\"][ind] = mw_subs/mw_pros\n",
    "    else:\n",
    "        df_reactions[\"MW_frac\"][ind] = np.inf\n",
    "        \n",
    "df_reactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating enzyme, reaction and substrate features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\marle/.cache\\torch\\hub\\facebookresearch_esm_main\n"
     ]
    }
   ],
   "source": [
    "model, alphabet = torch.hub.load(\"facebookresearch/esm:main\", \"esm2_t33_650M_UR50D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 982\n",
      "1 / 982\n",
      "2 / 982\n",
      "3 / 982\n",
      "4 / 982\n",
      "5 / 982\n",
      "6 / 982\n",
      "7 / 982\n",
      "8 / 982\n",
      "9 / 982\n",
      "10 / 982\n",
      "11 / 982\n",
      "12 / 982\n",
      "13 / 982\n",
      "14 / 982\n",
      "15 / 982\n",
      "16 / 982\n",
      "17 / 982\n",
      "18 / 982\n",
      "19 / 982\n",
      "20 / 982\n",
      "21 / 982\n",
      "22 / 982\n",
      "23 / 982\n",
      "24 / 982\n",
      "25 / 982\n",
      "26 / 982\n",
      "27 / 982\n",
      "28 / 982\n",
      "29 / 982\n",
      "30 / 982\n",
      "31 / 982\n",
      "32 / 982\n",
      "33 / 982\n",
      "34 / 982\n",
      "35 / 982\n",
      "36 / 982\n",
      "37 / 982\n",
      "38 / 982\n",
      "39 / 982\n",
      "40 / 982\n",
      "41 / 982\n",
      "42 / 982\n",
      "43 / 982\n",
      "44 / 982\n",
      "45 / 982\n",
      "46 / 982\n",
      "47 / 982\n",
      "48 / 982\n",
      "49 / 982\n",
      "50 / 982\n",
      "51 / 982\n",
      "52 / 982\n",
      "53 / 982\n",
      "54 / 982\n",
      "55 / 982\n",
      "56 / 982\n",
      "57 / 982\n",
      "58 / 982\n",
      "59 / 982\n",
      "60 / 982\n",
      "61 / 982\n",
      "62 / 982\n",
      "63 / 982\n",
      "64 / 982\n",
      "65 / 982\n",
      "66 / 982\n",
      "67 / 982\n",
      "68 / 982\n",
      "69 / 982\n",
      "70 / 982\n",
      "71 / 982\n",
      "72 / 982\n",
      "73 / 982\n",
      "74 / 982\n",
      "75 / 982\n",
      "76 / 982\n",
      "77 / 982\n",
      "78 / 982\n",
      "79 / 982\n",
      "80 / 982\n",
      "81 / 982\n",
      "82 / 982\n",
      "83 / 982\n",
      "84 / 982\n",
      "85 / 982\n",
      "86 / 982\n",
      "87 / 982\n",
      "88 / 982\n",
      "89 / 982\n",
      "90 / 982\n",
      "91 / 982\n",
      "92 / 982\n",
      "93 / 982\n",
      "94 / 982\n",
      "95 / 982\n",
      "96 / 982\n",
      "97 / 982\n",
      "98 / 982\n",
      "99 / 982\n",
      "100 / 982\n",
      "101 / 982\n",
      "102 / 982\n",
      "103 / 982\n",
      "104 / 982\n",
      "105 / 982\n",
      "106 / 982\n",
      "107 / 982\n",
      "108 / 982\n",
      "109 / 982\n",
      "110 / 982\n",
      "111 / 982\n",
      "112 / 982\n",
      "113 / 982\n",
      "114 / 982\n",
      "115 / 982\n",
      "116 / 982\n",
      "117 / 982\n",
      "118 / 982\n",
      "119 / 982\n",
      "120 / 982\n",
      "121 / 982\n",
      "122 / 982\n",
      "123 / 982\n",
      "124 / 982\n",
      "125 / 982\n",
      "126 / 982\n",
      "127 / 982\n",
      "128 / 982\n",
      "129 / 982\n",
      "130 / 982\n",
      "131 / 982\n",
      "132 / 982\n",
      "133 / 982\n",
      "134 / 982\n",
      "135 / 982\n",
      "136 / 982\n",
      "137 / 982\n",
      "138 / 982\n",
      "139 / 982\n",
      "140 / 982\n",
      "141 / 982\n",
      "142 / 982\n",
      "143 / 982\n",
      "144 / 982\n",
      "145 / 982\n",
      "146 / 982\n",
      "147 / 982\n",
      "148 / 982\n",
      "149 / 982\n",
      "150 / 982\n",
      "151 / 982\n",
      "152 / 982\n",
      "153 / 982\n",
      "154 / 982\n",
      "155 / 982\n",
      "156 / 982\n",
      "157 / 982\n",
      "158 / 982\n",
      "159 / 982\n",
      "160 / 982\n",
      "161 / 982\n",
      "162 / 982\n",
      "163 / 982\n",
      "164 / 982\n",
      "165 / 982\n",
      "166 / 982\n",
      "167 / 982\n",
      "168 / 982\n",
      "169 / 982\n",
      "170 / 982\n",
      "171 / 982\n",
      "172 / 982\n",
      "173 / 982\n",
      "174 / 982\n",
      "175 / 982\n",
      "176 / 982\n",
      "177 / 982\n",
      "178 / 982\n",
      "179 / 982\n",
      "180 / 982\n",
      "181 / 982\n",
      "182 / 982\n",
      "183 / 982\n",
      "184 / 982\n",
      "185 / 982\n",
      "186 / 982\n",
      "187 / 982\n",
      "188 / 982\n",
      "189 / 982\n",
      "190 / 982\n",
      "191 / 982\n",
      "192 / 982\n",
      "193 / 982\n",
      "194 / 982\n",
      "195 / 982\n",
      "196 / 982\n",
      "197 / 982\n",
      "198 / 982\n",
      "199 / 982\n",
      "200 / 982\n",
      "201 / 982\n",
      "202 / 982\n",
      "203 / 982\n",
      "204 / 982\n",
      "205 / 982\n",
      "206 / 982\n",
      "207 / 982\n",
      "208 / 982\n",
      "209 / 982\n",
      "210 / 982\n",
      "211 / 982\n",
      "212 / 982\n",
      "213 / 982\n",
      "214 / 982\n",
      "215 / 982\n",
      "216 / 982\n",
      "217 / 982\n",
      "218 / 982\n",
      "219 / 982\n",
      "220 / 982\n",
      "221 / 982\n",
      "222 / 982\n",
      "223 / 982\n",
      "224 / 982\n",
      "225 / 982\n",
      "226 / 982\n",
      "227 / 982\n",
      "228 / 982\n",
      "229 / 982\n",
      "230 / 982\n",
      "231 / 982\n",
      "232 / 982\n",
      "233 / 982\n",
      "234 / 982\n",
      "235 / 982\n",
      "236 / 982\n",
      "237 / 982\n",
      "238 / 982\n",
      "239 / 982\n",
      "240 / 982\n",
      "241 / 982\n",
      "242 / 982\n",
      "243 / 982\n",
      "244 / 982\n",
      "245 / 982\n",
      "246 / 982\n",
      "247 / 982\n",
      "248 / 982\n",
      "249 / 982\n",
      "250 / 982\n",
      "251 / 982\n",
      "252 / 982\n",
      "253 / 982\n",
      "254 / 982\n",
      "255 / 982\n",
      "256 / 982\n",
      "257 / 982\n",
      "258 / 982\n",
      "259 / 982\n",
      "260 / 982\n",
      "261 / 982\n",
      "262 / 982\n",
      "263 / 982\n",
      "264 / 982\n",
      "265 / 982\n",
      "266 / 982\n",
      "267 / 982\n",
      "268 / 982\n",
      "269 / 982\n",
      "270 / 982\n",
      "271 / 982\n",
      "272 / 982\n",
      "273 / 982\n",
      "274 / 982\n",
      "275 / 982\n",
      "276 / 982\n",
      "277 / 982\n",
      "278 / 982\n",
      "279 / 982\n",
      "280 / 982\n",
      "281 / 982\n",
      "282 / 982\n",
      "283 / 982\n",
      "284 / 982\n",
      "285 / 982\n",
      "286 / 982\n",
      "287 / 982\n",
      "288 / 982\n",
      "289 / 982\n",
      "290 / 982\n",
      "291 / 982\n",
      "292 / 982\n",
      "293 / 982\n",
      "294 / 982\n",
      "295 / 982\n",
      "296 / 982\n",
      "297 / 982\n",
      "298 / 982\n",
      "299 / 982\n",
      "300 / 982\n",
      "301 / 982\n",
      "302 / 982\n",
      "303 / 982\n",
      "304 / 982\n",
      "305 / 982\n",
      "306 / 982\n",
      "307 / 982\n",
      "308 / 982\n",
      "309 / 982\n",
      "310 / 982\n",
      "311 / 982\n",
      "312 / 982\n",
      "313 / 982\n",
      "314 / 982\n",
      "315 / 982\n",
      "316 / 982\n",
      "317 / 982\n",
      "318 / 982\n",
      "319 / 982\n",
      "320 / 982\n",
      "321 / 982\n",
      "322 / 982\n",
      "323 / 982\n",
      "324 / 982\n",
      "325 / 982\n",
      "326 / 982\n",
      "327 / 982\n",
      "328 / 982\n",
      "329 / 982\n",
      "330 / 982\n",
      "331 / 982\n",
      "332 / 982\n",
      "333 / 982\n",
      "334 / 982\n",
      "335 / 982\n",
      "336 / 982\n",
      "337 / 982\n",
      "338 / 982\n",
      "339 / 982\n",
      "340 / 982\n",
      "341 / 982\n",
      "342 / 982\n",
      "343 / 982\n",
      "344 / 982\n",
      "345 / 982\n",
      "346 / 982\n",
      "347 / 982\n",
      "348 / 982\n",
      "349 / 982\n",
      "350 / 982\n",
      "351 / 982\n",
      "352 / 982\n",
      "353 / 982\n",
      "354 / 982\n",
      "355 / 982\n",
      "356 / 982\n",
      "357 / 982\n",
      "358 / 982\n",
      "359 / 982\n",
      "360 / 982\n",
      "361 / 982\n",
      "362 / 982\n",
      "363 / 982\n",
      "364 / 982\n",
      "365 / 982\n",
      "366 / 982\n",
      "367 / 982\n",
      "368 / 982\n",
      "369 / 982\n",
      "370 / 982\n",
      "371 / 982\n",
      "372 / 982\n",
      "373 / 982\n",
      "374 / 982\n",
      "375 / 982\n",
      "376 / 982\n",
      "377 / 982\n",
      "378 / 982\n",
      "379 / 982\n",
      "380 / 982\n",
      "381 / 982\n",
      "382 / 982\n",
      "383 / 982\n",
      "384 / 982\n",
      "385 / 982\n",
      "386 / 982\n",
      "387 / 982\n",
      "388 / 982\n",
      "389 / 982\n",
      "390 / 982\n",
      "391 / 982\n",
      "392 / 982\n",
      "393 / 982\n",
      "394 / 982\n",
      "395 / 982\n",
      "396 / 982\n",
      "397 / 982\n",
      "398 / 982\n",
      "399 / 982\n",
      "400 / 982\n",
      "401 / 982\n",
      "402 / 982\n",
      "403 / 982\n",
      "404 / 982\n",
      "405 / 982\n",
      "406 / 982\n",
      "407 / 982\n",
      "408 / 982\n",
      "409 / 982\n",
      "410 / 982\n",
      "411 / 982\n",
      "412 / 982\n",
      "413 / 982\n",
      "414 / 982\n",
      "415 / 982\n",
      "416 / 982\n",
      "417 / 982\n",
      "418 / 982\n",
      "419 / 982\n",
      "420 / 982\n",
      "421 / 982\n",
      "422 / 982\n",
      "423 / 982\n",
      "424 / 982\n",
      "425 / 982\n",
      "426 / 982\n",
      "427 / 982\n",
      "428 / 982\n",
      "429 / 982\n",
      "430 / 982\n",
      "431 / 982\n",
      "432 / 982\n",
      "433 / 982\n",
      "434 / 982\n",
      "435 / 982\n",
      "436 / 982\n",
      "437 / 982\n",
      "438 / 982\n",
      "439 / 982\n",
      "440 / 982\n",
      "441 / 982\n",
      "442 / 982\n",
      "443 / 982\n",
      "444 / 982\n",
      "445 / 982\n",
      "446 / 982\n",
      "447 / 982\n",
      "448 / 982\n",
      "449 / 982\n",
      "450 / 982\n",
      "451 / 982\n",
      "452 / 982\n",
      "453 / 982\n",
      "454 / 982\n",
      "455 / 982\n",
      "456 / 982\n",
      "457 / 982\n",
      "458 / 982\n",
      "459 / 982\n",
      "460 / 982\n",
      "461 / 982\n",
      "462 / 982\n",
      "463 / 982\n",
      "464 / 982\n",
      "465 / 982\n",
      "466 / 982\n",
      "467 / 982\n",
      "468 / 982\n",
      "469 / 982\n",
      "470 / 982\n",
      "471 / 982\n",
      "472 / 982\n",
      "473 / 982\n",
      "474 / 982\n",
      "475 / 982\n",
      "476 / 982\n",
      "477 / 982\n",
      "478 / 982\n",
      "479 / 982\n",
      "480 / 982\n",
      "481 / 982\n",
      "482 / 982\n",
      "483 / 982\n",
      "484 / 982\n",
      "485 / 982\n",
      "486 / 982\n",
      "487 / 982\n",
      "488 / 982\n",
      "489 / 982\n",
      "490 / 982\n",
      "491 / 982\n",
      "492 / 982\n",
      "493 / 982\n",
      "494 / 982\n",
      "495 / 982\n",
      "496 / 982\n",
      "497 / 982\n",
      "498 / 982\n",
      "499 / 982\n",
      "500 / 982\n",
      "501 / 982\n",
      "502 / 982\n",
      "503 / 982\n",
      "504 / 982\n",
      "505 / 982\n",
      "506 / 982\n",
      "507 / 982\n",
      "508 / 982\n",
      "509 / 982\n",
      "510 / 982\n",
      "511 / 982\n",
      "512 / 982\n",
      "513 / 982\n",
      "514 / 982\n",
      "515 / 982\n",
      "516 / 982\n",
      "517 / 982\n",
      "518 / 982\n",
      "519 / 982\n",
      "520 / 982\n",
      "521 / 982\n",
      "522 / 982\n",
      "523 / 982\n",
      "524 / 982\n",
      "525 / 982\n",
      "526 / 982\n",
      "527 / 982\n",
      "528 / 982\n",
      "529 / 982\n",
      "530 / 982\n",
      "531 / 982\n",
      "532 / 982\n",
      "533 / 982\n",
      "534 / 982\n",
      "535 / 982\n",
      "536 / 982\n",
      "537 / 982\n",
      "538 / 982\n",
      "539 / 982\n",
      "540 / 982\n",
      "541 / 982\n",
      "542 / 982\n",
      "543 / 982\n",
      "544 / 982\n",
      "545 / 982\n",
      "546 / 982\n",
      "547 / 982\n",
      "548 / 982\n",
      "549 / 982\n",
      "550 / 982\n",
      "551 / 982\n",
      "552 / 982\n",
      "553 / 982\n",
      "554 / 982\n",
      "555 / 982\n",
      "556 / 982\n",
      "557 / 982\n",
      "558 / 982\n",
      "559 / 982\n",
      "560 / 982\n",
      "561 / 982\n",
      "562 / 982\n",
      "563 / 982\n",
      "564 / 982\n",
      "565 / 982\n",
      "566 / 982\n",
      "567 / 982\n",
      "568 / 982\n",
      "569 / 982\n",
      "570 / 982\n",
      "571 / 982\n",
      "572 / 982\n",
      "573 / 982\n",
      "574 / 982\n",
      "575 / 982\n",
      "576 / 982\n",
      "577 / 982\n",
      "578 / 982\n",
      "579 / 982\n",
      "580 / 982\n",
      "581 / 982\n",
      "582 / 982\n",
      "583 / 982\n",
      "584 / 982\n",
      "585 / 982\n",
      "586 / 982\n",
      "587 / 982\n",
      "588 / 982\n",
      "589 / 982\n",
      "590 / 982\n",
      "591 / 982\n",
      "592 / 982\n",
      "593 / 982\n",
      "594 / 982\n",
      "595 / 982\n",
      "596 / 982\n",
      "597 / 982\n",
      "598 / 982\n",
      "599 / 982\n",
      "600 / 982\n",
      "601 / 982\n",
      "602 / 982\n",
      "603 / 982\n",
      "604 / 982\n",
      "605 / 982\n",
      "606 / 982\n",
      "607 / 982\n",
      "608 / 982\n",
      "609 / 982\n",
      "610 / 982\n",
      "611 / 982\n",
      "612 / 982\n",
      "613 / 982\n",
      "614 / 982\n",
      "615 / 982\n",
      "616 / 982\n",
      "617 / 982\n",
      "618 / 982\n",
      "619 / 982\n",
      "620 / 982\n",
      "621 / 982\n",
      "622 / 982\n",
      "623 / 982\n",
      "624 / 982\n",
      "625 / 982\n",
      "626 / 982\n",
      "627 / 982\n",
      "628 / 982\n",
      "629 / 982\n",
      "630 / 982\n",
      "631 / 982\n",
      "632 / 982\n",
      "633 / 982\n",
      "634 / 982\n",
      "635 / 982\n",
      "636 / 982\n",
      "637 / 982\n",
      "638 / 982\n",
      "639 / 982\n",
      "640 / 982\n",
      "641 / 982\n",
      "642 / 982\n",
      "643 / 982\n",
      "644 / 982\n",
      "645 / 982\n",
      "646 / 982\n",
      "647 / 982\n",
      "648 / 982\n",
      "649 / 982\n",
      "650 / 982\n",
      "651 / 982\n",
      "652 / 982\n",
      "653 / 982\n",
      "654 / 982\n",
      "655 / 982\n",
      "656 / 982\n",
      "657 / 982\n",
      "658 / 982\n",
      "659 / 982\n",
      "660 / 982\n",
      "661 / 982\n",
      "662 / 982\n",
      "663 / 982\n",
      "664 / 982\n",
      "665 / 982\n",
      "666 / 982\n",
      "667 / 982\n",
      "668 / 982\n",
      "669 / 982\n",
      "670 / 982\n",
      "671 / 982\n",
      "672 / 982\n",
      "673 / 982\n",
      "674 / 982\n",
      "675 / 982\n",
      "676 / 982\n",
      "677 / 982\n",
      "678 / 982\n",
      "679 / 982\n",
      "680 / 982\n",
      "681 / 982\n",
      "682 / 982\n",
      "683 / 982\n",
      "684 / 982\n",
      "685 / 982\n",
      "686 / 982\n",
      "687 / 982\n",
      "688 / 982\n",
      "689 / 982\n",
      "690 / 982\n",
      "691 / 982\n",
      "692 / 982\n",
      "693 / 982\n",
      "694 / 982\n",
      "695 / 982\n",
      "696 / 982\n",
      "697 / 982\n",
      "698 / 982\n",
      "699 / 982\n",
      "700 / 982\n",
      "701 / 982\n",
      "702 / 982\n",
      "703 / 982\n",
      "704 / 982\n",
      "705 / 982\n",
      "706 / 982\n",
      "707 / 982\n",
      "708 / 982\n",
      "709 / 982\n",
      "710 / 982\n",
      "711 / 982\n",
      "712 / 982\n",
      "713 / 982\n",
      "714 / 982\n",
      "715 / 982\n",
      "716 / 982\n",
      "717 / 982\n",
      "718 / 982\n",
      "719 / 982\n",
      "720 / 982\n",
      "721 / 982\n",
      "722 / 982\n",
      "723 / 982\n",
      "724 / 982\n",
      "725 / 982\n",
      "726 / 982\n",
      "727 / 982\n",
      "728 / 982\n",
      "729 / 982\n",
      "730 / 982\n",
      "731 / 982\n",
      "732 / 982\n",
      "733 / 982\n",
      "734 / 982\n",
      "735 / 982\n",
      "736 / 982\n",
      "737 / 982\n",
      "738 / 982\n",
      "739 / 982\n",
      "740 / 982\n",
      "741 / 982\n",
      "742 / 982\n",
      "743 / 982\n",
      "744 / 982\n",
      "745 / 982\n",
      "746 / 982\n",
      "747 / 982\n",
      "748 / 982\n",
      "749 / 982\n",
      "750 / 982\n",
      "751 / 982\n",
      "752 / 982\n",
      "753 / 982\n",
      "754 / 982\n",
      "755 / 982\n",
      "756 / 982\n",
      "757 / 982\n",
      "758 / 982\n",
      "759 / 982\n",
      "760 / 982\n",
      "761 / 982\n",
      "762 / 982\n",
      "763 / 982\n",
      "764 / 982\n",
      "765 / 982\n",
      "766 / 982\n",
      "767 / 982\n",
      "768 / 982\n",
      "769 / 982\n",
      "770 / 982\n",
      "771 / 982\n",
      "772 / 982\n",
      "773 / 982\n",
      "774 / 982\n",
      "775 / 982\n",
      "776 / 982\n",
      "777 / 982\n",
      "778 / 982\n",
      "779 / 982\n",
      "780 / 982\n",
      "781 / 982\n",
      "782 / 982\n",
      "783 / 982\n",
      "784 / 982\n",
      "785 / 982\n",
      "786 / 982\n",
      "787 / 982\n",
      "788 / 982\n",
      "789 / 982\n",
      "790 / 982\n",
      "791 / 982\n",
      "792 / 982\n",
      "793 / 982\n",
      "794 / 982\n",
      "795 / 982\n",
      "796 / 982\n",
      "797 / 982\n",
      "798 / 982\n",
      "799 / 982\n",
      "800 / 982\n",
      "801 / 982\n",
      "802 / 982\n",
      "803 / 982\n",
      "804 / 982\n",
      "805 / 982\n",
      "806 / 982\n",
      "807 / 982\n",
      "808 / 982\n",
      "809 / 982\n",
      "810 / 982\n",
      "811 / 982\n",
      "812 / 982\n",
      "813 / 982\n",
      "814 / 982\n",
      "815 / 982\n",
      "816 / 982\n",
      "817 / 982\n",
      "818 / 982\n",
      "819 / 982\n",
      "820 / 982\n",
      "821 / 982\n",
      "822 / 982\n",
      "823 / 982\n",
      "824 / 982\n",
      "825 / 982\n",
      "826 / 982\n",
      "827 / 982\n",
      "828 / 982\n",
      "829 / 982\n",
      "830 / 982\n",
      "831 / 982\n",
      "832 / 982\n",
      "833 / 982\n",
      "834 / 982\n",
      "835 / 982\n",
      "836 / 982\n",
      "837 / 982\n",
      "838 / 982\n",
      "839 / 982\n",
      "840 / 982\n",
      "841 / 982\n",
      "842 / 982\n",
      "843 / 982\n",
      "844 / 982\n",
      "845 / 982\n",
      "846 / 982\n",
      "847 / 982\n",
      "848 / 982\n",
      "849 / 982\n",
      "850 / 982\n",
      "851 / 982\n",
      "852 / 982\n",
      "853 / 982\n",
      "854 / 982\n",
      "855 / 982\n",
      "856 / 982\n",
      "857 / 982\n",
      "858 / 982\n",
      "859 / 982\n",
      "860 / 982\n",
      "861 / 982\n",
      "862 / 982\n",
      "863 / 982\n",
      "864 / 982\n",
      "865 / 982\n",
      "866 / 982\n",
      "867 / 982\n",
      "868 / 982\n",
      "869 / 982\n",
      "870 / 982\n",
      "871 / 982\n",
      "872 / 982\n",
      "873 / 982\n",
      "874 / 982\n",
      "875 / 982\n",
      "876 / 982\n",
      "877 / 982\n",
      "878 / 982\n",
      "879 / 982\n",
      "880 / 982\n",
      "881 / 982\n",
      "882 / 982\n",
      "883 / 982\n",
      "884 / 982\n",
      "885 / 982\n",
      "886 / 982\n",
      "887 / 982\n",
      "888 / 982\n",
      "889 / 982\n",
      "890 / 982\n",
      "891 / 982\n",
      "892 / 982\n",
      "893 / 982\n",
      "894 / 982\n",
      "895 / 982\n",
      "896 / 982\n",
      "897 / 982\n",
      "898 / 982\n",
      "899 / 982\n",
      "900 / 982\n",
      "901 / 982\n",
      "902 / 982\n",
      "903 / 982\n",
      "904 / 982\n",
      "905 / 982\n",
      "906 / 982\n",
      "907 / 982\n",
      "908 / 982\n",
      "909 / 982\n",
      "910 / 982\n",
      "911 / 982\n",
      "912 / 982\n",
      "913 / 982\n",
      "914 / 982\n",
      "915 / 982\n",
      "916 / 982\n",
      "917 / 982\n",
      "918 / 982\n",
      "919 / 982\n",
      "920 / 982\n",
      "921 / 982\n",
      "922 / 982\n",
      "923 / 982\n",
      "924 / 982\n",
      "925 / 982\n",
      "926 / 982\n",
      "927 / 982\n",
      "928 / 982\n",
      "929 / 982\n",
      "930 / 982\n",
      "931 / 982\n",
      "932 / 982\n",
      "933 / 982\n",
      "934 / 982\n",
      "935 / 982\n",
      "936 / 982\n",
      "937 / 982\n",
      "938 / 982\n",
      "939 / 982\n",
      "940 / 982\n",
      "941 / 982\n",
      "942 / 982\n",
      "943 / 982\n",
      "944 / 982\n",
      "945 / 982\n",
      "946 / 982\n",
      "947 / 982\n",
      "948 / 982\n",
      "949 / 982\n",
      "950 / 982\n",
      "951 / 982\n",
      "952 / 982\n",
      "953 / 982\n",
      "954 / 982\n",
      "955 / 982\n",
      "956 / 982\n",
      "957 / 982\n",
      "958 / 982\n",
      "959 / 982\n",
      "960 / 982\n",
      "961 / 982\n",
      "962 / 982\n",
      "963 / 982\n",
      "964 / 982\n",
      "965 / 982\n",
      "966 / 982\n",
      "967 / 982\n",
      "968 / 982\n",
      "969 / 982\n",
      "970 / 982\n",
      "971 / 982\n",
      "972 / 982\n",
      "973 / 982\n",
      "974 / 982\n",
      "975 / 982\n",
      "976 / 982\n",
      "977 / 982\n",
      "978 / 982\n",
      "979 / 982\n",
      "980 / 982\n",
      "981 / 982\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sequence</th>\n",
       "      <th>Sequence ID</th>\n",
       "      <th>model_input</th>\n",
       "      <th>Enzyme rep</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MTTGKGKILILGATGYLGKYMVKASISLGHPTYAYVMPLKKNSDDS...</td>\n",
       "      <td>Sequence_0</td>\n",
       "      <td>MTTGKGKILILGATGYLGKYMVKASISLGHPTYAYVMPLKKNSDDS...</td>\n",
       "      <td>[-0.03220587, -0.031796478, -0.051493254, 0.03...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MEENGMKSKILIFGGTGYIGNHMVKGSLKLGHPTYVFTRPNSSKTT...</td>\n",
       "      <td>Sequence_1</td>\n",
       "      <td>MEENGMKSKILIFGGTGYIGNHMVKGSLKLGHPTYVFTRPNSSKTT...</td>\n",
       "      <td>[-0.016749369, -0.048214775, -0.049711403, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MGKGGNSEDAVSGKEHGEENMAAWLLGIKTLKIQPYILPSLGPYDV...</td>\n",
       "      <td>Sequence_2</td>\n",
       "      <td>MGKGGNSEDAVSGKEHGEENMAAWLLGIKTLKIQPYILPSLGPYDV...</td>\n",
       "      <td>[-0.0048636524, -0.069875315, -0.014163645, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MANLRESSRDKSRWSLEGMTALVTGGSKGIGEAVVEELAMLGARVH...</td>\n",
       "      <td>Sequence_3</td>\n",
       "      <td>MANLRESSRDKSRWSLEGMTALVTGGSKGIGEAVVEELAMLGARVH...</td>\n",
       "      <td>[-0.0007728286, -0.061243184, 0.041369658, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MAKEGGLGENSRWSLGGMTALVTGGSKGIGEAVVEELAMLGAKVHT...</td>\n",
       "      <td>Sequence_4</td>\n",
       "      <td>MAKEGGLGENSRWSLGGMTALVTGGSKGIGEAVVEELAMLGAKVHT...</td>\n",
       "      <td>[-0.011831594, -0.06318853, 0.038726423, 0.021...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sequence Sequence ID  \\\n",
       "0  MTTGKGKILILGATGYLGKYMVKASISLGHPTYAYVMPLKKNSDDS...  Sequence_0   \n",
       "1  MEENGMKSKILIFGGTGYIGNHMVKGSLKLGHPTYVFTRPNSSKTT...  Sequence_1   \n",
       "2  MGKGGNSEDAVSGKEHGEENMAAWLLGIKTLKIQPYILPSLGPYDV...  Sequence_2   \n",
       "3  MANLRESSRDKSRWSLEGMTALVTGGSKGIGEAVVEELAMLGARVH...  Sequence_3   \n",
       "4  MAKEGGLGENSRWSLGGMTALVTGGSKGIGEAVVEELAMLGAKVHT...  Sequence_4   \n",
       "\n",
       "                                         model_input  \\\n",
       "0  MTTGKGKILILGATGYLGKYMVKASISLGHPTYAYVMPLKKNSDDS...   \n",
       "1  MEENGMKSKILIFGGTGYIGNHMVKGSLKLGHPTYVFTRPNSSKTT...   \n",
       "2  MGKGGNSEDAVSGKEHGEENMAAWLLGIKTLKIQPYILPSLGPYDV...   \n",
       "3  MANLRESSRDKSRWSLEGMTALVTGGSKGIGEAVVEELAMLGARVH...   \n",
       "4  MAKEGGLGENSRWSLGGMTALVTGGSKGIGEAVVEELAMLGAKVHT...   \n",
       "\n",
       "                                          Enzyme rep  \n",
       "0  [-0.03220587, -0.031796478, -0.051493254, 0.03...  \n",
       "1  [-0.016749369, -0.048214775, -0.049711403, 0.0...  \n",
       "2  [-0.0048636524, -0.069875315, -0.014163645, 0....  \n",
       "3  [-0.0007728286, -0.061243184, 0.041369658, 0.0...  \n",
       "4  [-0.011831594, -0.06318853, 0.038726423, 0.021...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating model input:\n",
    "df_sequences[\"model_input\"] = [seq[:1022] for seq in df_sequences[\"Sequence\"]]\n",
    "model_input = [(df_sequences[\"Sequence ID\"][ind], df_sequences[\"model_input\"][ind]) for ind in df_sequences.index]\n",
    "seqs = [model_input[i][1] for i in range(len(model_input))]\n",
    "# loading ESM-2 model:\n",
    "print(\".....2(a) Loading ESM-2 model.\")\n",
    "model, alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n",
    "batch_converter = alphabet.get_batch_converter()\n",
    "#convert input into batches:\n",
    "\n",
    "#Calculate ESM-2 representations\n",
    "print(\".....2(b) Calculating enzyme representations.\")\n",
    "df_sequences[\"Enzyme rep\"] = \"\"\n",
    "\n",
    "for ind in df_sequences.index:\n",
    "    print(ind,\"/\",len(df_sequences))\n",
    "    batch_labels, batch_strs, batch_tokens = batch_converter([(df_sequences[\"Sequence ID\"][ind], df_sequences[\"model_input\"][ind])])\n",
    "    with torch.no_grad():\n",
    "        results = model(batch_tokens, repr_layers=[33])\n",
    "    df_sequences[\"Enzyme rep\"][ind] = results[\"representations\"][33][0, 1 : len(df_sequences[\"model_input\"][ind]) + 1].mean(0).numpy()\n",
    "    \n",
    "df_sequences.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metabolite_type(met):\n",
    "    if is_KEGG_ID(met):\n",
    "        return(\"KEGG\")\n",
    "    elif is_InChI(met):\n",
    "        return(\"InChI\")\n",
    "    else:\n",
    "        return(\"invalid\")\n",
    "\n",
    "def get_reaction_site_smarts(metabolites):\n",
    "    reaction_site = \"\"\n",
    "    for met in metabolites:\n",
    "        met_type = get_metabolite_type(met)\n",
    "        if met_type == \"KEGG\":\n",
    "            try:\n",
    "                Smarts = Chem.MolToSmarts(Chem.MolFromMolFile(join(\"..\", \"..\", \"data\", \"mol-files\",  met + \".mol\")))\n",
    "            except OSError:\n",
    "                return(np.nan)\n",
    "        elif met_type == \"InChI\":\n",
    "            Smarts = Chem.MolToSmarts(Chem.inchi.MolFromInchi(met))\n",
    "        else:\n",
    "            Smarts = \"invalid\"\n",
    "        reaction_site = reaction_site + \".\" + Smarts\n",
    "    return(reaction_site[1:])\n",
    "\n",
    "\n",
    "def is_KEGG_ID(met):\n",
    "    #a valid KEGG ID starts with a \"C\" or \"D\" followed by a 5 digit number:\n",
    "    if len(met) == 6 and met[0] in [\"C\", \"D\"]:\n",
    "        try:\n",
    "            int(met[1:])\n",
    "            return(True)\n",
    "        except: \n",
    "            pass\n",
    "    return(False)\n",
    "\n",
    "def is_InChI(met):\n",
    "    m = Chem.inchi.MolFromInchi(met,sanitize=False)\n",
    "    if m is None:\n",
    "      return(False)\n",
    "    else:\n",
    "      try:\n",
    "        Chem.SanitizeMol(m)\n",
    "      except:\n",
    "        print('.......Metabolite string \"%s\" is in InChI format but has invalid chemistry' % met)\n",
    "        return(False)\n",
    "    return(True)\n",
    "\n",
    "def convert_fp_to_array(difference_fp_dict):\n",
    "    fp = np.zeros(2048)\n",
    "    for key in difference_fp_dict.keys():\n",
    "        fp[key] = difference_fp_dict[key]\n",
    "    return(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>substrates</th>\n",
       "      <th>products</th>\n",
       "      <th>Reaction ID</th>\n",
       "      <th>min_Km_for_RID</th>\n",
       "      <th>MW_frac</th>\n",
       "      <th>difference_fp</th>\n",
       "      <th>structural_fp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{InChI=1S/C12H14O4/c1-9(13)16-7-3-4-10-5-6-11(...</td>\n",
       "      <td>{InChI=1S/C2H4O2/c1-2(3)4/h1H3,(H,3,4)/p-1, In...</td>\n",
       "      <td>Reaction_0</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>1.001043</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>1100111100000001001000110110010001001111111100...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{InChI=1S/C12H14O4/c1-9(13)16-7-3-4-10-5-6-11(...</td>\n",
       "      <td>{InChI=1S/C2H4O2/c1-2(3)4/h1H3,(H,3,4)/p-1, In...</td>\n",
       "      <td>Reaction_1</td>\n",
       "      <td>0.000131</td>\n",
       "      <td>1.001043</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>1100111100000001001000110110010001001111111100...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{InChI=1S/C21H27N7O14P2/c22-17-12-19(25-7-24-1...</td>\n",
       "      <td>{InChI=1S/C21H29N7O14P2/c22-17-12-19(25-7-24-1...</td>\n",
       "      <td>Reaction_2</td>\n",
       "      <td>0.002200</td>\n",
       "      <td>0.998829</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>1100111100000001001000110110010001001101111100...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{InChI=1S/C21H29N7O14P2/c22-17-12-19(25-7-24-1...</td>\n",
       "      <td>{InChI=1S/C21H27N7O14P2/c22-17-12-19(25-7-24-1...</td>\n",
       "      <td>Reaction_3</td>\n",
       "      <td>0.012500</td>\n",
       "      <td>1.001173</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>1100111100000001001000110110010001001111111100...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{InChI=1S/C7H12O/c1-6-3-2-4-7(8)5-6/h6H,2-5H2,...</td>\n",
       "      <td>{InChI=1S/C21H28N7O17P3/c22-17-12-19(25-7-24-1...</td>\n",
       "      <td>Reaction_4</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>1.001175</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>1100111100000001001000110110010001001111111100...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          substrates  \\\n",
       "0  {InChI=1S/C12H14O4/c1-9(13)16-7-3-4-10-5-6-11(...   \n",
       "1  {InChI=1S/C12H14O4/c1-9(13)16-7-3-4-10-5-6-11(...   \n",
       "2  {InChI=1S/C21H27N7O14P2/c22-17-12-19(25-7-24-1...   \n",
       "3  {InChI=1S/C21H29N7O14P2/c22-17-12-19(25-7-24-1...   \n",
       "4  {InChI=1S/C7H12O/c1-6-3-2-4-7(8)5-6/h6H,2-5H2,...   \n",
       "\n",
       "                                            products Reaction ID  \\\n",
       "0  {InChI=1S/C2H4O2/c1-2(3)4/h1H3,(H,3,4)/p-1, In...  Reaction_0   \n",
       "1  {InChI=1S/C2H4O2/c1-2(3)4/h1H3,(H,3,4)/p-1, In...  Reaction_1   \n",
       "2  {InChI=1S/C21H29N7O14P2/c22-17-12-19(25-7-24-1...  Reaction_2   \n",
       "3  {InChI=1S/C21H27N7O14P2/c22-17-12-19(25-7-24-1...  Reaction_3   \n",
       "4  {InChI=1S/C21H28N7O17P3/c22-17-12-19(25-7-24-1...  Reaction_4   \n",
       "\n",
       "   min_Km_for_RID   MW_frac  \\\n",
       "0        0.000073  1.001043   \n",
       "1        0.000131  1.001043   \n",
       "2        0.002200  0.998829   \n",
       "3        0.012500  1.001173   \n",
       "4        0.000007  1.001175   \n",
       "\n",
       "                                       difference_fp  \\\n",
       "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "                                       structural_fp  \n",
       "0  1100111100000001001000110110010001001111111100...  \n",
       "1  1100111100000001001000110110010001001111111100...  \n",
       "2  1100111100000001001000110110010001001101111100...  \n",
       "3  1100111100000001001000110110010001001111111100...  \n",
       "4  1100111100000001001000110110010001001111111100...  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reactions[\"difference_fp\"], df_reactions[\"structural_fp\"],  = \"\", \"\"\n",
    "#each metabolite should be either a KEGG ID, InChI string, or a SMILES:\n",
    "for ind in df_reactions.index:\n",
    "    if df_reactions[\"difference_fp\"][ind] == \"\" or df_reactions[\"structural_fp\"][ind] == \"\":\n",
    "        left_site = get_reaction_site_smarts(df_reactions[\"substrates\"][ind])\n",
    "        right_site = get_reaction_site_smarts(df_reactions[\"products\"][ind])\n",
    "        if not pd.isnull(left_site) and not pd.isnull(right_site):\n",
    "            rxn_forward = AllChem.ReactionFromSmarts(left_site + \">>\" + right_site)\n",
    "            difference_fp = Chem.rdChemReactions.CreateDifferenceFingerprintForReaction(rxn_forward)\n",
    "            difference_fp = convert_fp_to_array(difference_fp.GetNonzeroElements())\n",
    "            df_reactions[\"difference_fp\"][ind] = difference_fp\n",
    "            df_reactions[\"structural_fp\"][ind] = Chem.rdChemReactions.CreateStructuralFingerprintForReaction(rxn_forward).ToBitString()\n",
    "\n",
    "df_reactions.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sequences.to_pickle(join(datasets_dir, \"all_sequences_with_IDs.pkl\"))\n",
    "df_reactions.to_pickle(join(datasets_dir, \"all_reactions_with_IDs.pkl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mapping Sequence and Reaction IDs to Km_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Km = df_Km.merge(df_sequences, on = \"Sequence\", how = \"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reactions.rename(columns = {\"substrates\" : \"substrate_IDs\",\n",
    "                              \"products\" : \"product_IDs\"}, inplace = True)\n",
    "\n",
    "df_Km[\"Reaction ID\"] = np.nan\n",
    "df_Km[\"MW_frac\"] = np.nan\n",
    "df_Km[\"min_Km_for_RID\"] = np.nan\n",
    "df_Km[\"difference_fp\"] = \"\"\n",
    "df_Km[\"structural_fp\"] = \"\"\n",
    "\n",
    "for ind in df_Km.index:\n",
    "    sub_set, pro_set = df_Km[\"substrate_IDs\"][ind], df_Km[\"product_IDs\"][ind]\n",
    "    help_df = df_reactions.loc[df_reactions[\"substrate_IDs\"] == sub_set].loc[df_reactions[\"product_IDs\"] == pro_set]\n",
    "    if len(help_df) == 1:\n",
    "        df_Km[\"Reaction ID\"][ind] = list(help_df[\"Reaction ID\"])[0]\n",
    "        df_Km[\"min_Km_for_RID\"][ind] = list(help_df[\"min_Km_for_RID\"])[0]\n",
    "        df_Km[\"MW_frac\"][ind] = list(help_df[\"MW_frac\"])[0]\n",
    "        df_Km[\"difference_fp\"][ind] = list(help_df[\"difference_fp\"])[0]\n",
    "        df_Km[\"structural_fp\"][ind] = list(help_df[\"structural_fp\"])[0]\n",
    "df_Km.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Km[\"MACCS FP\"] = \"\"\n",
    "\n",
    "for ind in df_Km.index:\n",
    "    id = df_Km[\"Main Substrate\"][ind]\n",
    "    # try:\n",
    "    #     id = df_Km['Substrate_IDs'][ind][df_Km[\"Substrates\"][ind].split(';').index(substrate)]\n",
    "    # except:\n",
    "    #     for i,s in enumerate(df_Km[\"Substrates\"][ind].split(';')[:-1]):\n",
    "    #         if substrate in s or s in substrate:\n",
    "                # id = list(df_Km['Substrate_IDs'][ind])[i]\n",
    "    if id[0] == \"C\":\n",
    "        try:\n",
    "            mol = Chem.MolFromMolFile(join(datasets_dir,\"mol-files\", id + '.mol'))\n",
    "        except OSError:\n",
    "            None\n",
    "    else:\n",
    "        try:\n",
    "            mol = Chem.inchi.MolFromInchi(id,sanitize=False)\n",
    "        except OSError:\n",
    "            None\n",
    "    if mol is not None:\n",
    "        maccs_fp = MACCSkeys.GenMACCSKeys(mol).ToBitString()\n",
    "        df_Km[\"MACCS FP\"][ind] = maccs_fp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating the minimal Km value for every EC number in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_EC_Km = pd.read_csv(join(\"..\", \"..\", \"data\", \"min_EC_\" + organism + \".tsv\"), sep = \"\\t\", header=0)\n",
    "# df_EC_Km = df_EC_Km.rename(columns={0: \"EC\", 1: \"min_Km\"})\n",
    "\n",
    "for ind in df_EC_Km.index:\n",
    "    try:\n",
    "        Km_min = df_EC_Km[df_EC_Km[\"EC\"] == df_Km[\"ECs\"]][\"min_Km\"]\n",
    "        df_EC_Km[\"min_Km\"][ind] = Km_min\n",
    "        print(ind, Km_min)\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "df_EC_Km.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_EC_Km = pd.read_csv(join(\"..\", \"..\", \"data\", \"min_EC_\" + organism + \".tsv\"), sep = \"\\t\", header=0)\n",
    "\n",
    "df_EC_Km.head(5)\n",
    "df_Km[\"min_Km_for_EC\"] = np.nan\n",
    "\n",
    "for ind in df_Km.index:\n",
    "    EC = df_Km[\"ECs\"][ind]\n",
    "    min_Km = 0\n",
    "    try:\n",
    "        print(EC)\n",
    "        min_Km = df_EC_Km.loc[df_EC_Km[\"EC\"] == EC, \"min_Km\"].iloc[0]\n",
    "        print(min_Km)\n",
    "    except:\n",
    "        pass\n",
    "    if min_Km != 0:\n",
    "        df_Km[\"min_Km_for_EC\"][ind] = min_Km\n",
    "df_Km.to_pickle(join(\"..\", \"..\", \"data\", \"merged_and_grouped_Km_dataset.pkl\"))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Removing outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing non-optimally measured values\n",
    "\n",
    "To ignore $Km$ values that were obtained under non-optimal conditions, we exclude values higher than 10000\\% than the minimal $Km$ value for the same enzyme-reaction combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Km[\"frac_of_min_UID\"] = np.nan\n",
    "df_Km[\"frac_of_min_RID\"] = np.nan\n",
    "df_Km[\"frac_of_min_EC\"] = np.nan\n",
    "\n",
    "for ind in df_Km.index:\n",
    "    df_Km[\"frac_of_min_UID\"][ind] =  df_Km[\"min_Km_for_UID\"][ind]/df_Km[\"Km\"][ind]\n",
    "    df_Km[\"frac_of_min_RID\"][ind] =  df_Km[\"min_Km_for_RID\"][ind]/df_Km[\"Km\"][ind]\n",
    "    df_Km[\"frac_of_min_EC\"][ind] = df_Km[\"min_Km_for_EC\"][ind]/df_Km[\"Km\"][ind]\n",
    "\n",
    "len(df_Km)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(df_Km)\n",
    "\n",
    "df_Km = df_Km.loc[df_Km[\"frac_of_min_UID\"] >= 0.01]\n",
    "df_Km = df_Km.loc[df_Km[\"frac_of_min_RID\"] >= 0.01]\n",
    "\n",
    "df_Km[\"frac_of_min_EC\"].loc[pd.isnull(df_Km[\"frac_of_min_EC\"])] = 1\n",
    "df_Km = df_Km.loc[df_Km[\"frac_of_min_EC\"] <= 10]\n",
    "df_Km = df_Km.loc[df_Km[\"frac_of_min_EC\"] >= 0.01]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"We remove %s data points, because we suspect that these Km values were not measure for the natural reaction \" \\\n",
    "    \"of an enzyme or under non-optimal conditions.\" % (n-len(df_Km)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing data points with reaction queations with uneven fraction of molecular weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(df_Km)\n",
    "\n",
    "df_Km = df_Km.loc[df_Km[\"MW_frac\"] < 3]\n",
    "df_Km = df_Km.loc[df_Km[\"MW_frac\"] > 1/3]\n",
    "\n",
    "print(\"We remove %s data points because the sum of molecular weights of substrates does not match the sum of molecular\" \\\n",
    "      \"weights of the products.\" % (n-len(df_Km)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Size of final Km dataset: %s\" % len(df_Km))\n",
    "df_Km.to_pickle(join(\"..\", \"..\", \"data\", \"final_Km_dataset_\" + organism + \".pkl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Preparing dataset and splitting into train-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Km = pd.read_pickle(join(\"..\", \"..\", \"data\", \"final_Km_dataset_\" + organism + \".pkl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making input for GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for ind in df_Km.index:\n",
    "#     substrate = df_Km[\"Main Substrate\"][ind]\n",
    "#     try:\n",
    "#         id = list(df_Km['substrate_IDs'][ind])[df_Km[\"Substrates\"][ind].split(';').index(substrate)]\n",
    "#     except:\n",
    "#         for i,s in enumerate(df_Km[\"Substrates\"][ind].split(';')[:-1]):\n",
    "#             if substrate in s or s in substrate:\n",
    "#                 id = list(df_Km['substrate_IDs'][ind])[i]\n",
    "#     df_Km[\"Main Substrate\"][ind] = id\n",
    "\n",
    "inchi_ids = {}\n",
    "for i, element in enumerate(df_Km[\"Main Substrate\"]):\n",
    "    if element[0] != 'C' and element not in inchi_ids.keys():\n",
    "        inchi_ids[element] = str(i)\n",
    "        # mol = Chem.inchi.MolFromInchi(element)\n",
    "        # if not mol is None:\n",
    "        #     calculate_atom_and_bond_feature_vectors(mol, str(i))\n",
    "        # Chem.rdmolfiles.MolToMolFile(Chem.inchi.MolFromInchi(element), join(datasets_dir,\"mol-files\", str(i) + \".mol\")  )  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting glucosinolates into validation dataset\n",
    "\n",
    "Search UniProt for GO term related to glucosionalte metabolic process, download file as .tsv and filter dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "glucosinolates = pd.read_table(join(datasets_dir,\"glucosinolates.tsv\"))[\"Entry\"].tolist()\n",
    "df_validation = df_Km[df_Km[\"Uniprot IDs\"].isin(glucosinolates)]\n",
    "df_validation.reset_index(inplace=True, drop = True)\n",
    "df_Km = df_Km[~df_Km[\"Uniprot IDs\"].isin(glucosinolates)]\n",
    "df_Km.reset_index(inplace=True, drop = True)\n",
    "split = \"full\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If training-testing with only Arabidopsis data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_Km = df_Km[df_Km[\"Organism\"] == 'Arabidopsis thaliana']\n",
    "# df_Km.reset_index(inplace=True, drop = True)\n",
    "# split = \"Arabidopsis\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If training-testing with only Brassicaceae data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ncbi = NCBITaxa()\n",
    "\n",
    "# organisms = {}\n",
    "\n",
    "# def is_brassicaceae(org):\n",
    "#     try:\n",
    "#         tax_id = ncbi.get_name_translator([org])[org][0]\n",
    "#         lineage = ncbi.get_lineage(tax_id)\n",
    "#         if 3700 not in lineage:\n",
    "#             return(False)\n",
    "#         else:\n",
    "#             return(True)\n",
    "#     except KeyError:\n",
    "#         return(False)\n",
    "    \n",
    "# for org in df_Km[\"Organism\"].tolist():\n",
    "#     if org not in organisms.keys():\n",
    "#         organisms[org] = is_brassicaceae(org)\n",
    "\n",
    "# df_Km = df_Km[df_Km[\"Organism\"].isin([key for key, value in organisms.items() if value is True])]\n",
    "# df_Km.reset_index(inplace=True, drop = True)\n",
    "# split = \"Brassicaceae\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If training-testing only with wildtype data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_Km = df_Km[df_Km[\"Type\"].str.contains(\"wildtype\")]\n",
    "# df_Km.reset_index(inplace=True, drop = True)\n",
    "# split = \"wildtype\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If training-testing only with secondary metabolite data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "secondary = pd.read_table(join(datasets_dir,\"secondary_metabolites.tsv\"))[\"Entry\"].tolist()\n",
    "df_Km = df_Km[df_Km[\"Uniprot IDs\"].isin(secondary)]\n",
    "df_Km.reset_index(inplace=True, drop = True)\n",
    "split = \"secondary\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.mkdir(join(datasets_dir, \"splits\", split))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating arithmetic mean for Km values of same enzyme-reaction-substrate combination-pH-temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_new = pd.DataFrame(data = {\"Reaction ID\" : df_Km[\"Reaction ID\"],\n",
    "#                                   \"Sequence ID\" : df_Km[\"Sequence ID\"],\n",
    "#                                   \"Temperature\" : df_Km[\"Temperature\"],\n",
    "#                                     \"pH\" : df_Km[\"pH\"],\n",
    "#                                  \"Type\": df_Km[\"Type\"],\n",
    "#                              \"MACCS FP\" : df_Km[\"MACCS FP\"]})\n",
    "\n",
    "# df_new.drop_duplicates(inplace = True)\n",
    "# df_new.reset_index(inplace = True, drop = True)\n",
    "\n",
    "# df_new[\"Km_values\"], df_new[\"Uniprot IDs\"], df_new[\"ECs\"], df_new[\"Substrates\"], df_new[\"Products\"], df_new[\"ESM2\"], df_new[\"Sequence\"], df_new[\"difference_fp\"], df_new[\"structural_fp\"] = \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"\n",
    "\n",
    "# for ind in df_new.index:\n",
    "#     RID, SID, Temp, pH, Type, MSubstrate = df_new[\"Reaction ID\"][ind], df_new[\"Sequence ID\"][ind], df_new[\"Temperature\"][ind], df_new[\"pH\"][ind], df_new[\"Type\"][ind], df_new[\"MACCS FP\"][ind]\n",
    "#     help_df = df_Km.loc[df_Km[\"Reaction ID\"] \n",
    "#                                  == RID].loc[df_Km[\"Sequence ID\"] \n",
    "#                                              == SID].loc[df_Km[\"Temperature\"] \n",
    "#                                                          == Temp].loc[df_Km[\"pH\"] \n",
    "#                                                                       == pH].loc[df_Km[\"Type\"] \n",
    "#                                                                                  == Type].loc[df_Km[\"MACCS FP\"] \n",
    "#                                                                                               == MSubstrate]\n",
    "#     print(help_df)\n",
    "#     df_new[\"ECs\"][ind] = list(help_df[\"ECs\"])\n",
    "#     df_new[\"Km_values\"][ind] = list(help_df[\"Km\"])\n",
    "#     df_new[\"Uniprot IDs\"][ind] = list(help_df[\"Uniprot IDs\"])\n",
    "#     df_new[\"Sequence\"][ind] = help_df[\"Sequence\"].values[0]\n",
    "#     df_new[\"ESM2\"][ind] = help_df[\"Enzyme rep\"].values[0]\n",
    "#     df_new[\"difference_fp\"][ind], df_new[\"structural_fp\"][ind] = help_df[\"difference_fp\"].values[0], help_df[\"structural_fp\"].values[0]\n",
    "#     df_new[\"Substrates\"][ind], df_new[\"Products\"][ind] = help_df[\"Substrates\"].values[0], help_df[\"Products\"].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_new2 = pd.DataFrame(data = {\"Reaction ID\" : df_validation[\"Reaction ID\"],\n",
    "#                                   \"Sequence ID\" : df_validation[\"Sequence ID\"],\n",
    "#                                   \"Temperature\" : df_validation[\"Temperature\"],\n",
    "#                                     \"pH\" : df_validation[\"pH\"],\n",
    "#                                   \"Type\" : df_validation[\"Type\"],\n",
    "#                                   \"MACCS FP\" : df_validation[\"MACCS FP\"]})\n",
    "\n",
    "# df_new2.drop_duplicates(inplace = True)\n",
    "# df_new2.reset_index(inplace = True, drop = True)\n",
    "\n",
    "# df_new2[\"Km_values\"], df_new2[\"Uniprot IDs\"], df_new2[\"ECs\"], df_new2[\"Organisms\"], df_new2[\"Substrates\"], df_new2[\"Products\"], df_new2[\"ESM2\"], df_new2[\"Sequence\"], df_new2[\"difference_fp\"], df_new2[\"structural_fp\"] = \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"\n",
    "\n",
    "# for ind in df_new2.index:\n",
    "#     RID, SID, Temp, pH, Type, MSubstrate = df_new2[\"Reaction ID\"][ind], df_new2[\"Sequence ID\"][ind], df_new2[\"Temperature\"][ind], df_new2[\"pH\"][ind], df_new2[\"Type\"][ind], df_new2[\"MACCS FP\"][ind]\n",
    "#     help_df = df_validation.loc[df_validation[\"Reaction ID\"] \n",
    "#                               == RID].loc[df_validation[\"Sequence ID\"] \n",
    "#                                           == SID].loc[df_validation[\"Temperature\"] \n",
    "#                                                       == Temp].loc[df_validation[\"pH\"] \n",
    "#                                                                     == pH].loc[df_validation[\"Type\"] \n",
    "#                                                                               == Type].loc[df_validation[\"MACCS FP\"] \n",
    "#                                                                                                             == MSubstrate]\n",
    "#     df_new2[\"ECs\"][ind] = list(help_df[\"ECs\"])\n",
    "#     df_new2[\"Km_values\"][ind] = list(help_df[\"Km\"])\n",
    "#     df_new2[\"Uniprot IDs\"][ind] = list(help_df[\"Uniprot IDs\"])\n",
    "#     df_new2[\"Organisms\"][ind] = list(help_df[\"Organism\"])\n",
    "#     df_new2[\"Type\"][ind]\n",
    "#     df_new2[\"Sequence\"][ind] = help_df[\"Sequence\"].values[0]\n",
    "#     df_new2[\"ESM2\"][ind] = help_df[\"Enzyme rep\"].values[0]\n",
    "#     df_new2[\"difference_fp\"][ind], df_new2[\"structural_fp\"][ind] = help_df[\"difference_fp\"].values[0], help_df[\"structural_fp\"].values[0]\n",
    "#     df_new2[\"Substrates\"][ind], df_new2[\"Products\"][ind] = help_df[\"Substrates\"].values[0], help_df[\"Products\"].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_Km = df_new\n",
    "# df_validation = df_new2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_Km[\"geomean_Km\"] = np.nan\n",
    "# for ind in df_Km.index:\n",
    "#     all_Km = np.array(df_Km[\"Km_values\"][ind]).astype(float)\n",
    "    # min_Km = min(all_Km)\n",
    "    # all_Km_top = [Km for Km in all_Km  if min_Km/Km >= 0.01]\n",
    "    # df_arabidopsis[\"geomean_Km\"][ind] = np.mean((all_Km_top))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_validation[\"geomean_Km\"] = np.nan\n",
    "# for ind in df_validation.index:\n",
    "#     all_Km = np.array(df_validation[\"Km_values\"][ind]).astype(float)\n",
    "#     min_Km = min(all_Km)\n",
    "#     all_Km_top = [Km for Km in all_Km  if min_Km/Km >= 0.01]\n",
    "#     df_validation[\"geomean_Km\"][ind] = np.mean((all_Km_top))\n",
    "    \n",
    "# df_validation.to_pickle(join(datasets_dir, \"splits\", \"validation_%s.pkl\" %organism))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting into train-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_Km.copy()\n",
    "df = df.sample(frac = 1, random_state = 123)\n",
    "df.reset_index(drop= True, inplace = True)\n",
    "\n",
    "train_df, test_df = split_dataframe_enzyme(frac = 5, df = df.copy())\n",
    "print(\"Test set size: %s\" % len(test_df))\n",
    "print(\"Training set size: %s\" % len(train_df))\n",
    "print(\"Size of test set in percent: %s\" % np.round(100*len(test_df)/ (len(test_df) + len(train_df))))\n",
    "\n",
    "train_df.reset_index(inplace = True, drop = True)\n",
    "test_df.reset_index(inplace = True, drop = True)\n",
    "\n",
    "train_df.to_pickle(join(datasets_dir, \"splits\", split, \"train_df_Km_%s.pkl\" %organism))\n",
    "test_df.to_pickle(join(datasets_dir, \"splits\", split, \"test_df_Km_%s.pkl\" %organism))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting CV folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train2 = train_df.copy()\n",
    "data_train2[\"index\"] = list(data_train2.index)\n",
    "\n",
    "data_train2, df_fold = split_dataframe_enzyme(df = data_train2, frac=5)\n",
    "indices_fold1 = list(df_fold[\"index\"])\n",
    "print(len(data_train2), len(indices_fold1))\n",
    "\n",
    "data_train2, df_fold = split_dataframe_enzyme(df = data_train2, frac=4)\n",
    "indices_fold2 = list(df_fold[\"index\"])\n",
    "print(len(data_train2), len(indices_fold2))\n",
    "\n",
    "data_train2, df_fold = split_dataframe_enzyme(df = data_train2, frac=3)\n",
    "indices_fold3 = list(df_fold[\"index\"])\n",
    "print(len(data_train2), len(indices_fold3))\n",
    "\n",
    "data_train2, df_fold = split_dataframe_enzyme(df = data_train2, frac=2)\n",
    "indices_fold4 = list(df_fold[\"index\"])\n",
    "indices_fold5 = list(data_train2[\"index\"])\n",
    "print(len(data_train2), len(indices_fold4))\n",
    "\n",
    "\n",
    "fold_indices = [indices_fold1, indices_fold2, indices_fold3, indices_fold4, indices_fold5]\n",
    "\n",
    "CV_train_indices = [[], [], [], [], []]\n",
    "CV_test_indices = [[], [], [], [], []]\n",
    "\n",
    "for i in range(5):\n",
    "    for j in range(5):\n",
    "        if i != j:\n",
    "            CV_train_indices[i] = CV_train_indices[i] + fold_indices[j]\n",
    "    CV_test_indices[i] = fold_indices[i]\n",
    "    \n",
    "    \n",
    "np.save(join(datasets_dir, \"splits\", split, \"CV_train_indices_%s\" %organism), CV_train_indices)\n",
    "np.save(join(datasets_dir, \"splits\", split, \"CV_test_indices_%s\" %organism), CV_test_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Building GNN for substrate representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.mkdir(join(datasets_dir, \"GNN_input_data\", split))\n",
    "\n",
    "for ind in train_df.index:\n",
    "    calculate_and_save_input_matrixes(inchi_ids, sample_ID = \"train_\" + str(ind), df = train_df,\n",
    "                                      save_folder = join(datasets_dir, \"GNN_input_data\", split))\n",
    "    \n",
    "for ind in test_df.index:\n",
    "    calculate_and_save_input_matrixes(inchi_ids, sample_ID = \"test_\" + str(ind), df = test_df,\n",
    "                                      save_folder = join(datasets_dir, \"GNN_input_data\", split))\n",
    "    \n",
    "for ind in df_validation.index:\n",
    "    calculate_and_save_input_matrixes(inchi_ids, sample_ID = \"val_\" + str(ind), df = df_validation,\n",
    "                                    save_folder = join(datasets_dir, \"GNN_input_data\", split))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices = os.listdir(join(datasets_dir, \"GNN_input_data\", split))\n",
    "train_indices = [index[:index.rfind(\"_\")] for index in train_indices]\n",
    "train_indices = list(set([index for index in train_indices if \"train\" in index]))\n",
    "\n",
    "test_indices = os.listdir(join(datasets_dir, \"GNN_input_data\", split))\n",
    "test_indices = [index[:index.rfind(\"_\")] for index in test_indices]\n",
    "test_indices = list(set([index for index in test_indices if \"test\" in index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyper-parameter optimization with CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'batch_size': [96,96,128],\n",
    "                'D': [50,100],\n",
    "                'learning_rate': [0.01, 0.1],\n",
    "                'epochs': [30,50,80],\n",
    "                'l2_reg_fc' : [0.01, 0.1, 1],\n",
    "                'l2_reg_conv': [0.01, 0.1, 1],\n",
    "                'rho': [0.9, 0.95, 0.99]}\n",
    "\n",
    "params_list = [(batch_size, D, learning_rate, epochs, l2_reg_fc, l2_reg_conv, rho) for batch_size in param_grid['batch_size'] for D in param_grid[\"D\"] for learning_rate in param_grid['learning_rate']\n",
    "                for epochs in param_grid['epochs'] for l2_reg_fc in param_grid['l2_reg_fc'] for l2_reg_conv in param_grid['l2_reg_conv'] for rho in param_grid[\"rho\"]]\n",
    "\n",
    "params_list = random.sample(params_list, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count = 0\n",
    "# results=[]\n",
    "\n",
    "# for params in params_list:\n",
    "\n",
    "#     batch_size, D, learning_rate, epochs, l2_reg_fc, l2_reg_conv, rho = params\n",
    "#     count +=1\n",
    "#     MAE = []\n",
    "\n",
    "#     for i in range(5):\n",
    "#         train_index, test_index  = CV_train_indices[i], CV_test_indices[i]\n",
    "#         train_index = [ind for ind in train_indices if int(ind.split(\"_\")[1]) in train_index]\n",
    "#         test_index = [ind for ind in train_indices if int(ind.split(\"_\")[1]) in test_index]\n",
    "\n",
    "#         train_params = {'batch_size': batch_size,\n",
    "#                     'folder' :join(datasets_dir, \"GNN_input_data/full\"),\n",
    "#                     'list_IDs' : np.array(train_index),\n",
    "#                     'shuffle': True}\n",
    "\n",
    "#         test_params = {'batch_size': min(batch_size,len(test_index)),\n",
    "#                     'folder' : join(datasets_dir, \"GNN_input_data/full\"),\n",
    "#                     'list_IDs' : np.array(test_index),\n",
    "#                     'shuffle': False}\n",
    "\n",
    "#         training_generator = DataGenerator(**train_params)\n",
    "#         test_generator = DataGenerator(**test_params)\n",
    "\n",
    "\n",
    "#         model = DMPNN_without_extra_features(l2_reg_conv = l2_reg_conv, l2_reg_fc = l2_reg_fc, learning_rate = learning_rate,\n",
    "#                         D = D, N = N, F1 = F1, F2 = F2, F= F, drop_rate = 0.0, ada_rho = rho)\n",
    "#         model.fit(training_generator, epochs= epochs, shuffle = True, verbose = 1)\n",
    "\n",
    "#         #get test_y:\n",
    "#         test_indices_y = [int(ind.split(\"_\")[1]) for ind in train_indices if ind in test_index]\n",
    "#         test_y = np.array([train_df[\"Km\"][ind] for ind in test_indices_y])\n",
    "\n",
    "#         pred_test = model.predict(test_generator)\n",
    "#         mae = np.median(abs(pred_test - np.reshape(test_y[:len(pred_test)], (-1,1))))\n",
    "#         print(mae)\n",
    "#         MAE.append(mae)\n",
    "\n",
    "#     results.append({\"batch_size\" : batch_size, \"D\" : D , \"learning_rate\" : learning_rate, \"epochs\" : epochs,\n",
    "#                     \"l2_reg_fc\" : l2_reg_fc, \"l2_reg_conv\" : l2_reg_conv, \"rho\" : rho, \"cv_mae\" : np.mean(MAE)})\n",
    "\n",
    "# params = min(results, key=lambda d: d['cv_mae'])\n",
    "# print(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{'batch_size': 96, 'D': 50, 'learning_rate': 0.1, 'epochs': 80, 'l2_reg_fc': 0.1, 'l2_reg_conv': 0.1, 'rho': 0.99}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the model with the best set of hyperparmeters on the whole training set and validate it on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 96\n",
    "D = 50\n",
    "learning_rate = 0.1\n",
    "epochs = 80\n",
    "l2_reg_fc = 0.1\n",
    "l2_reg_conv = 0.1\n",
    "rho = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_indices = os.listdir(join(datasets_dir, \"GNN_input_data/full\"))\n",
    "# train_indices = [index[:index.rfind(\"_\")] for index in train_indices]\n",
    "# train_indices = list(set([index for index in train_indices if \"train\" in index]))\n",
    "\n",
    "# test_indices = os.listdir(join(datasets_dir, \"GNN_input_data/full\"))\n",
    "# test_indices = [index[:index.rfind(\"_\")] for index in test_indices]\n",
    "# test_indices = list(set([index for index in test_indices if \"test\" in index]))\n",
    "\n",
    "\n",
    "# train_params = {'batch_size': batch_size,\n",
    "#               'folder' :join(datasets_dir, \"GNN_input_data/full\"),\n",
    "#               'list_IDs' : train_indices,\n",
    "#               'shuffle': True}\n",
    "\n",
    "# test_params = {'batch_size': batch_size,\n",
    "#               'folder' :join(datasets_dir, \"GNN_input_data/full\"),\n",
    "#               'list_IDs' : test_indices,\n",
    "#               'shuffle': False}\n",
    "\n",
    "# training_generator = DataGenerator(**train_params)\n",
    "# test_generator = DataGenerator(**test_params)\n",
    "\n",
    "# model = DMPNN_without_extra_features(l2_reg_conv = l2_reg_conv, l2_reg_fc = l2_reg_fc, learning_rate = learning_rate,\n",
    "#                   D = D, N = N, F1 = F1, F2 = F2, F= F, drop_rate = 0.0, ada_rho = rho)\n",
    "\n",
    "# model.fit(training_generator, epochs= epochs, shuffle = True, verbose = 1)\n",
    "# model.save_weights(join(datasets_dir, \"model_weights\", \"saved_model_GNN_best_hyperparameters\"))\n",
    "\n",
    "# pred_test = model.predict(test_generator)\n",
    "# test_indices_y = [int(ind.split(\"_\")[1]) for ind in np.array(test_indices)]\n",
    "# test_y = np.array([test_df[\"Km\"][ind] for ind in test_indices_y])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating substrate representation for every data point in training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DMPNN_without_extra_features(l2_reg_conv = l2_reg_conv, l2_reg_fc = l2_reg_fc, learning_rate = learning_rate,\n",
    "                  D = D, N = N, F1 = F1, F2 = F2, F= F, drop_rate = 0.0, ada_rho = rho)\n",
    "model.load_weights(join(datasets_dir, \"model_weights\", \"saved_model_GNN_best_hyperparameters\"))\n",
    "\n",
    "get_fingerprint_fct = K.function([model.layers[0].input, model.layers[26].input,\n",
    "                                  model.layers[3].input],\n",
    "                                  [model.layers[-10].output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_indices = os.listdir(join(datasets_dir, \"GNN_input_data\", split))\n",
    "# val_indices = [index[:index.rfind(\"_\")] for index in val_indices]\n",
    "# val_indices = list(set([index for index in val_indices if \"val\" in index]))\n",
    "\n",
    "# val_params = {'batch_size': len(val_indices),\n",
    "#               'folder' :join(datasets_dir, \"GNN_input_data\"),\n",
    "#               'list_IDs' : val_indices,\n",
    "#               'shuffle': False}\n",
    "\n",
    "# val_generator = DataGenerator(**val_params)\n",
    "\n",
    "# pred_val = model.predict(val_generator)\n",
    "\n",
    "# val_indices_y = [int(ind.split(\"_\")[1]) for ind in np.array(val_indices)]\n",
    "# test_y = np.array([train_df[\"Km\"][ind] for ind in val_indices_y])\n",
    "\n",
    "# mae = np.median(abs(pred_val - np.reshape(test_y[:len(pred_val)], (-1,1))))\n",
    "# print(mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_folder = join(datasets_dir, \"GNN_input_data\", split)   \n",
    "\n",
    "def get_representation_input(cid_list):\n",
    "    XE = ();\n",
    "    X = ();\n",
    "    A = ();\n",
    "    # Generate data\n",
    "    for cid in cid_list:\n",
    "        try:\n",
    "            X = X + (np.load(join(input_data_folder, cid + '_X.npy')), );\n",
    "            XE = XE + (np.load(join(input_data_folder, cid + '_XE.npy')), );\n",
    "            A = A + (np.load(join(input_data_folder, cid + '_A.npy')), );\n",
    "        except FileNotFoundError: #return zero arrays:\n",
    "            X = X + (np.zeros((N,32)), );\n",
    "            XE = XE + (np.zeros((N,N,F)), );\n",
    "            A = A + (np.zeros((N,N,1)), );\n",
    "    return(XE, X, A)\n",
    "\n",
    "input_data_folder = join(datasets_dir, \"GNN_input_data\", split)   \n",
    "def get_substrate_representations(df, training_set, testing_set, get_fingerprint_fct):\n",
    "    df[\"GNN FP\"] = \"\"\n",
    "    i = 0\n",
    "    n = len(df)\n",
    "    \n",
    "    cid_all = list(df.index)\n",
    "    if training_set == True:\n",
    "        prefix = \"train_\"\n",
    "    elif testing_set == True:\n",
    "        prefix = \"test_\"\n",
    "    else:\n",
    "        prefix = \"val_\"\n",
    "    cid_all = [prefix + str(cid) for cid in cid_all]\n",
    "    \n",
    "    while i*96 <= n:\n",
    "        if (i+1)*96  <= n:\n",
    "            XE, X, A = get_representation_input(cid_all[i*96:(i+1)*96])\n",
    "            representations = get_fingerprint_fct([np.array(XE), np.array(X),np.array(A)])[0]\n",
    "            df[\"GNN FP\"][i*96:(i+1)*96] = list(representations[:, :52])\n",
    "        else:\n",
    "            print(i)\n",
    "            XE, X, A = get_representation_input(cid_all[-min(96,n):])\n",
    "            representations = get_fingerprint_fct([np.array(XE), np.array(X),np.array(A)])[0]\n",
    "            df[\"GNN FP\"][-min(96,n):] = list(representations[:, :52])\n",
    "        i += 1\n",
    "        \n",
    "    ### set all GNN FP-entries with no input matrices to np.nan:\n",
    "    all_X_matrices = os.listdir(input_data_folder)\n",
    "    for ind in df.index:\n",
    "        if prefix +str(ind) +\"_X.npy\" not in all_X_matrices:\n",
    "            df[\"GNN FP\"][ind] = np.nan\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating the GNN representations\n",
    "train_with_rep = get_substrate_representations(df = train_df, training_set = True, testing_set = False,\n",
    "                                                      get_fingerprint_fct = get_fingerprint_fct)\n",
    "test_with_rep = get_substrate_representations(df = test_df, training_set = False, testing_set = True,\n",
    "                                                     get_fingerprint_fct = get_fingerprint_fct)\n",
    "val_with_rep = get_substrate_representations(df = df_validation, training_set = False, testing_set = False,\n",
    "                                                     get_fingerprint_fct = get_fingerprint_fct)\n",
    "\n",
    "#Saving the DataFrames:\n",
    "train_with_rep.to_pickle(join(datasets_dir, \"splits\", split, \"training_data.pkl\"))\n",
    "test_with_rep.to_pickle(join(datasets_dir, \"splits\", split, \"test_data.pkl\"))\n",
    "val_with_rep.to_pickle(join(datasets_dir, \"splits\", split, \"val_data.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for split in [\"full\", \"Arabidopsis\", \"Brassicaceae\", \"wildtype\", \"secondary\"]:\n",
    "#     train_with_rep = pd.read_pickle(join(datasets_dir, \"splits\", split, \"training_data.pkl\"))\n",
    "#     test_with_rep = pd.read_pickle(join(datasets_dir, \"splits\", split, \"test_data.pkl\"))\n",
    "#     val_with_rep = pd.read_pickle(join(datasets_dir, \"splits\", split, \"val_data.pkl\"))\n",
    "#     for ind in train_with_rep.index:\n",
    "#         if train_with_rep[\"difference_fp\"][ind] == \"\" or train_with_rep[\"structural_fp\"][ind] == \"\":\n",
    "#             train_with_rep[\"difference_fp\"][ind] = list(df_reactions[df_reactions[\"Reaction ID\"] == train_with_rep[\"Reaction ID\"][ind]][\"difference_fp\"])[0]\n",
    "#             train_with_rep[\"structural_fp\"][ind] = list(df_reactions[df_reactions[\"Reaction ID\"] == train_with_rep[\"Reaction ID\"][ind]][\"structural_fp\"])[0]\n",
    "#     for ind in test_with_rep.index:\n",
    "#         if test_with_rep[\"difference_fp\"][ind] == \"\" or test_with_rep[\"structural_fp\"][ind] == \"\":\n",
    "#             test_with_rep[\"difference_fp\"][ind] = list(df_reactions[df_reactions[\"Reaction ID\"] == test_with_rep[\"Reaction ID\"][ind]][\"difference_fp\"])[0]\n",
    "#             test_with_rep[\"structural_fp\"][ind] = list(df_reactions[df_reactions[\"Reaction ID\"] == test_with_rep[\"Reaction ID\"][ind]][\"structural_fp\"])[0]\n",
    "#     for ind in val_with_rep.index:\n",
    "#         if val_with_rep[\"difference_fp\"][ind] == \"\" or val_with_rep[\"structural_fp\"][ind] == \"\":\n",
    "#             val_with_rep[\"difference_fp\"][ind] = list(df_reactions[df_reactions[\"Reaction ID\"] == val_with_rep[\"Reaction ID\"][ind]][\"difference_fp\"])[0]\n",
    "#             val_with_rep[\"structural_fp\"][ind] = list(df_reactions[df_reactions[\"Reaction ID\"] == val_with_rep[\"Reaction ID\"][ind]][\"structural_fp\"])[0]            \n",
    "\n",
    "#     train_with_rep.to_pickle(join(datasets_dir, \"splits\", split, \"training_data.pkl\"))\n",
    "#     test_with_rep.to_pickle(join(datasets_dir, \"splits\", split, \"test_data.pkl\"))\n",
    "#     val_with_rep.to_pickle(join(datasets_dir, \"splits\", split, \"val_data.pkl\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
