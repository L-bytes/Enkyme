{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\marle\\anaconda3\\envs\\py37\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os.path import join\n",
    "import os\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import MACCSkeys\n",
    "from ete3 import NCBITaxa\n",
    "import random\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
    "random.seed(10)\n",
    "import torch\n",
    "import esm\n",
    "from bioservices import *\n",
    "from data_preprocessing import *\n",
    "from functions_and_dicts_data_preprocessing_GNN import *\n",
    "from build_GNN import *\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "datasets_dir = \"../../data\"\n",
    "\n",
    "CURRENT_DIR = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading in Sabio data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Sabio data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data points: 3635\n",
      "Number of UniProt IDs: 835\n"
     ]
    }
   ],
   "source": [
    "organism = \"Seed plants\"\n",
    "\n",
    "df_Sabio = pd.read_table(join(\"..\", \"..\", \"data\", \"Km_model_\" + organism + \".tsv\"))\n",
    "\n",
    "df_Sabio[\"Km\"] = df_Sabio[\"Km\"].astype('float')\n",
    "df_Sabio[\"PMID\"] = df_Sabio[\"PMID\"].astype('Int64')\n",
    "\n",
    "df_Sabio[\"substrate_IDs\"] = df_Sabio[\"substrate_IDs\"].str.split('#')\n",
    "df_Sabio[\"product_IDs\"] = df_Sabio[\"product_IDs\"].str.split('#')\n",
    "\n",
    "print(\"Number of data points: %s\" % len(df_Sabio))\n",
    "print(\"Number of UniProt IDs: %s\" % len(set(df_Sabio[\"Uniprot IDs\"])))\n",
    "\n",
    "df_Km = df_Sabio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "droplist = []\n",
    "\n",
    "for ind in df_Km.index:\n",
    "    UID, Km = df_Km[\"Uniprot IDs\"][ind], df_Km[\"Km\"][ind]\n",
    "    help_df = df_Km.loc[df_Km[\"Uniprot IDs\"] == UID].loc[df_Km[\"Km\"] == Km]\n",
    "    \n",
    "    if len(help_df) > 1:\n",
    "        droplist = droplist + list(help_df.index)[1:]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Km.drop(list(set(droplist)), inplace = True)\n",
    "print(\"Dropping %s data points, because they are duplicated.\" % len(set(droplist)))\n",
    "df_Km.reset_index(inplace = True, drop = True)\n",
    "df_Km"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing top and bottom 3% of Km values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_outliers_IQR(df):\n",
    "\n",
    "   q1=df.quantile(0.25)\n",
    "\n",
    "   q3=df.quantile(0.75)\n",
    "\n",
    "   IQR=q3-q1\n",
    "\n",
    "   outliers = df[((df<(q1-1.5*IQR)) | (df>(q3+1.5*IQR)))]\n",
    "\n",
    "   return outliers\n",
    "\n",
    "find_outliers_IQR(df_Km[\"Km\"])\n",
    "\n",
    "print(df_Km['Km'].quantile(0.03),  df_Km['Km'].quantile(0.97))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Km = df_Km[(df_Km['Km'] > df_Km['Km'].quantile(0.03)) & (df_Km['Km'] < df_Km['Km'].quantile(0.97))]\n",
    "df_Km.reset_index(inplace = True, drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "todrop= []\n",
    "\n",
    "for ind in df_Km.index:\n",
    "    UID = df_Km[\"Uniprot IDs\"][ind]\n",
    "    if len(UID.split(';')) > 1:\n",
    "        todrop.append(ind)\n",
    "        print(df_Km[\"Uniprot IDs\"][ind])\n",
    "        print(todrop)\n",
    "        \n",
    "df_Km.drop(todrop, inplace=True)\n",
    "df_Km.reset_index(inplace = True, drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Km[\"substrate_IDs\"] = df_Km[\"substrate_IDs\"].apply(lambda x: (set(x)))\n",
    "df_Km[\"product_IDs\"] = df_Km[\"product_IDs\"].apply(lambda x: (set(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Km.to_pickle(join(\"..\", \"..\", \"data\", \"Km_data_merged.pkl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Assigning IDs to every unique sequence and to every unique reaction in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating DataFrames for all sequences and for all reactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reactions = pd.DataFrame({\"substrates\": df_Km[\"substrate_IDs\"],\n",
    "                            \"products\" : df_Km[\"product_IDs\"]})\n",
    "\n",
    "df_reactions = df_reactions.loc[df_reactions[\"substrates\"] != set([])]\n",
    "df_reactions = df_reactions.loc[df_reactions[\"products\"] != set([])]\n",
    "\n",
    "\n",
    "droplist = []\n",
    "for ind in df_reactions.index:\n",
    "    sub_IDs, pro_IDs = df_reactions[\"substrates\"][ind], df_reactions[\"products\"][ind]\n",
    "    help_df = df_reactions.loc[df_reactions[\"substrates\"] == sub_IDs].loc[df_reactions[\"products\"] == pro_IDs]\n",
    "    if len(help_df):\n",
    "        for ind in list(help_df.index)[1:]:\n",
    "            droplist.append(ind)\n",
    "            \n",
    "df_reactions.drop(list(set(droplist)), inplace = True)\n",
    "df_reactions.reset_index(inplace = True, drop =True)\n",
    "\n",
    "df_reactions[\"Reaction ID\"] = [\"Reaction_\" + str(ind) for ind in df_reactions.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sequences = pd.DataFrame(data = {\"Sequence\" : df_Km[\"Sequence\"].unique()})\n",
    "df_sequences = df_sequences.loc[~pd.isnull(df_sequences[\"Sequence\"])]\n",
    "df_sequences.reset_index(inplace = True, drop = True)\n",
    "df_sequences[\"Sequence ID\"] = [\"Sequence_\" + str(ind) for ind in df_sequences.index]\n",
    "\n",
    "df_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating minimal Km value for each reaction and sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reactions[\"min_Km_for_RID\"] = np.nan\n",
    "for ind in df_reactions.index:\n",
    "    df_reactions[\"min_Km_for_RID\"][ind] = min(df_Km.loc[df_Km[\"substrate_IDs\"] == df_reactions[\"substrates\"][ind]].loc[df_Km[\"product_IDs\"] == df_reactions[\"products\"][ind]][\"Km\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sequences[\"min_Km_for_UID\"] = np.nan\n",
    "for ind in df_sequences.index:\n",
    "    df_sequences[\"min_Km_for_UID\"][ind] = min(df_Km.loc[df_Km[\"Sequence\"] == df_sequences['Sequence'][ind]][\"Km\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating the sum of the molecular weights of all substrates and of all products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reactions[\"MW_frac\"] = np.nan\n",
    "\n",
    "for ind in df_reactions.index:\n",
    "    substrates = list(df_reactions[\"substrates\"][ind])\n",
    "    products = list(df_reactions[\"products\"][ind])\n",
    "    \n",
    "    mw_subs = mw_mets(metabolites = substrates)\n",
    "    mw_pros = mw_mets(metabolites = products)\n",
    "    \n",
    "    if mw_subs == np.nan or mw_pros == np.nan:\n",
    "        df_reactions[\"MW_frac\"][ind] = np.inf\n",
    "    if mw_pros != 0:\n",
    "        df_reactions[\"MW_frac\"][ind] = mw_subs/mw_pros\n",
    "    else:\n",
    "        df_reactions[\"MW_frac\"][ind] = np.inf\n",
    "        \n",
    "df_reactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating enzyme, reaction and substrate features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, alphabet = torch.hub.load(\"facebookresearch/esm:main\", \"esm2_t33_650M_UR50D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#creating model input:\n",
    "df_sequences[\"model_input\"] = [seq[:1022] for seq in df_sequences[\"Sequence\"]]\n",
    "model_input = [(df_sequences[\"Sequence ID\"][ind], df_sequences[\"model_input\"][ind]) for ind in df_sequences.index]\n",
    "seqs = [model_input[i][1] for i in range(len(model_input))]\n",
    "#loading ESM-2 model:\n",
    "print(\".....2(a) Loading ESM-2 model.\")\n",
    "model, alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n",
    "batch_converter = alphabet.get_batch_converter()\n",
    "#convert input into batches:\n",
    "\n",
    "#Calculate ESM-2 representations\n",
    "print(\".....2(b) Calculating enzyme representations.\")\n",
    "df_sequences[\"Enzyme rep\"] = \"\"\n",
    "\n",
    "for ind in df_sequences.index:\n",
    "    batch_labels, batch_strs, batch_tokens = batch_converter([(df_sequences[\"Sequence ID\"][ind], df_sequences[\"model_input\"][ind])])\n",
    "    with torch.no_grad():\n",
    "        results = model(batch_tokens, repr_layers=[33])\n",
    "    df_sequences[\"Enzyme rep\"][ind] = results[\"representations\"][33][0, 1 : len(df_sequences[\"model_input\"][ind]) + 1].mean(0).numpy()\n",
    "    \n",
    "df_sequences.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metabolite_type(met):\n",
    "    if is_KEGG_ID(met):\n",
    "        return(\"KEGG\")\n",
    "    elif is_InChI(met):\n",
    "        return(\"InChI\")\n",
    "    else:\n",
    "        return(\"invalid\")\n",
    "\n",
    "def get_reaction_site_smarts(metabolites):\n",
    "    reaction_site = \"\"\n",
    "    for met in metabolites:\n",
    "        met_type = get_metabolite_type(met)\n",
    "        if met_type == \"KEGG\":\n",
    "            try:\n",
    "                Smarts = Chem.MolToSmarts(Chem.MolFromMolFile(join(CURRENT_DIR, \"data\", \"mol-files\",  met + \".mol\")))\n",
    "            except OSError:\n",
    "                return(np.nan)\n",
    "        elif met_type == \"InChI\":\n",
    "            Smarts = Chem.MolToSmarts(Chem.inchi.MolFromInchi(met))\n",
    "        else:\n",
    "            Smarts = \"invalid\"\n",
    "        reaction_site = reaction_site + \".\" + Smarts\n",
    "    return(reaction_site[1:])\n",
    "\n",
    "\n",
    "def is_KEGG_ID(met):\n",
    "    #a valid KEGG ID starts with a \"C\" or \"D\" followed by a 5 digit number:\n",
    "    if len(met) == 6 and met[0] in [\"C\", \"D\"]:\n",
    "        try:\n",
    "            int(met[1:])\n",
    "            return(True)\n",
    "        except: \n",
    "            pass\n",
    "    return(False)\n",
    "\n",
    "def is_InChI(met):\n",
    "    m = Chem.inchi.MolFromInchi(met,sanitize=False)\n",
    "    if m is None:\n",
    "      return(False)\n",
    "    else:\n",
    "      try:\n",
    "        Chem.SanitizeMol(m)\n",
    "      except:\n",
    "        print('.......Metabolite string \"%s\" is in InChI format but has invalid chemistry' % met)\n",
    "        return(False)\n",
    "    return(True)\n",
    "\n",
    "def convert_fp_to_array(difference_fp_dict):\n",
    "    fp = np.zeros(2048)\n",
    "    for key in difference_fp_dict.keys():\n",
    "        fp[key] = difference_fp_dict[key]\n",
    "    return(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reactions[\"difference_fp\"], df_reactions[\"structural_fp\"],  = \"\", \"\"\n",
    "#each metabolite should be either a KEGG ID, InChI string, or a SMILES:\n",
    "for ind in df_reactions.index:\n",
    "    left_site = get_reaction_site_smarts(df_reactions[\"substrates\"][ind])\n",
    "    right_site = get_reaction_site_smarts(df_reactions[\"products\"][ind])\n",
    "    if not pd.isnull(left_site) and not pd.isnull(right_site):\n",
    "        rxn_forward = AllChem.ReactionFromSmarts(left_site + \">>\" + right_site)\n",
    "        difference_fp = Chem.rdChemReactions.CreateDifferenceFingerprintForReaction(rxn_forward)\n",
    "        difference_fp = convert_fp_to_array(difference_fp.GetNonzeroElements())\n",
    "        df_reactions[\"difference_fp\"][ind] = difference_fp\n",
    "        df_reactions[\"structural_fp\"][ind] = Chem.rdChemReactions.CreateStructuralFingerprintForReaction(rxn_forward).ToBitString()\n",
    "\n",
    "df_reactions.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sequences.to_pickle(join(datasets_dir, \"all_sequences_with_IDs.pkl\"))\n",
    "df_reactions.to_pickle(join(datasets_dir, \"all_reactions_with_IDs.pkl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mapping Sequence and Reaction IDs to Km_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Km = df_Km.merge(df_sequences, on = \"Sequence\", how = \"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reactions.rename(columns = {\"substrates\" : \"substrate_IDs\",\n",
    "                              \"products\" : \"product_IDs\"}, inplace = True)\n",
    "\n",
    "df_Km[\"Reaction ID\"] = np.nan\n",
    "df_Km[\"MW_frac\"] = np.nan\n",
    "df_Km[\"min_Km_for_RID\"] = np.nan\n",
    "df_Km[\"difference_fp\"] = \"\"\n",
    "df_Km[\"structural_fp\"] = \"\"\n",
    "\n",
    "for ind in df_Km.index:\n",
    "    sub_set, pro_set = df_Km[\"substrate_IDs\"][ind], df_Km[\"product_IDs\"][ind]\n",
    "    help_df = df_reactions.loc[df_reactions[\"substrate_IDs\"] == sub_set].loc[df_reactions[\"product_IDs\"] == pro_set]\n",
    "    if len(help_df) == 1:\n",
    "        df_Km[\"Reaction ID\"][ind] = list(help_df[\"Reaction ID\"])[0]\n",
    "        df_Km[\"min_Km_for_RID\"][ind] = list(help_df[\"min_Km_for_RID\"])[0]\n",
    "        df_Km[\"MW_frac\"][ind] = list(help_df[\"MW_frac\"])[0]\n",
    "        df_Km[\"difference_fp\"][ind] = list(help_df[\"difference_fp\"])[0]\n",
    "        df_Km[\"structural_fp\"][ind] = list(help_df[\"structural_fp\"])[0]\n",
    "df_Km.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Km[\"MACCS FP\"] = \"\"\n",
    "\n",
    "for ind in df_Km.index:\n",
    "    substrate = df_Km[\"Main substrate\"][ind]\n",
    "    try:\n",
    "        id = df_Km['Substrate_IDs'][ind][df_Km[\"Substrates\"][ind].split(';').index(substrate)]\n",
    "    except:\n",
    "        for i,s in enumerate(df_Km[\"Substrates\"][ind].split(';')[:-1]):\n",
    "            if substrate in s or s in substrate:\n",
    "                id = list(df_Km['Substrate_IDs'][ind])[i]\n",
    "    if id[0] == \"C\":\n",
    "        try:\n",
    "            mol = Chem.MolFromMolFile(join(datasets_dir,\"mol-files\", id + '.mol'))\n",
    "        except OSError:\n",
    "            None\n",
    "    else:\n",
    "        try:\n",
    "            mol = Chem.inchi.MolFromInchi(id,sanitize=False)\n",
    "        except OSError:\n",
    "            None\n",
    "    if mol is not None:\n",
    "        maccs_fp = MACCSkeys.GenMACCSKeys(mol).ToBitString()\n",
    "        df_Km[\"MACCS FP\"][ind] = maccs_fp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating the minimal Km value for every EC number in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_EC_Km = pd.read_csv(join(\"..\", \"..\", \"data\", \"min_EC_\" + organism + \".tsv\"), sep = \"\\t\", header=0)\n",
    "# df_EC_Km = df_EC_Km.rename(columns={0: \"EC\", 1: \"min_Km\"})\n",
    "\n",
    "for ind in df_EC_Km.index:\n",
    "    try:\n",
    "        Km_min = df_EC_Km[df_EC_Km[\"EC\"] == df_Km[\"ECs\"]][\"min_Km\"]\n",
    "        df_EC_Km[\"min_Km\"][ind] = Km_min\n",
    "        print(ind, Km_min)\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "df_EC_Km.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_EC_Km = pd.read_csv(join(\"..\", \"..\", \"data\", \"min_EC_\" + organism + \".tsv\"), sep = \"\\t\", header=0)\n",
    "\n",
    "df_EC_Km.head(5)\n",
    "df_Km[\"min_Km_for_EC\"] = np.nan\n",
    "\n",
    "for ind in df_Km.index:\n",
    "    EC = df_Km[\"ECs\"][ind]\n",
    "    min_Km = 0\n",
    "    try:\n",
    "        print(EC)\n",
    "        min_Km = df_EC_Km.loc[df_EC_Km[\"EC\"] == EC, \"min_Km\"].iloc[0]\n",
    "        print(min_Km)\n",
    "    except:\n",
    "        pass\n",
    "    if min_Km != 0:\n",
    "        df_Km[\"min_Km_for_EC\"][ind] = min_Km\n",
    "df_Km.to_pickle(join(\"..\", \"..\", \"data\", \"merged_and_grouped_Km_dataset.pkl\"))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Removing outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing non-optimally measured values\n",
    "\n",
    "To ignore $Km$ values that were obtained under non-optimal conditions, we exclude values higher than 10000\\% than the minimal $Km$ value for the same enzyme-reaction combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Km[\"frac_of_min_UID\"] = np.nan\n",
    "df_Km[\"frac_of_min_RID\"] = np.nan\n",
    "df_Km[\"frac_of_min_EC\"] = np.nan\n",
    "\n",
    "for ind in df_Km.index:\n",
    "    df_Km[\"frac_of_min_UID\"][ind] =  df_Km[\"min_Km_for_UID\"][ind]/df_Km[\"Km\"][ind]\n",
    "    df_Km[\"frac_of_min_RID\"][ind] =  df_Km[\"min_Km_for_RID\"][ind]/df_Km[\"Km\"][ind]\n",
    "    df_Km[\"frac_of_min_EC\"][ind] = df_Km[\"min_Km_for_EC\"][ind]/df_Km[\"Km\"][ind]\n",
    "\n",
    "len(df_Km)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(df_Km)\n",
    "\n",
    "df_Km = df_Km.loc[df_Km[\"frac_of_min_UID\"] >= 0.01]\n",
    "df_Km = df_Km.loc[df_Km[\"frac_of_min_RID\"] >= 0.01]\n",
    "\n",
    "df_Km[\"frac_of_min_EC\"].loc[pd.isnull(df_Km[\"frac_of_min_EC\"])] = 1\n",
    "df_Km = df_Km.loc[df_Km[\"frac_of_min_EC\"] <= 10]\n",
    "df_Km = df_Km.loc[df_Km[\"frac_of_min_EC\"] >= 0.01]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"We remove %s data points, because we suspect that these Km values were not measure for the natural reaction \" \\\n",
    "    \"of an enzyme or under non-optimal conditions.\" % (n-len(df_Km)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing data points with reaction queations with uneven fraction of molecular weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(df_Km)\n",
    "\n",
    "df_Km = df_Km.loc[df_Km[\"MW_frac\"] < 3]\n",
    "df_Km = df_Km.loc[df_Km[\"MW_frac\"] > 1/3]\n",
    "\n",
    "print(\"We remove %s data points because the sum of molecular weights of substrates does not match the sum of molecular\" \\\n",
    "      \"weights of the products.\" % (n-len(df_Km)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Size of final Km dataset: %s\" % len(df_Km))\n",
    "df_Km.to_pickle(join(\"..\", \"..\", \"data\", \"final_Km_dataset_\" + organism + \".pkl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Preparing dataset and splitting into train-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Km = pd.read_pickle(join(\"..\", \"..\", \"data\", \"final_Km_dataset_\" + organism + \".pkl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making input for GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for ind in df_Km.index:\n",
    "#     substrate = df_Km[\"Main Substrate\"][ind]\n",
    "#     try:\n",
    "#         id = list(df_Km['substrate_IDs'][ind])[df_Km[\"Substrates\"][ind].split(';').index(substrate)]\n",
    "#     except:\n",
    "#         for i,s in enumerate(df_Km[\"Substrates\"][ind].split(';')[:-1]):\n",
    "#             if substrate in s or s in substrate:\n",
    "#                 id = list(df_Km['substrate_IDs'][ind])[i]\n",
    "#     df_Km[\"Main Substrate\"][ind] = id\n",
    "\n",
    "inchi_ids = {}\n",
    "for i, element in enumerate(df_Km[\"Main Substrate\"]):\n",
    "    if element[0] != 'C' and element not in inchi_ids.keys():\n",
    "        inchi_ids[element] = str(i)\n",
    "        mol = Chem.inchi.MolFromInchi(element)\n",
    "        if not mol is None:\n",
    "            calculate_atom_and_bond_feature_vectors(mol, str(i))\n",
    "        Chem.rdmolfiles.MolToMolFile(Chem.inchi.MolFromInchi(element), join(datasets_dir,\"mol-files\", str(i) + \".mol\")  )  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting glucosinolates into validation dataset\n",
    "\n",
    "Search UniProt for GO term related to glucosionalte metabolic process, download file as .tsv and filter dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "glucosinolates = pd.read_table(join(datasets_dir,\"glucosinolates.tsv\"))[\"Entry\"].tolist()\n",
    "df_validation = df_Km[df_Km[\"Uniprot IDs\"].isin(glucosinolates)]\n",
    "df_validation.reset_index(inplace=True, drop = True)\n",
    "df_Km = df_Km[~df_Km[\"Uniprot IDs\"].isin(glucosinolates)]\n",
    "df_Km.reset_index(inplace=True, drop = True)\n",
    "split = \"full\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If training-testing with only Arabidopsis data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_Km = df_Km[df_Km[\"Organism\"] == 'Arabidopsis thaliana']\n",
    "# df_Km.reset_index(inplace=True, drop = True)\n",
    "# split = \"Arabidopsis\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If training-testing with only Brassicaceae data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ncbi = NCBITaxa()\n",
    "\n",
    "# organisms = {}\n",
    "\n",
    "# def is_brassicaceae(org):\n",
    "#     try:\n",
    "#         tax_id = ncbi.get_name_translator([org])[org][0]\n",
    "#         lineage = ncbi.get_lineage(tax_id)\n",
    "#         if 3700 not in lineage:\n",
    "#             return(False)\n",
    "#         else:\n",
    "#             return(True)\n",
    "#     except KeyError:\n",
    "#         return(False)\n",
    "    \n",
    "# for org in df_Km[\"Organism\"].tolist():\n",
    "#     if org not in organisms.keys():\n",
    "#         organisms[org] = is_brassicaceae(org)\n",
    "\n",
    "# df_Km = df_Km[df_Km[\"Organism\"].isin([key for key, value in organisms.items() if value is True])]\n",
    "# df_Km.reset_index(inplace=True, drop = True)\n",
    "# split = \"Brassicaceae\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If training-testing only with wildtype data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_Km = df_Km[df_Km[\"Type\"] == \"wildtype\"]\n",
    "# df_Km.reset_index(inplace=True, drop = True)\n",
    "# split = \"wildtype\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If training-testing only with secondary metabolite data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "secondary = pd.read_table(join(datasets_dir,\"secondary_metabolites.tsv\"))[\"Entry\"].tolist()\n",
    "df_kcat = df_Km[df_Km[\"Uniprot IDs\"].isin(secondary)]\n",
    "df_kcat.reset_index(inplace=True, drop = True)\n",
    "split = \"secondary\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.mkdir(join(datasets_dir, \"splits\", split))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating arithmetic mean for Km values of same enzyme-reaction-substrate combination-pH-temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_new = pd.DataFrame(data = {\"Reaction ID\" : df_Km[\"Reaction ID\"],\n",
    "#                                   \"Sequence ID\" : df_Km[\"Sequence ID\"],\n",
    "#                                   \"Temperature\" : df_Km[\"Temperature\"],\n",
    "#                                     \"pH\" : df_Km[\"pH\"],\n",
    "#                                  \"Type\": df_Km[\"Type\"],\n",
    "#                              \"MACCS FP\" : df_Km[\"MACCS FP\"]})\n",
    "\n",
    "# df_new.drop_duplicates(inplace = True)\n",
    "# df_new.reset_index(inplace = True, drop = True)\n",
    "\n",
    "# df_new[\"Km_values\"], df_new[\"Uniprot IDs\"], df_new[\"ECs\"], df_new[\"Substrates\"], df_new[\"Products\"], df_new[\"ESM2\"], df_new[\"Sequence\"], df_new[\"difference_fp\"], df_new[\"structural_fp\"] = \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"\n",
    "\n",
    "# for ind in df_new.index:\n",
    "#     RID, SID, Temp, pH, Type, MSubstrate = df_new[\"Reaction ID\"][ind], df_new[\"Sequence ID\"][ind], df_new[\"Temperature\"][ind], df_new[\"pH\"][ind], df_new[\"Type\"][ind], df_new[\"MACCS FP\"][ind]\n",
    "#     help_df = df_Km.loc[df_Km[\"Reaction ID\"] \n",
    "#                                  == RID].loc[df_Km[\"Sequence ID\"] \n",
    "#                                              == SID].loc[df_Km[\"Temperature\"] \n",
    "#                                                          == Temp].loc[df_Km[\"pH\"] \n",
    "#                                                                       == pH].loc[df_Km[\"Type\"] \n",
    "#                                                                                  == Type].loc[df_Km[\"MACCS FP\"] \n",
    "#                                                                                               == MSubstrate]\n",
    "#     print(help_df)\n",
    "#     df_new[\"ECs\"][ind] = list(help_df[\"ECs\"])\n",
    "#     df_new[\"Km_values\"][ind] = list(help_df[\"Km\"])\n",
    "#     df_new[\"Uniprot IDs\"][ind] = list(help_df[\"Uniprot IDs\"])\n",
    "#     df_new[\"Sequence\"][ind] = help_df[\"Sequence\"].values[0]\n",
    "#     df_new[\"ESM2\"][ind] = help_df[\"Enzyme rep\"].values[0]\n",
    "#     df_new[\"difference_fp\"][ind], df_new[\"structural_fp\"][ind] = help_df[\"difference_fp\"].values[0], help_df[\"structural_fp\"].values[0]\n",
    "#     df_new[\"Substrates\"][ind], df_new[\"Products\"][ind] = help_df[\"Substrates\"].values[0], help_df[\"Products\"].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_new2 = pd.DataFrame(data = {\"Reaction ID\" : df_validation[\"Reaction ID\"],\n",
    "#                                   \"Sequence ID\" : df_validation[\"Sequence ID\"],\n",
    "#                                   \"Temperature\" : df_validation[\"Temperature\"],\n",
    "#                                     \"pH\" : df_validation[\"pH\"],\n",
    "#                                   \"Type\" : df_validation[\"Type\"],\n",
    "#                                   \"MACCS FP\" : df_validation[\"MACCS FP\"]})\n",
    "\n",
    "# df_new2.drop_duplicates(inplace = True)\n",
    "# df_new2.reset_index(inplace = True, drop = True)\n",
    "\n",
    "# df_new2[\"Km_values\"], df_new2[\"Uniprot IDs\"], df_new2[\"ECs\"], df_new2[\"Organisms\"], df_new2[\"Substrates\"], df_new2[\"Products\"], df_new2[\"ESM2\"], df_new2[\"Sequence\"], df_new2[\"difference_fp\"], df_new2[\"structural_fp\"] = \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"\n",
    "\n",
    "# for ind in df_new2.index:\n",
    "#     RID, SID, Temp, pH, Type, MSubstrate = df_new2[\"Reaction ID\"][ind], df_new2[\"Sequence ID\"][ind], df_new2[\"Temperature\"][ind], df_new2[\"pH\"][ind], df_new2[\"Type\"][ind], df_new2[\"MACCS FP\"][ind]\n",
    "#     help_df = df_validation.loc[df_validation[\"Reaction ID\"] \n",
    "#                               == RID].loc[df_validation[\"Sequence ID\"] \n",
    "#                                           == SID].loc[df_validation[\"Temperature\"] \n",
    "#                                                       == Temp].loc[df_validation[\"pH\"] \n",
    "#                                                                     == pH].loc[df_validation[\"Type\"] \n",
    "#                                                                               == Type].loc[df_validation[\"MACCS FP\"] \n",
    "#                                                                                                             == MSubstrate]\n",
    "#     df_new2[\"ECs\"][ind] = list(help_df[\"ECs\"])\n",
    "#     df_new2[\"Km_values\"][ind] = list(help_df[\"Km\"])\n",
    "#     df_new2[\"Uniprot IDs\"][ind] = list(help_df[\"Uniprot IDs\"])\n",
    "#     df_new2[\"Organisms\"][ind] = list(help_df[\"Organism\"])\n",
    "#     df_new2[\"Type\"][ind]\n",
    "#     df_new2[\"Sequence\"][ind] = help_df[\"Sequence\"].values[0]\n",
    "#     df_new2[\"ESM2\"][ind] = help_df[\"Enzyme rep\"].values[0]\n",
    "#     df_new2[\"difference_fp\"][ind], df_new2[\"structural_fp\"][ind] = help_df[\"difference_fp\"].values[0], help_df[\"structural_fp\"].values[0]\n",
    "#     df_new2[\"Substrates\"][ind], df_new2[\"Products\"][ind] = help_df[\"Substrates\"].values[0], help_df[\"Products\"].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_Km = df_new\n",
    "# df_validation = df_new2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_Km[\"geomean_Km\"] = np.nan\n",
    "# for ind in df_Km.index:\n",
    "#     all_Km = np.array(df_Km[\"Km_values\"][ind]).astype(float)\n",
    "    # min_Km = min(all_Km)\n",
    "    # all_Km_top = [Km for Km in all_Km  if min_Km/Km >= 0.01]\n",
    "    # df_arabidopsis[\"geomean_Km\"][ind] = np.mean((all_Km_top))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_validation[\"geomean_Km\"] = np.nan\n",
    "# for ind in df_validation.index:\n",
    "#     all_Km = np.array(df_validation[\"Km_values\"][ind]).astype(float)\n",
    "#     min_Km = min(all_Km)\n",
    "#     all_Km_top = [Km for Km in all_Km  if min_Km/Km >= 0.01]\n",
    "#     df_validation[\"geomean_Km\"][ind] = np.mean((all_Km_top))\n",
    "    \n",
    "# df_validation.to_pickle(join(datasets_dir, \"splits\", \"validation_%s.pkl\" %organism))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting into train-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set size: 475\n",
      "Training set size: 1652\n",
      "Size of test set in percent: 22.0\n"
     ]
    }
   ],
   "source": [
    "df = df_Km.copy()\n",
    "df = df.sample(frac = 1, random_state = 123)\n",
    "df.reset_index(drop= True, inplace = True)\n",
    "\n",
    "train_df, test_df = split_dataframe_enzyme(frac = 5, df = df.copy())\n",
    "print(\"Test set size: %s\" % len(test_df))\n",
    "print(\"Training set size: %s\" % len(train_df))\n",
    "print(\"Size of test set in percent: %s\" % np.round(100*len(test_df)/ (len(test_df) + len(train_df))))\n",
    "\n",
    "train_df.reset_index(inplace = True, drop = True)\n",
    "test_df.reset_index(inplace = True, drop = True)\n",
    "\n",
    "train_df.to_pickle(join(datasets_dir, \"splits\", split, \"train_df_Km_%s.pkl\" %organism))\n",
    "test_df.to_pickle(join(datasets_dir, \"splits\", split, \"test_df_Km_%s.pkl\" %organism))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting CV folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1283 369\n",
      "980 303\n",
      "726 254\n",
      "374 352\n"
     ]
    }
   ],
   "source": [
    "data_train2 = train_df.copy()\n",
    "data_train2[\"index\"] = list(data_train2.index)\n",
    "\n",
    "data_train2, df_fold = split_dataframe_enzyme(df = data_train2, frac=5)\n",
    "indices_fold1 = list(df_fold[\"index\"])\n",
    "print(len(data_train2), len(indices_fold1))\n",
    "\n",
    "data_train2, df_fold = split_dataframe_enzyme(df = data_train2, frac=4)\n",
    "indices_fold2 = list(df_fold[\"index\"])\n",
    "print(len(data_train2), len(indices_fold2))\n",
    "\n",
    "data_train2, df_fold = split_dataframe_enzyme(df = data_train2, frac=3)\n",
    "indices_fold3 = list(df_fold[\"index\"])\n",
    "print(len(data_train2), len(indices_fold3))\n",
    "\n",
    "data_train2, df_fold = split_dataframe_enzyme(df = data_train2, frac=2)\n",
    "indices_fold4 = list(df_fold[\"index\"])\n",
    "indices_fold5 = list(data_train2[\"index\"])\n",
    "print(len(data_train2), len(indices_fold4))\n",
    "\n",
    "\n",
    "fold_indices = [indices_fold1, indices_fold2, indices_fold3, indices_fold4, indices_fold5]\n",
    "\n",
    "CV_train_indices = [[], [], [], [], []]\n",
    "CV_test_indices = [[], [], [], [], []]\n",
    "\n",
    "for i in range(5):\n",
    "    for j in range(5):\n",
    "        if i != j:\n",
    "            CV_train_indices[i] = CV_train_indices[i] + fold_indices[j]\n",
    "    CV_test_indices[i] = fold_indices[i]\n",
    "    \n",
    "    \n",
    "np.save(join(datasets_dir, \"splits\", split, \"CV_train_indices_%s\" %organism), CV_train_indices)\n",
    "np.save(join(datasets_dir, \"splits\", split, \"CV_test_indices_%s\" %organism), CV_test_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Building GNN for substrate representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.mkdir(join(datasets_dir, \"GNN_input_data\", split))\n",
    "\n",
    "for ind in train_df.index:\n",
    "    calculate_and_save_input_matrixes(inchi_ids, sample_ID = \"train_\" + str(ind), df = train_df,\n",
    "                                      save_folder = join(datasets_dir, \"GNN_input_data\", split))\n",
    "    \n",
    "for ind in test_df.index:\n",
    "    calculate_and_save_input_matrixes(inchi_ids, sample_ID = \"test_\" + str(ind), df = test_df,\n",
    "                                      save_folder = join(datasets_dir, \"GNN_input_data\", split))\n",
    "    \n",
    "for ind in df_validation.index:\n",
    "    calculate_and_save_input_matrixes(inchi_ids, sample_ID = \"val_\" + str(ind), df = df_validation,\n",
    "                                    save_folder = join(datasets_dir, \"GNN_input_data\", split))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices = os.listdir(join(datasets_dir, \"GNN_input_data\", split))\n",
    "train_indices = [index[:index.rfind(\"_\")] for index in train_indices]\n",
    "train_indices = list(set([index for index in train_indices if \"train\" in index]))\n",
    "\n",
    "test_indices = os.listdir(join(datasets_dir, \"GNN_input_data\", split))\n",
    "test_indices = [index[:index.rfind(\"_\")] for index in test_indices]\n",
    "test_indices = list(set([index for index in test_indices if \"test\" in index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyper-parameter optimization with CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_grid = {'batch_size': [96,96,128],\n",
    "#                 'D': [50,100],\n",
    "#                 'learning_rate': [0.01, 0.1],\n",
    "#                 'epochs': [30,50,80],\n",
    "#                 'l2_reg_fc' : [0.01, 0.1, 1],\n",
    "#                 'l2_reg_conv': [0.01, 0.1, 1],\n",
    "#                 'rho': [0.9, 0.95, 0.99]}\n",
    "\n",
    "# params_list = [(batch_size, D, learning_rate, epochs, l2_reg_fc, l2_reg_conv, rho) for batch_size in param_grid['batch_size'] for D in param_grid[\"D\"] for learning_rate in param_grid['learning_rate']\n",
    "#                 for epochs in param_grid['epochs'] for l2_reg_fc in param_grid['l2_reg_fc'] for l2_reg_conv in param_grid['l2_reg_conv'] for rho in param_grid[\"rho\"]]\n",
    "\n",
    "# params_list = random.sample(params_list, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count = 0\n",
    "# results=[]\n",
    "\n",
    "# for params in params_list:\n",
    "\n",
    "#     batch_size, D, learning_rate, epochs, l2_reg_fc, l2_reg_conv, rho = params\n",
    "#     count +=1\n",
    "#     MAE = []\n",
    "\n",
    "#     for i in range(5):\n",
    "#         train_index, test_index  = CV_train_indices[i], CV_test_indices[i]\n",
    "#         train_index = [ind for ind in train_indices if int(ind.split(\"_\")[1]) in train_index]\n",
    "#         test_index = [ind for ind in train_indices if int(ind.split(\"_\")[1]) in test_index]\n",
    "\n",
    "#         train_params = {'batch_size': batch_size,\n",
    "#                     'folder' :join(datasets_dir, \"GNN_input_data\"),\n",
    "#                     'list_IDs' : np.array(train_index),\n",
    "#                     'shuffle': True}\n",
    "\n",
    "#         test_params = {'batch_size': min(batch_size,len(test_index)),\n",
    "#                     'folder' : join(datasets_dir, \"GNN_input_data\"),\n",
    "#                     'list_IDs' : np.array(test_index),\n",
    "#                     'shuffle': False}\n",
    "\n",
    "#         training_generator = DataGenerator(**train_params)\n",
    "#         test_generator = DataGenerator(**test_params)\n",
    "\n",
    "\n",
    "#         model = DMPNN_without_extra_features(l2_reg_conv = l2_reg_conv, l2_reg_fc = l2_reg_fc, learning_rate = learning_rate,\n",
    "#                         D = D, N = N, F1 = F1, F2 = F2, F= F, drop_rate = 0.0, ada_rho = rho)\n",
    "#         model.fit(training_generator, epochs= epochs, shuffle = True, verbose = 1)\n",
    "\n",
    "#         #get test_y:\n",
    "#         test_indices_y = [int(ind.split(\"_\")[1]) for ind in train_indices if ind in test_index]\n",
    "#         test_y = np.array([train_df[\"Km\"][ind] for ind in test_indices_y])\n",
    "\n",
    "#         pred_test = model.predict(test_generator)\n",
    "#         mae = np.median(abs(pred_test - np.reshape(test_y[:len(pred_test)], (-1,1))))\n",
    "#         print(mae)\n",
    "#         MAE.append(mae)\n",
    "\n",
    "#     results.append({\"batch_size\" : batch_size, \"D\" : D , \"learning_rate\" : learning_rate, \"epochs\" : epochs,\n",
    "#                     \"l2_reg_fc\" : l2_reg_fc, \"l2_reg_conv\" : l2_reg_conv, \"rho\" : rho, \"cv_mae\" : np.mean(MAE)})\n",
    "\n",
    "# params = min(results, key=lambda d: d['cv_mae'])\n",
    "# print(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{'batch_size': 96, 'D': 50, 'learning_rate': 0.1, 'epochs': 80, 'l2_reg_fc': 0.1, 'l2_reg_conv': 0.1, 'rho': 0.99, 'cv_mae': 0.005500673513143063}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the model with the best set of hyperparmeters on the whole training set and validate it on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 96\n",
    "D = 50\n",
    "learning_rate = 0.1\n",
    "epochs = 80\n",
    "l2_reg_fc = 0.1\n",
    "l2_reg_conv = 0.1\n",
    "rho = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_indices = os.listdir(join(datasets_dir, \"GNN_input_data\"))\n",
    "# train_indices = [index[:index.rfind(\"_\")] for index in train_indices]\n",
    "# train_indices = list(set([index for index in train_indices if \"train\" in index]))\n",
    "\n",
    "# test_indices = os.listdir(join(datasets_dir, \"GNN_input_data\"))\n",
    "# test_indices = [index[:index.rfind(\"_\")] for index in test_indices]\n",
    "# test_indices = list(set([index for index in test_indices if \"test\" in index]))\n",
    "\n",
    "\n",
    "# train_params = {'batch_size': batch_size,\n",
    "#               'folder' :join(datasets_dir, \"GNN_input_data\"),\n",
    "#               'list_IDs' : train_indices,\n",
    "#               'shuffle': True}\n",
    "\n",
    "# test_params = {'batch_size': batch_size,\n",
    "#               'folder' :join(datasets_dir, \"GNN_input_data\"),\n",
    "#               'list_IDs' : test_indices,\n",
    "#               'shuffle': False}\n",
    "\n",
    "# training_generator = DataGenerator(**train_params)\n",
    "# test_generator = DataGenerator(**test_params)\n",
    "\n",
    "# model = DMPNN_without_extra_features(l2_reg_conv = l2_reg_conv, l2_reg_fc = l2_reg_fc, learning_rate = learning_rate,\n",
    "#                   D = D, N = N, F1 = F1, F2 = F2, F= F, drop_rate = 0.0, ada_rho = rho)\n",
    "\n",
    "# model.fit(training_generator, epochs= epochs, shuffle = True, verbose = 1)\n",
    "# model.save_weights(join(datasets_dir, \"model_weights\", \"saved_model_GNN_best_hyperparameters\"))\n",
    "\n",
    "# pred_test = model.predict(test_generator)\n",
    "# test_indices_y = [int(ind.split(\"_\")[1]) for ind in np.array(test_indices)]\n",
    "# test_y = np.array([test_df[\"Km\"][ind] for ind in test_indices_y])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating substrate representation for every data point in training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x2ae98b587c8>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DMPNN_without_extra_features(l2_reg_conv = l2_reg_conv, l2_reg_fc = l2_reg_fc, learning_rate = learning_rate,\n",
    "                  D = D, N = N, F1 = F1, F2 = F2, F= F, drop_rate = 0.0, ada_rho = rho)\n",
    "model.load_weights(join(datasets_dir, \"model_weights\", \"saved_model_GNN_best_hyperparameters\"))\n",
    "\n",
    "# get_fingerprint_fct = K.function([model.layers[0].input, model.layers[26].input,\n",
    "#                                   model.layers[3].input],\n",
    "#                                   [model.layers[-10].output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n",
      "0.0054108362305164345\n"
     ]
    }
   ],
   "source": [
    "val_indices = os.listdir(join(datasets_dir, \"GNN_input_data\", split))\n",
    "val_indices = [index[:index.rfind(\"_\")] for index in val_indices]\n",
    "val_indices = list(set([index for index in val_indices if \"val\" in index]))\n",
    "\n",
    "val_params = {'batch_size': len(val_indices),\n",
    "              'folder' :join(datasets_dir, \"GNN_input_data\"),\n",
    "              'list_IDs' : val_indices,\n",
    "              'shuffle': False}\n",
    "\n",
    "val_generator = DataGenerator(**val_params)\n",
    "\n",
    "pred_val = model.predict(val_generator)\n",
    "\n",
    "val_indices_y = [int(ind.split(\"_\")[1]) for ind in np.array(val_indices)]\n",
    "test_y = np.array([train_df[\"Km\"][ind] for ind in val_indices_y])\n",
    "\n",
    "mae = np.median(abs(pred_val - np.reshape(test_y[:len(pred_val)], (-1,1))))\n",
    "print(mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_folder = join(datasets_dir, \"GNN_input_data\", split)   \n",
    "\n",
    "def get_representation_input(cid_list):\n",
    "    XE = ();\n",
    "    X = ();\n",
    "    A = ();\n",
    "    # Generate data\n",
    "    for cid in cid_list:\n",
    "        try:\n",
    "            X = X + (np.load(join(input_data_folder, cid + '_X.npy')), );\n",
    "            XE = XE + (np.load(join(input_data_folder, cid + '_XE.npy')), );\n",
    "            A = A + (np.load(join(input_data_folder, cid + '_A.npy')), );\n",
    "        except FileNotFoundError: #return zero arrays:\n",
    "            X = X + (np.zeros((N,32)), );\n",
    "            XE = XE + (np.zeros((N,N,F)), );\n",
    "            A = A + (np.zeros((N,N,1)), );\n",
    "    return(XE, X, A)\n",
    "\n",
    "input_data_folder = join(datasets_dir, \"GNN_input_data\", split)   \n",
    "def get_substrate_representations(df, training_set, testing_set, get_fingerprint_fct):\n",
    "    df[\"GNN FP\"] = \"\"\n",
    "    i = 0\n",
    "    n = len(df)\n",
    "    \n",
    "    cid_all = list(df.index)\n",
    "    if training_set == True:\n",
    "        prefix = \"train_\"\n",
    "    elif testing_set == True:\n",
    "        prefix = \"test_\"\n",
    "    else:\n",
    "        prefix = \"val_\"\n",
    "    cid_all = [prefix + str(cid) for cid in cid_all]\n",
    "    \n",
    "    while i*96 <= n:\n",
    "        if (i+1)*96  <= n:\n",
    "            XE, X, A = get_representation_input(cid_all[i*96:(i+1)*96])\n",
    "            representations = get_fingerprint_fct([np.array(XE), np.array(X),np.array(A)])[0]\n",
    "            df[\"GNN FP\"][i*96:(i+1)*96] = list(representations[:, :52])\n",
    "        else:\n",
    "            print(i)\n",
    "            XE, X, A = get_representation_input(cid_all[-min(96,n):])\n",
    "            representations = get_fingerprint_fct([np.array(XE), np.array(X),np.array(A)])[0]\n",
    "            df[\"GNN FP\"][-min(96,n):] = list(representations[:, :52])\n",
    "        i += 1\n",
    "        \n",
    "    ### set all GNN FP-entries with no input matrices to np.nan:\n",
    "    all_X_matrices = os.listdir(input_data_folder)\n",
    "    for ind in df.index:\n",
    "        if prefix +str(ind) +\"_X.npy\" not in all_X_matrices:\n",
    "            df[\"GNN FP\"][ind] = np.nan\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "4\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "#Calculating the GNN representations\n",
    "train_with_rep = get_substrate_representations(df = train_df, training_set = True, testing_set = False,\n",
    "                                                      get_fingerprint_fct = get_fingerprint_fct)\n",
    "test_with_rep = get_substrate_representations(df = test_df, training_set = False, testing_set = True,\n",
    "                                                     get_fingerprint_fct = get_fingerprint_fct)\n",
    "val_with_rep = get_substrate_representations(df = df_validation, training_set = False, testing_set = False,\n",
    "                                                     get_fingerprint_fct = get_fingerprint_fct)\n",
    "\n",
    "#Saving the DataFrames:\n",
    "train_with_rep.to_pickle(join(datasets_dir, \"splits\", split, \"training_data.pkl\"))\n",
    "test_with_rep.to_pickle(join(datasets_dir, \"splits\", split, \"test_data.pkl\"))\n",
    "val_with_rep.to_pickle(join(datasets_dir, \"splits\", split, \"val_data.pkl\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
