{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import os\n",
    "from os.path import join\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy import stats\n",
    "import xgboost as xgb\n",
    "import random\n",
    "# random.seed(10)\n",
    "from hyperopt import fmin, rand, hp, Trials, tpe\n",
    "rstate = np.random.default_rng(42)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import matplotlib as mpl\n",
    "\n",
    "datasets_dir = \"../../data\"\n",
    "# plt.style.use('CCB_plot_style_0v4.mplstyle')\n",
    "# c_styles      = mpl.rcParams['axes.prop_cycle'].by_key()['color']   # fetch the defined color styles\n",
    "# high_contrast = ['#004488', '#DDAA33', '#BB5566', '#000000']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading training, testing and validation data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(566, 153, 21)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split = \"Arabidopsis\"\n",
    "\n",
    "data_train = pd.read_pickle(join(datasets_dir, \"splits\", split, \"training_data.pkl\"))\n",
    "data_test = pd.read_pickle(join(datasets_dir, \"splits\", split, \"test_data.pkl\"))\n",
    "data_val = pd.read_pickle(join(datasets_dir, \"splits\", split, \"val_data.pkl\"))\n",
    "\n",
    "# data_train[\"geomean_Km\"] = np.log10(data_train[\"geomean_Km\"])\n",
    "# data_test[\"geomean_Km\"] = np.log10(data_test[\"geomean_Km\"])\n",
    "\n",
    "data_train[\"log10_Km\"] = np.log10(data_train[\"Km\"])\n",
    "data_test[\"log10_Km\"] = np.log10(data_test[\"Km\"])\n",
    "data_val[\"log10_Km\"] = np.log10(data_val[\"Km\"])\n",
    "\n",
    "data_train.rename(columns = {\"Enzyme rep\" : \"ESM2\"}, inplace = True)\n",
    "data_test.rename(columns = {\"Enzyme rep\" : \"ESM2\"}, inplace = True)\n",
    "data_val.rename(columns = {\"Enzyme rep\" : \"ESM2\"}, inplace = True)\n",
    "\n",
    "data_train['Temperature'] = data_train['Temperature'].replace('-', np.nan)\n",
    "data_test['Temperature'] = data_test['Temperature'].replace('-', np.nan)\n",
    "data_val['Temperature'] = data_val['Temperature'].replace('-', np.nan)\n",
    "data_train['pH'] = data_train['pH'].replace('-', np.nan)\n",
    "data_test['pH'] = data_test['pH'].replace('-', np.nan)\n",
    "data_val['pH'] = data_val['pH'].replace('-', np.nan)\n",
    "data_train['Type'] = data_train['Type'].replace('wildtype', 1)\n",
    "data_train['Type'] = data_train['Type'].replace('mutant', 2)\n",
    "data_test['Type'] = data_test['Type'].replace('wildtype', 1)\n",
    "data_test['Type'] = data_test['Type'].replace('mutant', 2)\n",
    "data_val['Type'] = data_val['Type'].replace('wildtype', 1)\n",
    "data_val['Type'] = data_val['Type'].replace('mutant', 2)\n",
    "\n",
    "data_train['MACCS FP'] = data_train['MACCS FP'].astype(str)\n",
    "data_test['MACCS FP'] = data_test['MACCS FP'].astype(str)\n",
    "data_val['MACCS FP'] = data_val['MACCS FP'].astype(str)\n",
    "\n",
    "len(data_train), len(data_test), len(data_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices = list(np.load(join(datasets_dir, \"splits\", split, \"CV_train_indices_Seed plants.npy\"), allow_pickle = True))\n",
    "test_indices = list(np.load(join(datasets_dir, \"splits\", split, \"CV_test_indices_Seed plants.npy\"), allow_pickle = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "data_test = data_test[~data_test['GNN FP'].isnull()]\n",
    "\n",
    "nan_rows = data_train[data_train['GNN FP'].apply(lambda x: not isinstance(x, np.ndarray))]\n",
    "\n",
    "# Get the indices of these rows\n",
    "indices_with_nan = nan_rows.index.tolist()\n",
    "# indices_with_nan.reverse()\n",
    "print(indices_with_nan)\n",
    "\n",
    "for ind, sub_list in enumerate(train_indices):\n",
    "    for elem in sub_list:\n",
    "        if elem in indices_with_nan:\n",
    "            sub_list.remove(elem)\n",
    "\n",
    "for ind, sub_list in enumerate(train_indices):\n",
    "    for num in indices_with_nan:\n",
    "        for i, elem in enumerate(sub_list):\n",
    "            if elem > num:\n",
    "                train_indices[ind][i] = elem-1\n",
    "\n",
    "for ind, sub_list in enumerate(test_indices):\n",
    "    for elem in sub_list:\n",
    "        if elem in indices_with_nan:\n",
    "            sub_list.remove(elem)\n",
    "\n",
    "for ind, sub_list in enumerate(test_indices):\n",
    "    for num in indices_with_nan:\n",
    "        for i, elem in enumerate(sub_list):\n",
    "            if elem > num:\n",
    "                test_indices[ind][i] = elem-1  \n",
    "\n",
    "data_train = data_train[data_train['GNN FP'].apply(lambda x: isinstance(x, np.ndarray))]\n",
    "data_train.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Training a model with enzyme and main substrate information (ESM-2/MACCS) + Temperature + pH :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Creating input matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = np.array(list(data_train[\"ESM2\"]))\n",
    "train_X = np.concatenate([train_X, np.array(list(data_train['GNN FP'])), np.array(list(data_train[\"Temperature\"]))[:, np.newaxis], np.array(list(data_train[\"pH\"]))[:, np.newaxis]], axis = 1)\n",
    "train_Y = np.array(list(data_train[\"log10_Km\"]))\n",
    "\n",
    "test_X = np.array(list(data_test[\"ESM2\"]))\n",
    "test_X = np.concatenate([test_X, np.array(list(data_test['GNN FP'])), np.array(list(data_test[\"Temperature\"]))[:, np.newaxis], np.array(list(data_test[\"pH\"]))[:, np.newaxis]], axis = 1)\n",
    "test_Y = np.array(list(data_test[\"log10_Km\"]))\n",
    "\n",
    "train_X = train_X.astype(float)\n",
    "test_X = test_X.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Hyperparameter optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [1:28:31<00:00, 265.58s/trial, best loss: -0.414221503315572]\n",
      "{'learning_rate': 0.1, 'max_delta_step': 2.0, 'max_depth': 4.0, 'min_child_weight': 6.0, 'num_rounds': 1000, 'reg_alpha': 0.2, 'reg_lambda': 0.4}\n"
     ]
    }
   ],
   "source": [
    "def cross_validation_mse_gradient_boosting(param):\n",
    "    num_round = param[\"num_rounds\"]\n",
    "    del param[\"num_rounds\"]\n",
    "    param[\"max_depth\"] = int(np.round(param[\"max_depth\"]))\n",
    "    # param[\"device\"] = \"cuda\"\n",
    "    param[\"tree_method\"] = \"hist\"\n",
    "    param[\"sampling_method\"] = \"gradient_based\"\n",
    "    \n",
    "    MSE = []\n",
    "    R2 = []\n",
    "    for i in range(5):\n",
    "        train_index, test_index  = train_indices[i], test_indices[i]\n",
    "        dtrain = xgb.DMatrix(train_X[train_index], label = train_Y[train_index])\n",
    "        dvalid = xgb.DMatrix(train_X[test_index])\n",
    "        bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "        y_valid_pred = bst.predict(dvalid)\n",
    "        MSE.append(np.mean(abs(np.reshape(train_Y[test_index], (-1)) - y_valid_pred)**2))\n",
    "        R2.append(r2_score(np.reshape(train_Y[test_index], (-1)),  y_valid_pred))\n",
    "    return(-np.mean(R2))\n",
    "\n",
    "space_gradient_boosting = {\n",
    "    \"learning_rate\": hp.choice(\"learning_rate\", [0.01,0.1,0.2]),\n",
    "    \"max_depth\": hp.quniform(\"max_depth\", 3, 10, 1),\n",
    "    # \"subsample\": hp.quniform(\"subsample\", 0.5, 1, 0.5),\n",
    "    \"reg_lambda\": hp.quniform(\"reg_lambda\", 0, 1, 0.2),\n",
    "    \"reg_alpha\": hp.quniform(\"reg_alpha\", 0, 1, 0.2),\n",
    "    \"max_delta_step\": hp.quniform(\"max_delta_step\", 1, 5, 1),\n",
    "    \"min_child_weight\": hp.quniform(\"min_child_weight\", 1, 6, 1),\n",
    "    \"num_rounds\":  hp.choice(\"num_rounds\", [100,250,500,1000])\n",
    "    }\n",
    "\n",
    "rstate = np.random.default_rng(42)\n",
    "trials = Trials()\n",
    "best = fmin(fn = cross_validation_mse_gradient_boosting, space = space_gradient_boosting,\n",
    "            algo=rand.suggest, max_evals = 20, trials=trials, return_argmin=False, rstate=rstate)\n",
    "\n",
    "print(best)\n",
    "param = best\n",
    "param[\"random_state\"] = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if split == \"full\":\n",
    "    param = {\"random_state\": 42, 'learning_rate': 0.1, 'max_delta_step': 3.0, 'max_depth': 4.0, 'min_child_weight': 5.0, 'num_rounds': 500, 'reg_alpha': 0.2, 'reg_lambda': 0.4}\n",
    "elif split == \"Arabidopsis\":\n",
    "    param = {\"random_state\": 42, 'learning_rate': 0.1, 'max_delta_step': 2.0, 'max_depth': 4.0, 'min_child_weight': 6.0, 'num_rounds': 1000, 'reg_alpha': 0.2, 'reg_lambda': 0.4}\n",
    "elif split == \"Brassicaceae\":\n",
    "    param = {\"random_state\": 42, 'learning_rate': 0.1, 'max_delta_step': 2.0, 'max_depth': 4.0, 'min_child_weight': 6.0, 'num_rounds': 1000, 'reg_alpha': 0.2, 'reg_lambda': 0.4}\n",
    "elif split == \"wildtype\":\n",
    "    param = {\"random_state\": 42, 'learning_rate': 0.1, 'max_delta_step': 3.0, 'max_depth': 4.0, 'min_child_weight': 5.0, 'num_rounds': 500, 'reg_alpha': 0.2, 'reg_lambda': 0.4}\n",
    "else:\n",
    "    param = {\"random_state\": 42, 'learning_rate': 0.2, 'max_delta_step': 1.0, 'max_depth': 7.0, 'min_child_weight': 1.0, 'num_rounds': 1000, 'reg_alpha': 0.6, 'reg_lambda': 0.4}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c) Training and validating model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_round = param[\"num_rounds\"]\n",
    "param[\"max_depth\"] = int(np.round(param[\"max_depth\"]))\n",
    "\n",
    "del param[\"num_rounds\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R2 = []\n",
    "MSE = []\n",
    "Pearson = []\n",
    "MAE = []\n",
    "MedAE = []\n",
    "\n",
    "for i in range(5):\n",
    "    train_index, test_index  = train_indices[i], test_indices[i]\n",
    "    dtrain = xgb.DMatrix(train_X[train_index], label = train_Y[train_index])\n",
    "    dvalid = xgb.DMatrix(train_X[test_index])\n",
    "    \n",
    "    bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "    \n",
    "    y_valid_pred = bst.predict(dvalid)\n",
    "    MSE.append(np.sqrt(np.mean(abs(np.array([10**x for x in np.reshape(train_Y[test_index], (-1))]) - np.array([10**x for x in y_valid_pred]))**2)))\n",
    "    R2.append(r2_score([10**x for x in np.reshape(train_Y[test_index], (-1))], [10**x for x in y_valid_pred]))\n",
    "    Pearson.append(stats.pearsonr([10**x for x in np.reshape(train_Y[test_index], (-1))], [10**x for x in y_valid_pred])[0])\n",
    "    MAE.append(np.mean(abs(np.array([10**x for x in train_Y[test_index]]) - np.array([10**x for x in y_valid_pred]))))\n",
    "    MedAE.append(np.median(abs(np.array([10**x for x in train_Y[test_index]]) - np.array([10**x for x in y_valid_pred]))))\n",
    "\n",
    "    # MSE.append(np.mean(abs(np.reshape(train_Y[test_index], (-1)) - y_valid_pred)**2))\n",
    "    # R2.append(r2_score(np.reshape(train_Y[test_index], (-1)), y_valid_pred))\n",
    "    # Pearson.append(stats.pearsonr(np.reshape(train_Y[test_index], (-1)), y_valid_pred)[0])\n",
    "\n",
    "print(Pearson)\n",
    "print(MSE)\n",
    "print(R2)\n",
    "print(MAE)\n",
    "print(MedAE)\n",
    "\n",
    "np.save(join(\"..\", \"..\", \"data\", split, \"Pearson_CV_xgboost_ESM2_gnn_fp.npy\"), np.array(Pearson))\n",
    "np.save(join(\"..\", \"..\", \"data\", split, \"MSE_CV_xgboost_ESM2_gnn_fp.npy\"), np.array(MSE))\n",
    "np.save(join(\"..\", \"..\", \"data\", split, \"R2_CV_xgboost_ESM2_gnn_fp.npy\"), np.array(R2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(train_X, label = train_Y)\n",
    "dtest = xgb.DMatrix(test_X, label = test_Y)\n",
    "\n",
    "bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "\n",
    "y_test_pred = bst.predict(dtest)\n",
    "\n",
    "data_test[\"Estimate Km\"] = y_test_pred\n",
    "\n",
    "MSE_dif_fp_test = np.sqrt(np.mean(abs(np.array([10**x for x in test_Y]) - np.array([10**x for x in y_test_pred]))**2))\n",
    "R2_dif_fp_test = r2_score(np.array([10**x for x in test_Y]), np.array([10**x for x in y_test_pred]))\n",
    "Pearson = stats.pearsonr(np.array([10**x for x in test_Y]), np.array([10**x for x in y_test_pred]))\n",
    "MAE = np.mean(abs(np.array([10**x for x in test_Y]) - np.array([10**x for x in y_test_pred])))\n",
    "MedAE = np.median(abs(np.array([10**x for x in test_Y]) - np.array([10**x for x in y_test_pred])))\n",
    "\n",
    "# MSE_dif_fp_test = np.mean(abs(np.reshape(test_Y, (-1)) - y_test_pred)**2)\n",
    "# R2_dif_fp_test = r2_score(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "# Pearson = stats.pearsonr(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "# MAE = np.mean(abs(np.reshape(test_Y, (-1)) - y_test_pred))\n",
    "\n",
    "print(np.round(Pearson[0],20) , np.round(MSE_dif_fp_test, 10), np.round(R2_dif_fp_test,3), np.round(MAE, 10), np.round(MedAE, 10))\n",
    "\n",
    "\n",
    "np.save(join(\"..\", \"..\", \"data\", split, \"y_test_pred_xgboost_ESM2_gnn_fp.npy\"), bst.predict(dtest))\n",
    "np.save(join(\"..\", \"..\", \"data\", split,  \"y_test_true_xgboost_ESM2_gnn_fp.npy\"), test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred_esm1b_ts_drfp = y_test_pred\n",
    "# data_test[\"Estimate Km\"] = [10**x for x in data_test[\"Estimate Km\"]]\n",
    "# filtered_df = data_test[data_test['Uniprot IDs'].apply(lambda x: \"Q9LE06\" in x)]\n",
    "# filtered_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (d) Training model with test and train data for production mode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = np.array(list(data_train[\"ESM2\"]))\n",
    "train_X = np.concatenate([train_X, np.array(list(data_train['GNN FP'])), np.array(list(data_train[\"Temperature\"]))[:, np.newaxis], np.array(list(data_train[\"pH\"]))[:, np.newaxis]], axis = 1)\n",
    "train_Y = np.array(list(data_train[\"log10_Km\"]))\n",
    "\n",
    "test_X = np.array(list(data_test[\"ESM2\"]))\n",
    "test_X = np.concatenate([test_X, np.array(list(data_test['GNN FP'])), np.array(list(data_test[\"Temperature\"]))[:, np.newaxis], np.array(list(data_test[\"pH\"]))[:, np.newaxis]], axis = 1)\n",
    "test_Y = np.array(list(data_test[\"log10_Km\"]))\n",
    "\n",
    "train_X = np.concatenate([train_X, test_X])\n",
    "train_Y = np.concatenate([train_Y, test_Y])\n",
    "\n",
    "train_X = train_X.astype(float)\n",
    "test_X = test_X.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(train_X, label = train_Y)\n",
    "dtest = xgb.DMatrix(test_X)\n",
    "\n",
    "bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "\n",
    "y_test_pred = bst.predict(dtest)\n",
    "\n",
    "MSE_dif_fp_test = np.sqrt(np.mean(abs(np.array([10**x for x in np.reshape(test_Y, (-1))]) - np.array([10**x for x in y_test_pred]))**2))\n",
    "R2_dif_fp_test = r2_score(np.array([10**x for x in np.reshape(test_Y, (-1))]), np.array([10**x for x in y_test_pred]))\n",
    "Pearson = stats.pearsonr(np.array([10**x for x in np.reshape(test_Y, (-1))]), np.array([10**x for x in y_test_pred]))\n",
    "\n",
    "# MSE_dif_fp_test = np.mean(abs(np.reshape(test_Y, (-1)) - y_test_pred)**2)\n",
    "# R2_dif_fp_test = r2_score(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "# Pearson = stats.pearsonr(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "\n",
    "print(Pearson, MSE_dif_fp_test, R2_dif_fp_test)\n",
    "\n",
    "pickle.dump(bst, open(join(\"..\", \"..\", \"data\", \"saved_models\",\n",
    "                          \"xgboost_train_and_test.pkl\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = bst.get_score()\n",
    "print(len(data_train[\"ESM2\"][1]))\n",
    "print(len(data_train[\"MACCS FP\"][1]))\n",
    "for key, value in importances.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Validating model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_X = np.array(list(data_val[\"ESM2\"]))\n",
    "val_X = np.concatenate([val_X, np.array(list(data_val[\"GNN FP\"])), np.array(list(data_val[\"Temperature\"]))[:, np.newaxis], np.array(list(data_val[\"pH\"]))[:, np.newaxis]], axis = 1)\n",
    "val_Y = np.array(list(data_val[\"log10_Km\"]))\n",
    "\n",
    "val_X = val_X.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(list(chain(*data_val[\"Km_values\"]))).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dval = xgb.DMatrix(val_X)\n",
    "\n",
    "y_val_pred = bst.predict(dval)\n",
    "data_val[\"Estimate Km\"] = y_val_pred\n",
    "\n",
    "MSE_dif_fp_val = np.sqrt(np.mean(abs(np.array([10**x for x in np.reshape(val_Y, (-1))]) - np.array([10**x for x in y_val_pred]))**2))\n",
    "R2_dif_fp_val = r2_score(np.array([10**x for x in np.reshape(val_Y, (-1))]), np.array([10**x for x in y_val_pred]))\n",
    "Pearson = stats.pearsonr(np.array([10**x for x in np.reshape(val_Y, (-1))]), np.array([10**x for x in y_val_pred]))\n",
    "MAE = np.mean(abs(np.array([10**x for x in val_Y]) - np.array([10**x for x in y_val_pred])))\n",
    "MedAE = np.median(abs(np.array([10**x for x in val_Y]) - np.array([10**x for x in y_val_pred])))\n",
    "\n",
    "# MSE_dif_fp_test = np.mean(abs(np.reshape(test_Y, (-1)) - y_test_pred)**2)\n",
    "# R2_dif_fp_test = r2_score(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "# Pearson = stats.pearsonr(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "\n",
    "print(Pearson, MSE_dif_fp_val, R2_dif_fp_val, MAE, MedAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib.colors as colors\n",
    "# import matplotlib.cm as cmx\n",
    "\n",
    "# plt.scatter(val_Y,y_val_pred, c='blue', edgecolors='black',s=15)\n",
    "    \n",
    "# plt.xlim(-6,2)\n",
    "# plt.ylim(-6,2)\n",
    "# plt.xticks(fontsize=14)\n",
    "# plt.yticks(fontsize=14)\n",
    "# plt.xlabel('Real value', fontsize=15)\n",
    "# plt.ylabel('Estimated value', fontsize=15)\n",
    "# plt.title('Predictions', fontsize=15)\n",
    "# plt.axline((1, 1), slope=1, c='red')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_combined = pd.concat([data_train, data_test],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calculate_identity_ignore_gaps(seq1, seq2):\n",
    "#     identical_residues = sum([1 for x, y in zip(seq1, seq2) if x == y and x != \"-\"])\n",
    "#     pid = identical_residues / sum([1 for x in seq1 if x != \"-\"]) \n",
    "#     return pid\n",
    "\n",
    "# from Bio import Align\n",
    "# from Bio.Align import substitution_matrices\n",
    "\n",
    "# data_val[\"max_identity\"] = np.nan\n",
    "\n",
    "# aligner=Align.PairwiseAligner()\n",
    "# aligner.substitution_matrix = substitution_matrices.load(\"BLOSUM62\")\n",
    "# aligner.mode = \"global\"\n",
    "# aligner.extend_gap_score = -0.5\n",
    "# aligner.open_gap_score = -10\n",
    "\n",
    "# for i in data_val.index:\n",
    "#     identities = []\n",
    "#     for j in data_combined.index:\n",
    "#         seq1 = str(data_val[\"Sequence\"][i])\n",
    "#         seq2 = str(data_combined[\"Sequence\"][j])\n",
    "#         if 'U' in seq1:\n",
    "#             seq1 = seq1.replace('U', 'C')\n",
    "#         if 'U' in seq2:\n",
    "#             seq2 = seq2.replace('U', 'C')\n",
    "#         alignments = aligner.align(seq1, seq2)\n",
    "#         identities.append(calculate_identity_ignore_gaps(alignments[0][0], alignments[0][1]))\n",
    "#     data_val[\"max_identity\"][i] = max(identities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_val[\"max_identity\"] = data_val[\"max_identity\"]*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import sklearn.metrics as sk\n",
    "# import math\n",
    "\n",
    "# fig, ax = plt.subplots(figsize= (10,8))\n",
    "# plt.rcParams.update({'font.size': 28})\n",
    "\n",
    "# splits = [\"0-40%\", \"40-80%\", \"80-99%\", \"99-100%\"]\n",
    "# lower_bounds = [0,40,80,99]\n",
    "# upper_bounds = [40,80,99,100]\n",
    "\n",
    "# points1 = []\n",
    "# points2 = []\n",
    "# n_points1, n_points2 = [], []\n",
    "\n",
    "# for i, split in enumerate(splits):\n",
    "\n",
    "#     lb, ub = lower_bounds[i], upper_bounds[i]\n",
    "    \n",
    "#     help_df = data_val.loc[data_val[\"max_identity\"]>= lb].loc[data_val[\"max_identity\"]<= ub]\n",
    "#     y_true = np.array([10**x for x in help_df[\"log10_Km\"]])\n",
    "#     y_pred = np.array([10**x for x in help_df[\"Estimate Km\"]])\n",
    "#     n_km = len(y_pred)\n",
    "#     R2 =  sk.r2_score(y_true, y_pred)\n",
    "#     abs_error = abs(y_true - y_pred)\n",
    "#     rmse = math.sqrt(np.mean(abs(y_true - y_pred)**2))\n",
    "#     print(len(y_true))\n",
    "#     print(split, R2, rmse)\n",
    "#     points1.append(R2)\n",
    "#     points2.append(rmse)\n",
    "#     n_points1.append(n_km)\n",
    "\n",
    "\n",
    "# ticks2 = np.array(range(len(splits)))\n",
    "# labs = splits\n",
    "# ax.set_xticks(ticks2)\n",
    "# ax.set_xticklabels(labs,  y= -0.03, fontsize=26)\n",
    "# ax.tick_params(axis='x', length=0, rotation = 0)\n",
    "\n",
    "# # plt.ylim((-0.1,2.5))\n",
    "# # plt.xlim((-0.2, 3.2))\n",
    "# plt.legend(loc = \"lower right\", fontsize=20)\n",
    "# plt.ylabel('RMSE')\n",
    "# plt.xlabel('Enzyme sequence identity')\n",
    "# ax.yaxis.set_label_coords(-0.15, 0.5)\n",
    "# ax.xaxis.set_label_coords(0.5,-0.13)\n",
    "\n",
    "# plt.plot([-0.15,4], [0,0], color='grey', linestyle='dashed')\n",
    "\n",
    "\n",
    "# plt.plot([0,1,2,3], points2, c= \"black\", linewidth=2)\n",
    "\n",
    "# for i, split in enumerate(splits):\n",
    "#     points1.append(R2)\n",
    "    \n",
    "#     if i ==0:\n",
    "#         plt.scatter(i, points2[i], c='black', marker=\"o\", linewidths= 8)\n",
    "#         ax.annotate(n_points1[i], (i-0.08, points2[i]+0.08), fontsize=17, c= \"red\", weight = \"bold\")\n",
    "\n",
    "#     else:\n",
    "#         plt.scatter(i, points2[i], c='black', marker=\"o\", linewidths= 8)\n",
    "#         ax.annotate(n_points1[i], (i-0.08, points2[i]+0.08), fontsize=17, c= \"red\", weight = \"bold\")\n",
    "            \n",
    "     \n",
    "# plt.savefig(join(\"..\",\"..\", \"data\", split, \"sequence_identity.png\"))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EC_km_pred =[[] for _ in range(6)]\n",
    "# EC_km =[[] for _ in range(6)]\n",
    "# for ind in data_val.index:\n",
    "#     try:\n",
    "#         EC = int(data_val[\"ECs\"][ind][0][0])\n",
    "#         EC_km[EC-1].append(data_val[\"log10_Km\"][ind])\n",
    "#         EC_km_pred[EC-1].append(data_val[\"Estimate Km\"][ind])\n",
    "#     except IndexError:\n",
    "#         pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(figsize= (8,8))\n",
    "# plt.rcParams.update({'font.size': 28})\n",
    "\n",
    "# classes = [str(i) for i in range(1,7)]\n",
    "\n",
    "# for i in range(len(EC_km)):\n",
    "    \n",
    "#     circle = plt.Circle((np.mean(EC_km[i]), np.mean(EC_km_pred[i]) ),\n",
    "#                         np.sqrt(len(EC_km_pred[i]))/300, color='navy', fill = True)\n",
    "#     ax.add_artist(circle)\n",
    "#     if i ==5:\n",
    "#         ax.annotate(\"EC\"+ str(i+1), (np.mean(EC_km[i])+0.01, np.mean(EC_km_pred[i])-0.05), fontsize=17, c='red', weight = \"bold\")\n",
    "#     else:\n",
    "#         ax.annotate(\"EC\"+ str(i+1), (np.mean(EC_km[i])+0.03, np.mean(EC_km_pred[i])-0.01), fontsize=17, c='red', weight = \"bold\")\n",
    "    \n",
    "\n",
    "# # ticks2 = [0.2, 0.6,1,1.4,1.8]\n",
    "# # labs = ticks2\n",
    "# # ax.set_xticks(ticks2)\n",
    "# # ax.set_xticklabels(labs,  y= -0.03, fontsize=26)\n",
    "# # ax.tick_params(axis='x', length=0, rotation = 0)\n",
    "\n",
    "# # ax.set_yticks(ticks2)\n",
    "# # ax.set_yticklabels(labs,  y= -0.03, fontsize=26)\n",
    "# # ax.tick_params(axis='y', length=0, rotation = 0)\n",
    "\n",
    "# plt.ylim((-6,-1))\n",
    "# plt.xlim((-6, -1))\n",
    "# plt.legend(loc = \"upper left\", fontsize=20)\n",
    "# plt.xlabel('mean measured \\n $k_{m}$ value on $\\log_{10}$-scale')\n",
    "# plt.ylabel('mean predicted \\n $k_{m}$ value on $\\log_{10}$-scale')\n",
    "# ax.yaxis.set_label_coords(-0.15, 0.5)\n",
    "# ax.xaxis.set_label_coords(0.5,-0.13)\n",
    "# plt.axline((1, 1), slope=1, c='grey', alpha = 0.3, linestyle='dashed')\n",
    "# plt.savefig(join(\"..\", \"..\", \"data\", split, \"EC_classes_mean_km.png\"))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import scipy\n",
    "\n",
    "# train_fps = [np.array(list(data_combined[\"MACCS FP\"][ind])).reshape(1,-1) for ind in data_combined.index]\n",
    "# test_fps = [np.array(list(data_val[\"MACCS FP\"][ind])).reshape(1,-1) for ind in data_val.index]\n",
    "\n",
    "# max_sim = []\n",
    "\n",
    "# for fp in test_fps:\n",
    "#     jaccard_sim = np.array([1 - scipy.spatial.distance.cdist(fp,train_fp, metric='jaccard')[0][0] for train_fp in train_fps])\n",
    "#     max_sim.append(np.max(jaccard_sim))\n",
    "    \n",
    "# data_val[\"substrate_sim\"] = max_sim\n",
    "\n",
    "# data_val[\"substrate_sim\"]= (data_val[\"substrate_sim\"] - np.min(data_val[\"substrate_sim\"]))\n",
    "# data_val[\"substrate_sim\"] = data_val[\"substrate_sim\"]/np.max(data_val[\"substrate_sim\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_val[\"global_sim\"] = (data_val[\"max_identity\"]/100)*data_val[\"substrate_sim\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_val.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import math\n",
    "# import scipy as sci\n",
    "# help_df = data_val\n",
    "\n",
    "# sim_bins_lb = [0.0, 0.4, 0.8]\n",
    "# sim_bins_ub = [0.4, 0.8, 1]\n",
    "# r2_scores, n_points, pearson_r, rmse = [], [], [], []\n",
    "# for i in range(len(sim_bins_lb)):\n",
    "#     help_df2 = help_df.loc[help_df[\"global_sim\"] <= sim_bins_ub[i]].loc[help_df[\"global_sim\"] >= sim_bins_lb[i]]\n",
    "#     pred = np.array([10**x for x in help_df2[\"log10_Km\"]])\n",
    "#     true = np.array([10**x for x in help_df2[\"Estimate Km\"]])\n",
    "#     r2_scores.append(sk.r2_score(true, pred))\n",
    "#     pearson_r.append(sci.stats.pearsonr(true, pred)[0])\n",
    "#     rmse.append(math.sqrt(np.mean(abs(true - pred)**2)))\n",
    "#     n_points.append(len(pred))\n",
    "#     print(\"%s - %s\" % (sim_bins_lb[i], sim_bins_ub[i]), r2_scores[-1], pearson_r[-1], rmse[-1], len(pred))\n",
    "    \n",
    "\n",
    "# plt.rcParams.update({'font.size': 24})\n",
    "\n",
    "# fig, ax = plt.subplots(figsize= (8,6))\n",
    "\n",
    "# for i in range(len(sim_bins_lb)):    \n",
    "#     plt.scatter(i, rmse[i], c='navy', marker=\"o\", linewidths= 8)\n",
    "#     ax.annotate(n_points[i], (i-0.08, rmse[i]+0.05), fontsize=17, c= \"black\", weight = \"bold\")\n",
    "\n",
    "    \n",
    "# plt.xlabel('Reaction similarity score')\n",
    "# plt.ylabel('RMSE')\n",
    "# ax.yaxis.set_label_coords(-0.2, 0.5)\n",
    "# ax.xaxis.set_label_coords(0.5,-0.23)\n",
    "\n",
    "# ticks2 = np.array(range(len(sim_bins_lb)))\n",
    "# labs = [\"%s - %s\" % (sim_bins_lb[i], sim_bins_ub[i]) for i in range(len(sim_bins_lb))]\n",
    "# ax.set_xticks(ticks2)\n",
    "# ax.set_xticklabels(labs,  y= -0.03, fontsize=20)\n",
    "# ax.tick_params(axis='x', length=0, rotation = 0)\n",
    "\n",
    "# # plt.ylim((0.5,2))\n",
    "# #plt.xlim((-0.5, 3.2))\n",
    "\n",
    "# # plt.plot([-0.49, 4], [0,0], color='grey', linestyle='dashed')\n",
    "# #plt.savefig(join(\"..\",\"..\", \"data\", split, \"figures\", \"Reaction_Similarity_Score.eps\"))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training a model with enzyme and reaction information (ESM-2/diff_fp) + Temperature + pH :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Creating input matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = np.array(list(data_train[\"ESM2\"]))\n",
    "train_X = np.concatenate([train_X, np.array(list(data_train[\"difference_fp\"])), np.array(list(data_train[\"Temperature\"]))[:, np.newaxis], np.array(list(data_train[\"pH\"]))[:, np.newaxis]], axis = 1)\n",
    "train_Y = np.array(list(data_train[\"log10_Km\"]))\n",
    "\n",
    "test_X = np.array(list(data_test[\"ESM2\"]))\n",
    "test_X = np.concatenate([test_X, np.array(list(data_test[\"difference_fp\"])), np.array(list(data_test[\"Temperature\"]))[:, np.newaxis], np.array(list(data_test[\"pH\"]))[:, np.newaxis]], axis = 1)\n",
    "test_Y = np.array(list(data_test[\"log10_Km\"]))\n",
    "\n",
    "train_X = train_X.astype(float)\n",
    "test_X = test_X.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Hyperparameter optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [1:45:09<00:00, 315.50s/trial, best loss: -0.3046461139724845]  \n",
      "{'learning_rate': 0.1, 'max_delta_step': 3.0, 'max_depth': 4.0, 'min_child_weight': 5.0, 'num_rounds': 500, 'reg_alpha': 0.2, 'reg_lambda': 0.4}\n"
     ]
    }
   ],
   "source": [
    "def cross_validation_mse_gradient_boosting(param):\n",
    "    num_round = param[\"num_rounds\"]\n",
    "    del param[\"num_rounds\"]\n",
    "    param[\"max_depth\"] = int(np.round(param[\"max_depth\"]))\n",
    "    # param[\"device\"] = \"cuda\"\n",
    "    param[\"tree_method\"] = \"hist\"\n",
    "    param[\"sampling_method\"] = \"gradient_based\"\n",
    "    \n",
    "    MSE = []\n",
    "    R2 = []\n",
    "    for i in range(5):\n",
    "        train_index, test_index  = train_indices[i], test_indices[i]\n",
    "        dtrain = xgb.DMatrix(train_X[train_index], label = train_Y[train_index])\n",
    "        dvalid = xgb.DMatrix(train_X[test_index])\n",
    "        bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "        y_valid_pred = bst.predict(dvalid)\n",
    "        MSE.append(np.mean(abs(np.reshape(train_Y[test_index], (-1)) - y_valid_pred)**2))\n",
    "        R2.append(r2_score(np.reshape(train_Y[test_index], (-1)),  y_valid_pred))\n",
    "    return(-np.mean(R2))\n",
    "\n",
    "space_gradient_boosting = {\n",
    "    \"learning_rate\": hp.choice(\"learning_rate\", [0.01,0.1,0.2]),\n",
    "    \"max_depth\": hp.quniform(\"max_depth\", 3, 10, 1),\n",
    "    # \"subsample\": hp.quniform(\"subsample\", 0.5, 1, 0.5),\n",
    "    \"reg_lambda\": hp.quniform(\"reg_lambda\", 0, 1, 0.2),\n",
    "    \"reg_alpha\": hp.quniform(\"reg_alpha\", 0, 1, 0.2),\n",
    "    \"max_delta_step\": hp.quniform(\"max_delta_step\", 1, 5, 1),\n",
    "    \"min_child_weight\": hp.quniform(\"min_child_weight\", 1, 6, 1),\n",
    "    \"num_rounds\":  hp.choice(\"num_rounds\", [100,250,500,1000])\n",
    "    }\n",
    "\n",
    "rstate = np.random.default_rng(42)\n",
    "trials = Trials()\n",
    "best = fmin(fn = cross_validation_mse_gradient_boosting, space = space_gradient_boosting,\n",
    "            algo=rand.suggest, max_evals = 20, trials=trials, return_argmin=False, rstate=rstate)\n",
    "\n",
    "print(best)\n",
    "param = best\n",
    "param[\"random_state\"] = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if split == \"full\":\n",
    "    param = {\"random_state\": 42, 'learning_rate': 0.01, 'max_delta_step': 5.0, 'max_depth': 5.0, 'min_child_weight': 3.0, 'num_rounds': 1000, 'reg_alpha': 0.6, 'reg_lambda': 0.8}\n",
    "elif split == \"Arabidopsis\":\n",
    "    param = {\"random_state\": 42, 'learning_rate': 0.1, 'max_delta_step': 3.0, 'max_depth': 4.0, 'min_child_weight': 5.0, 'num_rounds': 500, 'reg_alpha': 0.2, 'reg_lambda': 0.4}\n",
    "elif split == \"Brassicaceae\":\n",
    "    param = {\"random_state\": 42, 'learning_rate': 0.1, 'max_delta_step': 2.0, 'max_depth': 4.0, 'min_child_weight': 6.0, 'num_rounds': 1000, 'reg_alpha': 0.2, 'reg_lambda': 0.4}\n",
    "elif split == \"wildtype\":\n",
    "    param = {\"random_state\": 42, 'learning_rate': 0.1, 'max_delta_step': 3.0, 'max_depth': 4.0, 'min_child_weight': 5.0, 'num_rounds': 500, 'reg_alpha': 0.2, 'reg_lambda': 0.4}\n",
    "else:\n",
    "    param = {\"random_state\": 42, 'learning_rate': 0.2, 'max_delta_step': 4.0, 'max_depth': 4.0, 'min_child_weight': 3.0, 'num_rounds': 100, 'reg_alpha': 0.8, 'reg_lambda': 0.0}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c) Training and validating model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_round = param[\"num_rounds\"]\n",
    "param[\"max_depth\"] = int(np.round(param[\"max_depth\"]))\n",
    "\n",
    "del param[\"num_rounds\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R2 = []\n",
    "MSE = []\n",
    "Pearson = []\n",
    "MAE = []\n",
    "MedAE = []\n",
    "\n",
    "for i in range(5):\n",
    "    train_index, test_index  = train_indices[i], test_indices[i]\n",
    "    dtrain = xgb.DMatrix(train_X[train_index], label = train_Y[train_index])\n",
    "    dvalid = xgb.DMatrix(train_X[test_index])\n",
    "    \n",
    "    bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "    \n",
    "    y_valid_pred = bst.predict(dvalid)\n",
    "    MSE.append(np.sqrt(np.mean(abs(np.array([10**x for x in np.reshape(train_Y[test_index], (-1))]) - np.array([10**x for x in y_valid_pred]))**2)))\n",
    "    R2.append(r2_score([10**x for x in np.reshape(train_Y[test_index], (-1))], [10**x for x in y_valid_pred]))\n",
    "    Pearson.append(stats.pearsonr([10**x for x in np.reshape(train_Y[test_index], (-1))], [10**x for x in y_valid_pred])[0])\n",
    "    MAE.append(np.mean(abs(np.array([10**x for x in train_Y[test_index]]) - np.array([10**x for x in y_valid_pred]))))\n",
    "    MedAE.append(np.median(abs(np.array([10**x for x in train_Y[test_index]]) - np.array([10**x for x in y_valid_pred]))))\n",
    "\n",
    "    # MSE.append(np.mean(abs(np.reshape(train_Y[test_index], (-1)) - y_valid_pred)**2))\n",
    "    # R2.append(r2_score(np.reshape(train_Y[test_index], (-1)), y_valid_pred))\n",
    "    # Pearson.append(stats.pearsonr(np.reshape(train_Y[test_index], (-1)), y_valid_pred)[0])\n",
    "\n",
    "print(Pearson)\n",
    "print(MSE)\n",
    "print(R2)\n",
    "print(MAE)\n",
    "print(MedAE)\n",
    "\n",
    "np.save(join(\"..\", \"..\", \"data\", split, \"Pearson_CV_xgboost_ESM2_diff_fp.npy\"), np.array(Pearson))\n",
    "np.save(join(\"..\", \"..\", \"data\", split, \"MSE_CV_xgboost_ESM2_diff_fp.npy\"), np.array(MSE))\n",
    "np.save(join(\"..\", \"..\", \"data\", split, \"R2_CV_xgboost_ESM2_diff_fp.npy\"), np.array(R2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(train_X, label = train_Y)\n",
    "dtest = xgb.DMatrix(test_X, label = test_Y)\n",
    "\n",
    "bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "\n",
    "y_test_pred = bst.predict(dtest)\n",
    "\n",
    "data_test[\"Estimate Km\"] = y_test_pred\n",
    "\n",
    "MSE_dif_fp_test = np.sqrt(np.mean(abs(np.array([10**x for x in test_Y]) - np.array([10**x for x in y_test_pred]))**2))\n",
    "R2_dif_fp_test = r2_score(np.array([10**x for x in test_Y]), np.array([10**x for x in y_test_pred]))\n",
    "Pearson = stats.pearsonr(np.array([10**x for x in test_Y]), np.array([10**x for x in y_test_pred]))\n",
    "MAE = np.mean(abs(np.array([10**x for x in test_Y]) - np.array([10**x for x in y_test_pred])))\n",
    "MedAE = np.median(abs(np.array([10**x for x in test_Y]) - np.array([10**x for x in y_test_pred])))\n",
    "\n",
    "# MSE_dif_fp_test = np.mean(abs(np.reshape(test_Y, (-1)) - y_test_pred)**2)\n",
    "# R2_dif_fp_test = r2_score(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "# Pearson = stats.pearsonr(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "# MAE = np.mean(abs(np.reshape(test_Y, (-1)) - y_test_pred))\n",
    "\n",
    "print(np.round(Pearson[0],3) , np.round(MSE_dif_fp_test, 10), np.round(R2_dif_fp_test,3), np.round(MAE, 10), np.round(MedAE, 10))\n",
    "\n",
    "\n",
    "np.save(join(\"..\", \"..\", \"data\", split, \"y_test_pred_xgboost_ESM2_diff_fp.npy\"), bst.predict(dtest))\n",
    "np.save(join(\"..\", \"..\", \"data\", split,  \"y_test_true_xgboost_ESM2_diff_fp.npy\"), test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred_esm1b_ts_drfp = y_test_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (d) Training model with test and train data for production mode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = np.array(list(data_train[\"ESM2\"]))\n",
    "train_X = np.concatenate([train_X, np.array(list(data_train[\"difference_fp\"])), np.array(list(data_train[\"Temperature\"]))[:, np.newaxis], np.array(list(data_train[\"pH\"]))[:, np.newaxis]], axis = 1)\n",
    "train_Y = np.array(list(data_train[\"log10_Km\"]))\n",
    "\n",
    "test_X = np.array(list(data_test[\"ESM2\"]))\n",
    "test_X = np.concatenate([test_X, np.array(list(data_test[\"difference_fp\"])), np.array(list(data_test[\"Temperature\"]))[:, np.newaxis], np.array(list(data_test[\"pH\"]))[:, np.newaxis]], axis = 1)\n",
    "test_Y = np.array(list(data_test[\"log10_Km\"]))\n",
    "\n",
    "train_X = np.concatenate([train_X, test_X])\n",
    "train_Y = np.concatenate([train_Y, test_Y])\n",
    "\n",
    "train_X = train_X.astype(float)\n",
    "test_X = test_X.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dtrain = xgb.DMatrix(train_X, label = train_Y)\n",
    "dtest = xgb.DMatrix(test_X)\n",
    "\n",
    "bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "\n",
    "y_test_pred = bst.predict(dtest)\n",
    "\n",
    "MSE_dif_fp_test = np.sqrt(np.mean(abs(np.array([10**x for x in np.reshape(test_Y, (-1))]) - np.array([10**x for x in y_test_pred]))**2))\n",
    "R2_dif_fp_test = r2_score(np.array([10**x for x in np.reshape(test_Y, (-1))]), np.array([10**x for x in y_test_pred]))\n",
    "Pearson = stats.pearsonr(np.array([10**x for x in np.reshape(test_Y, (-1))]), np.array([10**x for x in y_test_pred]))\n",
    "\n",
    "# MSE_dif_fp_test = np.mean(abs(np.reshape(test_Y, (-1)) - y_test_pred)**2)\n",
    "# R2_dif_fp_test = r2_score(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "# Pearson = stats.pearsonr(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "\n",
    "print(Pearson, MSE_dif_fp_test, R2_dif_fp_test)\n",
    "\n",
    "pickle.dump(bst, open(join(\"..\", \"..\", \"data\", \"saved_models\",\n",
    "                          \"xgboost_train_and_test.pkl\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Validating model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_X = np.array(list(data_val[\"ESM2\"]))\n",
    "val_X = np.concatenate([val_X, np.array(list(data_val[\"difference_fp\"])), np.array(list(data_val[\"Temperature\"]))[:, np.newaxis], np.array(list(data_val[\"pH\"]))[:, np.newaxis]], axis = 1)\n",
    "val_Y = np.array(list(data_val[\"log10_Km\"]))\n",
    "\n",
    "val_X = val_X.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(list(chain(*data_val[\"Km_values\"]))).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dval = xgb.DMatrix(val_X)\n",
    "\n",
    "y_val_pred = bst.predict(dval)\n",
    "data_val[\"Estimate Km\"] = y_val_pred\n",
    "\n",
    "MSE_dif_fp_val = np.sqrt(np.mean(abs(np.array([10**x for x in np.reshape(val_Y, (-1))]) - np.array([10**x for x in y_val_pred]))**2))\n",
    "R2_dif_fp_val = r2_score(np.array([10**x for x in np.reshape(val_Y, (-1))]), np.array([10**x for x in y_val_pred]))\n",
    "Pearson = stats.pearsonr(np.array([10**x for x in np.reshape(val_Y, (-1))]), np.array([10**x for x in y_val_pred]))\n",
    "MAE = np.mean(abs(np.array([10**x for x in val_Y]) - np.array([10**x for x in y_val_pred])))\n",
    "MedAE = np.median(abs(np.array([10**x for x in val_Y]) - np.array([10**x for x in y_val_pred])))\n",
    "\n",
    "# MSE_dif_fp_test = np.mean(abs(np.reshape(test_Y, (-1)) - y_test_pred)**2)\n",
    "# R2_dif_fp_test = r2_score(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "# Pearson = stats.pearsonr(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "\n",
    "print(Pearson, MSE_dif_fp_val, R2_dif_fp_val, MAE, MedAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training a model with enzyme, main substrate (MACCS fp) and reaction information (ESM-2/diff_fp) + Temperature + pH :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Creating input matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = np.array(list(data_train[\"ESM2\"]))\n",
    "train_X = np.concatenate([train_X, np.array(list(data_train[\"difference_fp\"])), np.array(list(data_train[\"GNN FP\"])), np.array(list(data_train[\"Temperature\"]))[:, np.newaxis], np.array(list(data_train[\"pH\"]))[:, np.newaxis]], axis = 1)\n",
    "train_Y = np.array(list(data_train[\"log10_Km\"]))\n",
    "\n",
    "test_X = np.array(list(data_test[\"ESM2\"]))\n",
    "test_X = np.concatenate([test_X, np.array(list(data_test[\"difference_fp\"])), np.array(list(data_test[\"GNN FP\"])), np.array(list(data_test[\"Temperature\"]))[:, np.newaxis], np.array(list(data_test[\"pH\"]))[:, np.newaxis]], axis = 1)\n",
    "test_Y = np.array(list(data_test[\"log10_Km\"]))\n",
    "\n",
    "train_X = train_X.astype(float)\n",
    "test_X = test_X.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Hyperparameter optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [1:38:09<00:00, 294.45s/trial, best loss: -0.4060774827273209]  \n",
      "{'learning_rate': 0.01, 'max_delta_step': 5.0, 'max_depth': 5.0, 'min_child_weight': 3.0, 'num_rounds': 1000, 'reg_alpha': 0.6000000000000001, 'reg_lambda': 0.8}\n"
     ]
    }
   ],
   "source": [
    "def cross_validation_mse_gradient_boosting(param):\n",
    "    num_round = param[\"num_rounds\"]\n",
    "    del param[\"num_rounds\"]\n",
    "    param[\"max_depth\"] = int(np.round(param[\"max_depth\"]))\n",
    "    # param[\"device\"] = \"cuda\"\n",
    "    param[\"tree_method\"] = \"hist\"\n",
    "    param[\"sampling_method\"] = \"gradient_based\"\n",
    "    \n",
    "    MSE = []\n",
    "    R2 = []\n",
    "    for i in range(5):\n",
    "        train_index, test_index  = train_indices[i], test_indices[i]\n",
    "        dtrain = xgb.DMatrix(train_X[train_index], label = train_Y[train_index])\n",
    "        dvalid = xgb.DMatrix(train_X[test_index])\n",
    "        bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "        y_valid_pred = bst.predict(dvalid)\n",
    "        MSE.append(np.mean(abs(np.reshape(train_Y[test_index], (-1)) - y_valid_pred)**2))\n",
    "        R2.append(r2_score(np.reshape(train_Y[test_index], (-1)),  y_valid_pred))\n",
    "    return(-np.mean(R2))\n",
    "\n",
    "space_gradient_boosting = {\n",
    "    \"learning_rate\": hp.choice(\"learning_rate\", [0.01,0.1,0.2]),\n",
    "    \"max_depth\": hp.quniform(\"max_depth\", 3, 10, 1),\n",
    "    # \"subsample\": hp.quniform(\"subsample\", 0.5, 1, 0.5),\n",
    "    \"reg_lambda\": hp.quniform(\"reg_lambda\", 0, 1, 0.2),\n",
    "    \"reg_alpha\": hp.quniform(\"reg_alpha\", 0, 1, 0.2),\n",
    "    \"max_delta_step\": hp.quniform(\"max_delta_step\", 1, 5, 1),\n",
    "    \"min_child_weight\": hp.quniform(\"min_child_weight\", 1, 6, 1),\n",
    "    \"num_rounds\":  hp.choice(\"num_rounds\", [100,250,500,1000])\n",
    "    }\n",
    "\n",
    "rstate = np.random.default_rng(42)\n",
    "trials = Trials()\n",
    "best = fmin(fn = cross_validation_mse_gradient_boosting, space = space_gradient_boosting,\n",
    "            algo=rand.suggest, max_evals = 20, trials=trials, return_argmin=False, rstate=rstate)\n",
    "\n",
    "print(best)\n",
    "param = best\n",
    "param[\"random_state\"] = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if split == \"full\":\n",
    "    param = {\"random_state\": 42, 'learning_rate': 0.01, 'max_delta_step': 5.0, 'max_depth': 5.0, 'min_child_weight': 3.0, 'num_rounds': 1000, 'reg_alpha': 0.6, 'reg_lambda': 0.8}\n",
    "elif split == \"Arabidopsis\":\n",
    "    param = {\"random_state\": 42, 'learning_rate': 0.01, 'max_delta_step': 5.0, 'max_depth': 5.0, 'min_child_weight': 3.0, 'num_rounds': 1000, 'reg_alpha': 0.6, 'reg_lambda': 0.8}\n",
    "elif split == \"Brassicaceae\":\n",
    "    param = {\"random_state\": 42, 'learning_rate': 0.1, 'max_delta_step': 3.0, 'max_depth': 4.0, 'min_child_weight': 5.0, 'num_rounds': 500, 'reg_alpha': 0.2, 'reg_lambda': 0.4}\n",
    "elif split == \"wildtype\":\n",
    "    param = {\"random_state\": 42, 'learning_rate': 0.1, 'max_delta_step': 2.0, 'max_depth': 4.0, 'min_child_weight': 6.0, 'num_rounds': 1000, 'reg_alpha': 0.2, 'reg_lambda': 0.4}\n",
    "else:\n",
    "    param = {\"random_state\": 42, 'learning_rate': 0.2, 'max_delta_step': 1.0, 'max_depth': 7.0, 'min_child_weight': 1.0, 'num_rounds': 1000, 'reg_alpha': 0.6, 'reg_lambda': 0.4}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c) Training and validating model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_round = param[\"num_rounds\"]\n",
    "param[\"max_depth\"] = int(np.round(param[\"max_depth\"]))\n",
    "\n",
    "del param[\"num_rounds\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R2 = []\n",
    "MSE = []\n",
    "Pearson = []\n",
    "MAE = []\n",
    "MedAE = []\n",
    "\n",
    "for i in range(5):\n",
    "    train_index, test_index  = train_indices[i], test_indices[i]\n",
    "    dtrain = xgb.DMatrix(train_X[train_index], label = train_Y[train_index])\n",
    "    dvalid = xgb.DMatrix(train_X[test_index])\n",
    "    \n",
    "    bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "    \n",
    "    y_valid_pred = bst.predict(dvalid)\n",
    "    MSE.append(np.sqrt(np.mean(abs(np.array([10**x for x in np.reshape(train_Y[test_index], (-1))]) - np.array([10**x for x in y_valid_pred]))**2)))\n",
    "    R2.append(r2_score([10**x for x in np.reshape(train_Y[test_index], (-1))], [10**x for x in y_valid_pred]))\n",
    "    Pearson.append(stats.pearsonr([10**x for x in np.reshape(train_Y[test_index], (-1))], [10**x for x in y_valid_pred])[0])\n",
    "    MAE.append(np.mean(abs(np.array([10**x for x in train_Y[test_index]]) - np.array([10**x for x in y_valid_pred]))))\n",
    "    MedAE.append(np.median(abs(np.array([10**x for x in train_Y[test_index]]) - np.array([10**x for x in y_valid_pred]))))\n",
    "\n",
    "    # MSE.append(np.mean(abs(np.reshape(train_Y[test_index], (-1)) - y_valid_pred)**2))\n",
    "    # R2.append(r2_score(np.reshape(train_Y[test_index], (-1)), y_valid_pred))\n",
    "    # Pearson.append(stats.pearsonr(np.reshape(train_Y[test_index], (-1)), y_valid_pred)[0])\n",
    "\n",
    "print(Pearson)\n",
    "print(MSE)\n",
    "print(R2)\n",
    "print(MAE)\n",
    "print(MedAE)\n",
    "\n",
    "np.save(join(\"..\", \"..\", \"data\", split, \"Pearson_CV_xgboost_ESM2_gnn_fp_diff_fp.npy\"), np.array(Pearson))\n",
    "np.save(join(\"..\", \"..\", \"data\", split, \"MSE_CV_xgboost_ESM2_gnn_fp_diff_fp.npy\"), np.array(MSE))\n",
    "np.save(join(\"..\", \"..\", \"data\", split, \"R2_CV_xgboost_ESM2_gnn_fp_diff_fp.npy\"), np.array(R2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(train_X, label = train_Y)\n",
    "dtest = xgb.DMatrix(test_X, label = test_Y)\n",
    "\n",
    "bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "\n",
    "y_test_pred = bst.predict(dtest)\n",
    "\n",
    "data_test[\"Estimate Km\"] = y_test_pred\n",
    "\n",
    "MSE_dif_fp_test = np.sqrt(np.mean(abs(np.array([10**x for x in test_Y]) - np.array([10**x for x in y_test_pred]))**2))\n",
    "R2_dif_fp_test = r2_score(np.array([10**x for x in test_Y]), np.array([10**x for x in y_test_pred]))\n",
    "Pearson = stats.pearsonr(np.array([10**x for x in test_Y]), np.array([10**x for x in y_test_pred]))\n",
    "MAE = np.mean(abs(np.array([10**x for x in test_Y]) - np.array([10**x for x in y_test_pred])))\n",
    "MedAE = np.median(abs(np.array([10**x for x in test_Y]) - np.array([10**x for x in y_test_pred])))\n",
    "\n",
    "# MSE_dif_fp_test = np.mean(abs(np.reshape(test_Y, (-1)) - y_test_pred)**2)\n",
    "# R2_dif_fp_test = r2_score(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "# Pearson = stats.pearsonr(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "# MAE = np.mean(abs(np.reshape(test_Y, (-1)) - y_test_pred))\n",
    "\n",
    "print(np.round(Pearson[0],3) , np.round(MSE_dif_fp_test, 10), np.round(R2_dif_fp_test,3), np.round(MAE, 10), np.round(MedAE, 10))\n",
    "\n",
    "\n",
    "np.save(join(\"..\", \"..\", \"data\", split, \"y_test_pred_xgboost_ESM2_gnn_fp_diff_fp.npy\"), bst.predict(dtest))\n",
    "np.save(join(\"..\", \"..\", \"data\", split,  \"y_test_true_xgboost_ESM2_gnn_fp_diff_fp.npy\"), test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred_esm1b_ts_drfp = y_test_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (d) Training model with test and train data for production mode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = np.array(list(data_train[\"ESM2\"]))\n",
    "train_X = np.concatenate([train_X, np.array(list(data_train[\"difference_fp\"])), np.array(list(data_train[\"GNN FP\"])), np.array(list(data_train[\"Temperature\"]))[:, np.newaxis], np.array(list(data_train[\"pH\"]))[:, np.newaxis]], axis = 1)\n",
    "train_Y = np.array(list(data_train[\"log10_Km\"]))\n",
    "\n",
    "test_X = np.array(list(data_test[\"ESM2\"]))\n",
    "test_X = np.concatenate([test_X, np.array(list(data_test[\"difference_fp\"])), np.array(list(data_test[\"GNN FP\"])), np.array(list(data_test[\"Temperature\"]))[:, np.newaxis], np.array(list(data_test[\"pH\"]))[:, np.newaxis]], axis = 1)\n",
    "test_Y = np.array(list(data_test[\"log10_Km\"]))\n",
    "\n",
    "train_X = np.concatenate([train_X, test_X])\n",
    "train_Y = np.concatenate([train_Y, test_Y])\n",
    "\n",
    "train_X = train_X.astype(float)\n",
    "test_X = test_X.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dtrain = xgb.DMatrix(train_X, label = train_Y)\n",
    "dtest = xgb.DMatrix(test_X)\n",
    "\n",
    "bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "\n",
    "y_test_pred = bst.predict(dtest)\n",
    "\n",
    "MSE_dif_fp_test = np.sqrt(np.mean(abs(np.array([10**x for x in np.reshape(test_Y, (-1))]) - np.array([10**x for x in y_test_pred]))**2))\n",
    "R2_dif_fp_test = r2_score(np.array([10**x for x in np.reshape(test_Y, (-1))]), np.array([10**x for x in y_test_pred]))\n",
    "Pearson = stats.pearsonr(np.array([10**x for x in np.reshape(test_Y, (-1))]), np.array([10**x for x in y_test_pred]))\n",
    "\n",
    "# MSE_dif_fp_test = np.mean(abs(np.reshape(test_Y, (-1)) - y_test_pred)**2)\n",
    "# R2_dif_fp_test = r2_score(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "# Pearson = stats.pearsonr(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "\n",
    "print(Pearson, MSE_dif_fp_test, R2_dif_fp_test)\n",
    "\n",
    "pickle.dump(bst, open(join(\"..\", \"..\", \"data\", \"saved_models\",\n",
    "                          \"xgboost_train_and_test.pkl\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Validating model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_X = np.array(list(data_val[\"ESM2\"]))\n",
    "val_X = np.concatenate([val_X, np.array(list(data_val[\"difference_fp\"])), np.array(list(data_val[\"GNN FP\"])), np.array(list(data_val[\"Temperature\"]))[:, np.newaxis], np.array(list(data_val[\"pH\"]))[:, np.newaxis]], axis = 1)\n",
    "val_Y = np.array(list(data_val[\"log10_Km\"]))\n",
    "\n",
    "val_X = val_X.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(list(chain(*data_val[\"Km_values\"]))).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dval = xgb.DMatrix(val_X)\n",
    "\n",
    "y_val_pred = bst.predict(dval)\n",
    "data_val[\"Estimate Km\"] = y_val_pred\n",
    "\n",
    "MSE_dif_fp_val = np.sqrt(np.mean(abs(np.array([10**x for x in np.reshape(val_Y, (-1))]) - np.array([10**x for x in y_val_pred]))**2))\n",
    "R2_dif_fp_val = r2_score(np.array([10**x for x in np.reshape(val_Y, (-1))]), np.array([10**x for x in y_val_pred]))\n",
    "Pearson = stats.pearsonr(np.array([10**x for x in np.reshape(val_Y, (-1))]), np.array([10**x for x in y_val_pred]))\n",
    "MAE = np.mean(abs(np.array([10**x for x in val_Y]) - np.array([10**x for x in y_val_pred])))\n",
    "MedAE = np.median(abs(np.array([10**x for x in val_Y]) - np.array([10**x for x in y_val_pred])))\n",
    "\n",
    "# MSE_dif_fp_test = np.mean(abs(np.reshape(test_Y, (-1)) - y_test_pred)**2)\n",
    "# R2_dif_fp_test = r2_score(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "# Pearson = stats.pearsonr(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "\n",
    "print(Pearson, MSE_dif_fp_val, R2_dif_fp_val, MAE, MedAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training a model with enzyme information (ESM-2) + Temperature + pH:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Creating input matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = np.array(list(data_train[\"ESM2\"]))\n",
    "train_X = np.concatenate([train_X, np.array(list(data_train[\"Temperature\"]))[:, np.newaxis], np.array(list(data_train[\"pH\"]))[:, np.newaxis]], axis = 1)\n",
    "train_Y = np.array(list(data_train[\"log10_Km\"]))\n",
    "\n",
    "test_X = np.array(list(data_test[\"ESM2\"]))\n",
    "test_X = np.concatenate([test_X, np.array(list(data_test[\"Temperature\"]))[:, np.newaxis], np.array(list(data_test[\"pH\"]))[:, np.newaxis]], axis = 1)\n",
    "test_Y = np.array(list(data_test[\"log10_Km\"]))\n",
    "\n",
    "train_X = train_X.astype(float)\n",
    "test_X = test_X.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Hyperparameter optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [1:10:40<00:00, 212.04s/trial, best loss: -0.3048205093241337]\n",
      "{'learning_rate': 0.1, 'max_delta_step': 2.0, 'max_depth': 4.0, 'min_child_weight': 6.0, 'num_rounds': 1000, 'reg_alpha': 0.2, 'reg_lambda': 0.4}\n"
     ]
    }
   ],
   "source": [
    "def cross_validation_mse_gradient_boosting(param):\n",
    "    num_round = param[\"num_rounds\"]\n",
    "    del param[\"num_rounds\"]\n",
    "    param[\"max_depth\"] = int(np.round(param[\"max_depth\"]))\n",
    "    # param[\"device\"] = \"cuda\"\n",
    "    param[\"tree_method\"] = \"hist\"\n",
    "    param[\"sampling_method\"] = \"gradient_based\"\n",
    "    \n",
    "    MSE = []\n",
    "    R2 = []\n",
    "    for i in range(5):\n",
    "        train_index, test_index  = train_indices[i], test_indices[i]\n",
    "        dtrain = xgb.DMatrix(train_X[train_index], label = train_Y[train_index])\n",
    "        dvalid = xgb.DMatrix(train_X[test_index])\n",
    "        bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "        y_valid_pred = bst.predict(dvalid)\n",
    "        MSE.append(np.mean(abs(np.reshape(train_Y[test_index], (-1)) - y_valid_pred)**2))\n",
    "        R2.append(r2_score(np.reshape(train_Y[test_index], (-1)),  y_valid_pred))\n",
    "    return(-np.mean(R2))\n",
    "\n",
    "space_gradient_boosting = {\n",
    "    \"learning_rate\": hp.choice(\"learning_rate\", [0.01,0.1,0.2]),\n",
    "    \"max_depth\": hp.quniform(\"max_depth\", 3, 10, 1),\n",
    "    # \"subsample\": hp.quniform(\"subsample\", 0.5, 1, 0.5),\n",
    "    \"reg_lambda\": hp.quniform(\"reg_lambda\", 0, 1, 0.2),\n",
    "    \"reg_alpha\": hp.quniform(\"reg_alpha\", 0, 1, 0.2),\n",
    "    \"max_delta_step\": hp.quniform(\"max_delta_step\", 1, 5, 1),\n",
    "    \"min_child_weight\": hp.quniform(\"min_child_weight\", 1, 6, 1),\n",
    "    \"num_rounds\":  hp.choice(\"num_rounds\", [100,250,500,1000])\n",
    "    }\n",
    "\n",
    "rstate = np.random.default_rng(42)\n",
    "trials = Trials()\n",
    "best = fmin(fn = cross_validation_mse_gradient_boosting, space = space_gradient_boosting,\n",
    "            algo=rand.suggest, max_evals = 20, trials=trials, return_argmin=False, rstate=rstate)\n",
    "\n",
    "print(best)\n",
    "param = best\n",
    "param[\"random_state\"] = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if split == \"full\":\n",
    "    param = {\"random_state\": 42, 'learning_rate': 0.01, 'max_delta_step': 5.0, 'max_depth': 5.0, 'min_child_weight': 3.0, 'num_rounds': 1000, 'reg_alpha': 0.6, 'reg_lambda': 0.8}\n",
    "elif split == \"Arabidopsis\":\n",
    "    param = {\"random_state\": 42, 'learning_rate': 0.1, 'max_delta_step': 2.0, 'max_depth': 4.0, 'min_child_weight': 6.0, 'num_rounds': 1000, 'reg_alpha': 0.2, 'reg_lambda': 0.4}\n",
    "elif split == \"Brassicaceae\":\n",
    "    param = {\"random_state\": 42, 'learning_rate': 0.1, 'max_delta_step': 2.0, 'max_depth': 4.0, 'min_child_weight': 6.0, 'num_rounds': 1000, 'reg_alpha': 0.2, 'reg_lambda': 0.4}\n",
    "elif split == \"wildtype\":\n",
    "    param = {\"random_state\": 42, 'learning_rate': 0.1, 'max_delta_step': 2.0, 'max_depth': 4.0, 'min_child_weight': 6.0, 'num_rounds': 1000, 'reg_alpha': 0.2, 'reg_lambda': 0.4}\n",
    "else:\n",
    "    param = {\"random_state\": 42, 'learning_rate': 0.01, 'max_delta_step': 1.0, 'max_depth': 9.0, 'min_child_weight': 3.0, 'num_rounds': 250, 'reg_alpha': 0.8, 'reg_lambda': 0.8}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c) Training and validating model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_round = param[\"num_rounds\"]\n",
    "param[\"max_depth\"] = int(np.round(param[\"max_depth\"]))\n",
    "\n",
    "del param[\"num_rounds\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R2 = []\n",
    "MSE = []\n",
    "Pearson = []\n",
    "MAE = []\n",
    "MedAE = []\n",
    "\n",
    "for i in range(5):\n",
    "    train_index, test_index  = train_indices[i], test_indices[i]\n",
    "    dtrain = xgb.DMatrix(train_X[train_index], label = train_Y[train_index])\n",
    "    dvalid = xgb.DMatrix(train_X[test_index])\n",
    "    \n",
    "    bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "    \n",
    "    y_valid_pred = bst.predict(dvalid)\n",
    "    MSE.append(np.sqrt(np.mean(abs(np.array([10**x for x in np.reshape(train_Y[test_index], (-1))]) - np.array([10**x for x in y_valid_pred]))**2)))\n",
    "    R2.append(r2_score([10**x for x in np.reshape(train_Y[test_index], (-1))], [10**x for x in y_valid_pred]))\n",
    "    Pearson.append(stats.pearsonr([10**x for x in np.reshape(train_Y[test_index], (-1))], [10**x for x in y_valid_pred]))\n",
    "    MAE.append(np.mean(abs(np.array([10**x for x in train_Y[test_index]]) - np.array([10**x for x in y_valid_pred]))))\n",
    "    MedAE.append(np.median(abs(np.array([10**x for x in train_Y[test_index]]) - np.array([10**x for x in y_valid_pred]))))\n",
    "\n",
    "    # MSE.append(np.mean(abs(np.reshape(train_Y[test_index], (-1)) - y_valid_pred)**2))\n",
    "    # R2.append(r2_score(np.reshape(train_Y[test_index], (-1)), y_valid_pred))\n",
    "    # Pearson.append(stats.pearsonr(np.reshape(train_Y[test_index], (-1)), y_valid_pred)[0])\n",
    "\n",
    "print(Pearson)\n",
    "print(MSE)\n",
    "print(R2)\n",
    "print(MAE)\n",
    "print(MedAE)\n",
    "\n",
    "np.save(join(\"..\", \"..\", \"data\", split, \"Pearson_CV_xgboost_ESM2.npy\"), np.array(Pearson))\n",
    "np.save(join(\"..\", \"..\", \"data\", split, \"MSE_CV_xgboost_ESM2.npy\"), np.array(MSE))\n",
    "np.save(join(\"..\", \"..\", \"data\", split, \"R2_CV_xgboost_ESM2.npy\"), np.array(R2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(train_X, label = train_Y)\n",
    "dtest = xgb.DMatrix(test_X, label = test_Y)\n",
    "\n",
    "bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "\n",
    "y_test_pred = bst.predict(dtest)\n",
    "\n",
    "data_test[\"Estimate Km\"] = y_test_pred\n",
    "\n",
    "MSE_dif_fp_test = np.sqrt(np.mean(abs(np.array([10**x for x in test_Y]) - np.array([10**x for x in y_test_pred]))**2))\n",
    "R2_dif_fp_test = r2_score(np.array([10**x for x in test_Y]), np.array([10**x for x in y_test_pred]))\n",
    "Pearson = stats.pearsonr(np.array([10**x for x in test_Y]), np.array([10**x for x in y_test_pred]))\n",
    "MAE = np.mean(abs(np.array([10**x for x in test_Y]) - np.array([10**x for x in y_test_pred])))\n",
    "MedAE = np.median(abs(np.array([10**x for x in test_Y]) - np.array([10**x for x in y_test_pred])))\n",
    "\n",
    "# MSE_dif_fp_test = np.mean(abs(np.reshape(test_Y, (-1)) - y_test_pred)**2)\n",
    "# R2_dif_fp_test = r2_score(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "# Pearson = stats.pearsonr(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "# MAE = np.mean(abs(np.reshape(test_Y, (-1)) - y_test_pred))\n",
    "\n",
    "print(np.round(Pearson[0],3) , np.round(MSE_dif_fp_test, 10), np.round(R2_dif_fp_test,3), np.round(MAE, 10), np.round(MedAE, 10))\n",
    "\n",
    "\n",
    "np.save(join(\"..\", \"..\", \"data\", split, \"y_test_pred_xgboost_ESM2.npy\"), bst.predict(dtest))\n",
    "np.save(join(\"..\", \"..\", \"data\", split,  \"y_test_true_xgboost_ESM2.npy\"), test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred_esm1b_ts_drfp = y_test_pred\n",
    "# data_test[\"Estimate Km\"] = [10**x for x in data_test[\"Estimate Km\"]]\n",
    "# filtered_df = data_test[data_test['Uniprot IDs'].apply(lambda x: \"Q9LE06\" in x)]\n",
    "# filtered_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (d) Training model with test and train data for production mode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = np.array(list(data_train[\"ESM2\"]))\n",
    "train_X = np.concatenate([train_X, np.array(list(data_train[\"Temperature\"]))[:, np.newaxis], np.array(list(data_train[\"pH\"]))[:, np.newaxis]], axis = 1)\n",
    "train_Y = np.array(list(data_train[\"log10_Km\"]))\n",
    "\n",
    "test_X = np.array(list(data_test[\"ESM2\"]))\n",
    "test_X = np.concatenate([test_X, np.array(list(data_test[\"Temperature\"]))[:, np.newaxis], np.array(list(data_test[\"pH\"]))[:, np.newaxis]], axis = 1)\n",
    "test_Y = np.array(list(data_test[\"log10_Km\"]))\n",
    "\n",
    "train_X = np.concatenate([train_X, test_X])\n",
    "train_Y = np.concatenate([train_Y, test_Y])\n",
    "\n",
    "train_X = train_X.astype(float)\n",
    "test_X = test_X.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dtrain = xgb.DMatrix(train_X, label = train_Y)\n",
    "dtest = xgb.DMatrix(test_X)\n",
    "\n",
    "bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "\n",
    "y_test_pred = bst.predict(dtest)\n",
    "\n",
    "MSE_dif_fp_test = np.sqrt(np.mean(abs(np.array([10**x for x in np.reshape(test_Y, (-1))]) - np.array([10**x for x in y_test_pred]))**2))\n",
    "R2_dif_fp_test = r2_score(np.array([10**x for x in np.reshape(test_Y, (-1))]), np.array([10**x for x in y_test_pred]))\n",
    "Pearson = stats.pearsonr(np.array([10**x for x in np.reshape(test_Y, (-1))]), np.array([10**x for x in y_test_pred]))\n",
    "\n",
    "# MSE_dif_fp_test = np.mean(abs(np.reshape(test_Y, (-1)) - y_test_pred)**2)\n",
    "# R2_dif_fp_test = r2_score(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "# Pearson = stats.pearsonr(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "\n",
    "print(Pearson, MSE_dif_fp_test, R2_dif_fp_test)\n",
    "\n",
    "pickle.dump(bst, open(join(\"..\", \"..\", \"data\", \"saved_models\",\n",
    "                          \"xgboost_train_and_test.pkl\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = bst.get_score()\n",
    "print(len(data_train[\"ESM2\"][1]))\n",
    "print(len(data_train[\"MACCS FP\"][1]))\n",
    "for key, value in importances.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (e) Validating model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_X = np.array(list(data_val[\"ESM2\"]))\n",
    "val_X = np.concatenate([val_X, np.array(list(data_val[\"Temperature\"]))[:, np.newaxis], np.array(list(data_val[\"pH\"]))[:, np.newaxis]], axis = 1)\n",
    "val_Y = np.array(list(data_val[\"log10_Km\"]))\n",
    "\n",
    "val_X = val_X.astype(float)\n",
    "\n",
    "dval = xgb.DMatrix(val_X)\n",
    "\n",
    "y_val_pred = bst.predict(dval)\n",
    "data_val[\"Estimate Km\"] = y_val_pred\n",
    "\n",
    "MSE_dif_fp_val = np.sqrt(np.mean(abs(np.array([10**x for x in np.reshape(val_Y, (-1))]) - np.array([10**x for x in y_val_pred]))**2))\n",
    "R2_dif_fp_val = r2_score(np.array([10**x for x in np.reshape(val_Y, (-1))]), np.array([10**x for x in y_val_pred]))\n",
    "Pearson = stats.pearsonr(np.array([10**x for x in np.reshape(val_Y, (-1))]), np.array([10**x for x in y_val_pred]))\n",
    "MAE = np.mean(abs(np.array([10**x for x in val_Y]) - np.array([10**x for x in y_val_pred])))\n",
    "MedAE = np.median(abs(np.array([10**x for x in val_Y]) - np.array([10**x for x in y_val_pred])))\n",
    "\n",
    "# MSE_dif_fp_test = np.mean(abs(np.reshape(test_Y, (-1)) - y_test_pred)**2)\n",
    "# R2_dif_fp_test = r2_score(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "# Pearson = stats.pearsonr(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "\n",
    "print(Pearson, MSE_dif_fp_val, R2_dif_fp_val, MAE, MedAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.cm as cmx\n",
    "\n",
    "plt.scatter([10**x for x in val_Y],[10**x for x in y_val_pred],c='green', edgecolors='black',s=40)\n",
    "    \n",
    "plt.xlim(-0.0005,0.003)\n",
    "plt.ylim(-0.0005,0.0015)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.xlabel('Real value', fontsize=15)\n",
    "plt.ylabel('Estimated value', fontsize=15)\n",
    "plt.title('Predictions for Km', fontsize=15)\n",
    "plt.axline((1, 1), slope=1, c='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training a model with main substrate information (MACCS) + Temperature + pH:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Creating input matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = np.array(list(data_train['GNN FP']))\n",
    "train_X = np.concatenate([train_X, np.array(list(data_train[\"Temperature\"]))[:, np.newaxis], np.array(list(data_train[\"pH\"]))[:, np.newaxis]], axis = 1)\n",
    "train_Y = np.array(list(data_train[\"log10_Km\"]))\n",
    "\n",
    "test_X = np.array(list(data_test['GNN FP']))\n",
    "test_X = np.concatenate([test_X, np.array(list(data_test[\"Temperature\"]))[:, np.newaxis], np.array(list(data_test[\"pH\"]))[:, np.newaxis]], axis = 1)\n",
    "test_Y = np.array(list(data_test[\"log10_Km\"]))\n",
    "\n",
    "train_X = train_X.astype(float)\n",
    "test_X = test_X.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Hyperparameter optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [02:23<00:00,  7.16s/trial, best loss: -0.3268180821337774]\n",
      "{'learning_rate': 0.2, 'max_delta_step': 4.0, 'max_depth': 4.0, 'min_child_weight': 3.0, 'num_rounds': 100, 'reg_alpha': 0.8, 'reg_lambda': 0.0}\n"
     ]
    }
   ],
   "source": [
    "def cross_validation_mse_gradient_boosting(param):\n",
    "    num_round = param[\"num_rounds\"]\n",
    "    del param[\"num_rounds\"]\n",
    "    param[\"max_depth\"] = int(np.round(param[\"max_depth\"]))\n",
    "    # param[\"device\"] = \"cuda\"\n",
    "    param[\"tree_method\"] = \"hist\"\n",
    "    param[\"sampling_method\"] = \"gradient_based\"\n",
    "    \n",
    "    MSE = []\n",
    "    R2 = []\n",
    "    for i in range(5):\n",
    "        train_index, test_index  = train_indices[i], test_indices[i]\n",
    "        dtrain = xgb.DMatrix(train_X[train_index], label = train_Y[train_index])\n",
    "        dvalid = xgb.DMatrix(train_X[test_index])\n",
    "        bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "        y_valid_pred = bst.predict(dvalid)\n",
    "        MSE.append(np.mean(abs(np.reshape(train_Y[test_index], (-1)) - y_valid_pred)**2))\n",
    "        R2.append(r2_score(np.reshape(train_Y[test_index], (-1)),  y_valid_pred))\n",
    "    return(-np.mean(R2))\n",
    "\n",
    "space_gradient_boosting = {\n",
    "    \"learning_rate\": hp.choice(\"learning_rate\", [0.01,0.1,0.2]),\n",
    "    \"max_depth\": hp.quniform(\"max_depth\", 3, 10, 1),\n",
    "    # \"subsample\": hp.quniform(\"subsample\", 0.5, 1, 0.5),\n",
    "    \"reg_lambda\": hp.quniform(\"reg_lambda\", 0, 1, 0.2),\n",
    "    \"reg_alpha\": hp.quniform(\"reg_alpha\", 0, 1, 0.2),\n",
    "    \"max_delta_step\": hp.quniform(\"max_delta_step\", 1, 5, 1),\n",
    "    \"min_child_weight\": hp.quniform(\"min_child_weight\", 1, 6, 1),\n",
    "    \"num_rounds\":  hp.choice(\"num_rounds\", [100,250,500,1000])\n",
    "    }\n",
    "\n",
    "rstate = np.random.default_rng(42)\n",
    "trials = Trials()\n",
    "best = fmin(fn = cross_validation_mse_gradient_boosting, space = space_gradient_boosting,\n",
    "            algo=rand.suggest, max_evals = 20, trials=trials, return_argmin=False, rstate=rstate)\n",
    "\n",
    "print(best)\n",
    "param = best\n",
    "param[\"random_state\"] = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if split == \"full\":\n",
    "    param = {\"random_state\": 42, 'learning_rate': 0.01, 'max_delta_step': 5.0, 'max_depth': 5.0, 'min_child_weight': 3.0, 'num_rounds': 1000, 'reg_alpha': 0.6, 'reg_lambda': 0.8}\n",
    "elif split == \"Arabidopsis\":\n",
    "    param = {\"random_state\": 42, 'learning_rate': 0.2, 'max_delta_step': 4.0, 'max_depth': 4.0, 'min_child_weight': 3.0, 'num_rounds': 100, 'reg_alpha': 0.8, 'reg_lambda': 0.0}\n",
    "elif split == \"Brassicaceae\":\n",
    "    param = {\"random_state\": 42, 'learning_rate': 0.01, 'max_delta_step': 4.0, 'max_depth': 5.0, 'min_child_weight': 3.0, 'num_rounds': 250, 'reg_alpha': 0.8, 'reg_lambda': 0.2}\n",
    "elif split == \"wildtype\":\n",
    "    param = {\"random_state\": 42, 'learning_rate': 0.01, 'max_delta_step': 5.0, 'max_depth': 5.0, 'min_child_weight': 3.0, 'num_rounds': 1000, 'reg_alpha': 0.6, 'reg_lambda': 0.8}\n",
    "else:\n",
    "    param = {\"random_state\": 42, 'learning_rate': 0.1, 'max_delta_step': 3.0, 'max_depth': 9.0, 'min_child_weight': 2.0, 'num_rounds': 100, 'reg_alpha': 0.8, 'reg_lambda': 0.2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c) Training and validating model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_round = param[\"num_rounds\"]\n",
    "param[\"max_depth\"] = int(np.round(param[\"max_depth\"]))\n",
    "\n",
    "del param[\"num_rounds\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R2 = []\n",
    "MSE = []\n",
    "Pearson = []\n",
    "MAE = []\n",
    "MedAE = []\n",
    "\n",
    "for i in range(5):\n",
    "    train_index, test_index  = train_indices[i], test_indices[i]\n",
    "    dtrain = xgb.DMatrix(train_X[train_index], label = train_Y[train_index])\n",
    "    dvalid = xgb.DMatrix(train_X[test_index])\n",
    "    \n",
    "    bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "    \n",
    "    y_valid_pred = bst.predict(dvalid)\n",
    "    MSE.append(np.sqrt(np.mean(abs(np.array([10**x for x in np.reshape(train_Y[test_index], (-1))]) - np.array([10**x for x in y_valid_pred]))**2)))\n",
    "    R2.append(r2_score([10**x for x in np.reshape(train_Y[test_index], (-1))], [10**x for x in y_valid_pred]))\n",
    "    Pearson.append(stats.pearsonr([10**x for x in np.reshape(train_Y[test_index], (-1))], [10**x for x in y_valid_pred])[0])\n",
    "    MAE.append(np.mean(abs(np.array([10**x for x in train_Y[test_index]]) - np.array([10**x for x in y_valid_pred]))))\n",
    "    MedAE.append(np.median(abs(np.array([10**x for x in train_Y[test_index]]) - np.array([10**x for x in y_valid_pred]))))\n",
    "\n",
    "    # MSE.append(np.mean(abs(np.reshape(train_Y[test_index], (-1)) - y_valid_pred)**2))\n",
    "    # R2.append(r2_score(np.reshape(train_Y[test_index], (-1)), y_valid_pred))\n",
    "    # Pearson.append(stats.pearsonr(np.reshape(train_Y[test_index], (-1)), y_valid_pred)[0])\n",
    "\n",
    "print(Pearson)\n",
    "print(MSE)\n",
    "print(R2)\n",
    "print(MAE)\n",
    "print(MedAE)\n",
    "\n",
    "np.save(join(\"..\", \"..\", \"data\", split, \"Pearson_CV_xgboost_gnn_fp.npy\"), np.array(Pearson))\n",
    "np.save(join(\"..\", \"..\", \"data\", split, \"MSE_CV_xgboost_gnn_fp.npy\"), np.array(MSE))\n",
    "np.save(join(\"..\", \"..\", \"data\", split, \"R2_CV_xgboost_gnn_fp.npy\"), np.array(R2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(train_X, label = train_Y)\n",
    "dtest = xgb.DMatrix(test_X, label = test_Y)\n",
    "\n",
    "bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "\n",
    "y_test_pred = bst.predict(dtest)\n",
    "\n",
    "data_test[\"Estimate Km\"] = y_test_pred\n",
    "\n",
    "MSE_dif_fp_test = np.sqrt(np.mean(abs(np.array([10**x for x in test_Y]) - np.array([10**x for x in y_test_pred]))**2))\n",
    "R2_dif_fp_test = r2_score(np.array([10**x for x in test_Y]), np.array([10**x for x in y_test_pred]))\n",
    "Pearson = stats.pearsonr(np.array([10**x for x in test_Y]), np.array([10**x for x in y_test_pred]))\n",
    "MAE = np.mean(abs(np.array([10**x for x in test_Y]) - np.array([10**x for x in y_test_pred])))\n",
    "MedAE = np.median(abs(np.array([10**x for x in test_Y]) - np.array([10**x for x in y_test_pred])))\n",
    "\n",
    "# MSE_dif_fp_test = np.mean(abs(np.reshape(test_Y, (-1)) - y_test_pred)**2)\n",
    "# R2_dif_fp_test = r2_score(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "# Pearson = stats.pearsonr(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "# MAE = np.mean(abs(np.reshape(test_Y, (-1)) - y_test_pred))\n",
    "\n",
    "print(np.round(Pearson[0],3) , np.round(MSE_dif_fp_test, 10), np.round(R2_dif_fp_test,3), np.round(MAE, 10), np.round(MedAE, 10))\n",
    "\n",
    "\n",
    "np.save(join(\"..\", \"..\", \"data\", split, \"y_test_pred_xgboost_gnn_fp.npy\"), bst.predict(dtest))\n",
    "np.save(join(\"..\", \"..\", \"data\", split,  \"y_test_true_xgboost_gnn_fp.npy\"), test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred_esm1b_ts_drfp = y_test_pred\n",
    "# data_test[\"Estimate Km\"] = [10**x for x in data_test[\"Estimate Km\"]]\n",
    "# filtered_df = data_test[data_test['Uniprot IDs'].apply(lambda x: \"Q9LE06\" in x)]\n",
    "# filtered_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (d) Training model with test and train data for production mode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = np.array(list(data_train[\"GNN FP\"]))\n",
    "train_X = np.concatenate([train_X, np.array(list(data_train[\"Temperature\"]))[:, np.newaxis], np.array(list(data_train[\"pH\"]))[:, np.newaxis]], axis = 1)\n",
    "train_Y = np.array(list(data_train[\"log10_Km\"]))\n",
    "\n",
    "test_X = np.array(list(data_test[\"GNN FP\"]))\n",
    "test_X = np.concatenate([test_X, np.array(list(data_test[\"Temperature\"]))[:, np.newaxis], np.array(list(data_test[\"pH\"]))[:, np.newaxis]], axis = 1)\n",
    "test_Y = np.array(list(data_test[\"log10_Km\"]))\n",
    "\n",
    "train_X = np.concatenate([train_X, test_X])\n",
    "train_Y = np.concatenate([train_Y, test_Y])\n",
    "\n",
    "train_X = train_X.astype(float)\n",
    "test_X = test_X.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dtrain = xgb.DMatrix(train_X, label = train_Y)\n",
    "dtest = xgb.DMatrix(test_X)\n",
    "\n",
    "bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "\n",
    "y_test_pred = bst.predict(dtest)\n",
    "\n",
    "MSE_dif_fp_test = np.sqrt(np.mean(abs(np.array([10**x for x in np.reshape(test_Y, (-1))]) - np.array([10**x for x in y_test_pred]))**2))\n",
    "R2_dif_fp_test = r2_score(np.array([10**x for x in np.reshape(test_Y, (-1))]), np.array([10**x for x in y_test_pred]))\n",
    "Pearson = stats.pearsonr(np.array([10**x for x in np.reshape(test_Y, (-1))]), np.array([10**x for x in y_test_pred]))\n",
    "\n",
    "# MSE_dif_fp_test = np.mean(abs(np.reshape(test_Y, (-1)) - y_test_pred)**2)\n",
    "# R2_dif_fp_test = r2_score(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "# Pearson = stats.pearsonr(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "\n",
    "print(Pearson, MSE_dif_fp_test, R2_dif_fp_test)\n",
    "\n",
    "pickle.dump(bst, open(join(\"..\", \"..\", \"data\", \"saved_models\",\n",
    "                          \"xgboost_train_and_test.pkl\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = bst.get_score()\n",
    "print(len(data_train[\"ESM2\"][1]))\n",
    "print(len(data_train[\"MACCS FP\"][1]))\n",
    "for key, value in importances.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (e) Validating model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_X = np.array(list(data_val[\"GNN FP\"]))\n",
    "val_X = np.concatenate([val_X, np.array(list(data_val[\"Temperature\"]))[:, np.newaxis], np.array(list(data_val[\"pH\"]))[:, np.newaxis]], axis = 1)\n",
    "val_Y = np.array(list(data_val[\"log10_Km\"]))\n",
    "\n",
    "val_X = val_X.astype(float)\n",
    "\n",
    "dval = xgb.DMatrix(val_X)\n",
    "\n",
    "y_val_pred = bst.predict(dval)\n",
    "data_val[\"Estimate Km\"] = y_val_pred\n",
    "\n",
    "MSE_dif_fp_val = np.sqrt(np.mean(abs(np.array([10**x for x in np.reshape(val_Y, (-1))]) - np.array([10**x for x in y_val_pred]))**2))\n",
    "R2_dif_fp_val = r2_score(np.array([10**x for x in np.reshape(val_Y, (-1))]), np.array([10**x for x in y_val_pred]))\n",
    "Pearson = stats.pearsonr(np.array([10**x for x in np.reshape(val_Y, (-1))]), np.array([10**x for x in y_val_pred]))\n",
    "MAE = np.mean(abs(np.array([10**x for x in val_Y]) - np.array([10**x for x in y_val_pred])))\n",
    "MedAE = np.median(abs(np.array([10**x for x in val_Y]) - np.array([10**x for x in y_val_pred])))\n",
    "\n",
    "# MSE_dif_fp_test = np.mean(abs(np.reshape(test_Y, (-1)) - y_test_pred)**2)\n",
    "# R2_dif_fp_test = r2_score(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "# Pearson = stats.pearsonr(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "\n",
    "print(Pearson, MSE_dif_fp_val, R2_dif_fp_val, MAE,MedAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training a model with reaction information (diff-fp) + Temperature + pH:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Creating input matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = np.array(list(data_train[\"difference_fp\"]))\n",
    "train_X = np.concatenate([train_X, np.array(list(data_train[\"Temperature\"]))[:, np.newaxis], np.array(list(data_train[\"pH\"]))[:, np.newaxis]], axis = 1)\n",
    "train_Y = np.array(list(data_train[\"log10_Km\"]))\n",
    "\n",
    "test_X = np.array(list(data_test[\"difference_fp\"]))\n",
    "test_X = np.concatenate([test_X, np.array(list(data_test[\"Temperature\"]))[:, np.newaxis], np.array(list(data_test[\"pH\"]))[:, np.newaxis]], axis = 1)\n",
    "test_Y = np.array(list(data_test[\"log10_Km\"]))\n",
    "\n",
    "train_X = train_X.astype(float)\n",
    "test_X = test_X.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Hyperparameter optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [04:19<00:00, 12.96s/trial, best loss: -0.27532956498887395]\n",
      "{'learning_rate': 0.01, 'max_delta_step': 5.0, 'max_depth': 5.0, 'min_child_weight': 3.0, 'num_rounds': 1000, 'reg_alpha': 0.6000000000000001, 'reg_lambda': 0.8}\n"
     ]
    }
   ],
   "source": [
    "def cross_validation_mse_gradient_boosting(param):\n",
    "    num_round = param[\"num_rounds\"]\n",
    "    del param[\"num_rounds\"]\n",
    "    param[\"max_depth\"] = int(np.round(param[\"max_depth\"]))\n",
    "    # param[\"device\"] = \"cuda\"\n",
    "    param[\"tree_method\"] = \"hist\"\n",
    "    param[\"sampling_method\"] = \"gradient_based\"\n",
    "    \n",
    "    MSE = []\n",
    "    R2 = []\n",
    "    for i in range(5):\n",
    "        train_index, test_index  = train_indices[i], test_indices[i]\n",
    "        dtrain = xgb.DMatrix(train_X[train_index], label = train_Y[train_index])\n",
    "        dvalid = xgb.DMatrix(train_X[test_index])\n",
    "        bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "        y_valid_pred = bst.predict(dvalid)\n",
    "        MSE.append(np.mean(abs(np.reshape(train_Y[test_index], (-1)) - y_valid_pred)**2))\n",
    "        R2.append(r2_score(np.reshape(train_Y[test_index], (-1)),  y_valid_pred))\n",
    "    return(-np.mean(R2))\n",
    "\n",
    "space_gradient_boosting = {\n",
    "    \"learning_rate\": hp.choice(\"learning_rate\", [0.01,0.1,0.2]),\n",
    "    \"max_depth\": hp.quniform(\"max_depth\", 3, 10, 1),\n",
    "    # \"subsample\": hp.quniform(\"subsample\", 0.5, 1, 0.5),\n",
    "    \"reg_lambda\": hp.quniform(\"reg_lambda\", 0, 1, 0.2),\n",
    "    \"reg_alpha\": hp.quniform(\"reg_alpha\", 0, 1, 0.2),\n",
    "    \"max_delta_step\": hp.quniform(\"max_delta_step\", 1, 5, 1),\n",
    "    \"min_child_weight\": hp.quniform(\"min_child_weight\", 1, 6, 1),\n",
    "    \"num_rounds\":  hp.choice(\"num_rounds\", [100,250,500,1000])\n",
    "    }\n",
    "\n",
    "rstate = np.random.default_rng(42)\n",
    "trials = Trials()\n",
    "best = fmin(fn = cross_validation_mse_gradient_boosting, space = space_gradient_boosting,\n",
    "            algo=rand.suggest, max_evals = 20, trials=trials, return_argmin=False, rstate=rstate)\n",
    "\n",
    "print(best)\n",
    "param = best\n",
    "param[\"random_state\"] = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if split == \"full\":\n",
    "    param = {\"random_state\": 42, 'learning_rate': 0.1, 'max_delta_step': 3.0, 'max_depth': 9.0, 'min_child_weight': 2.0, 'num_rounds': 100, 'reg_alpha': 0.8, 'reg_lambda': 0.2}\n",
    "elif split == \"Arabidopsis\":\n",
    "    param = {\"random_state\": 42, 'learning_rate': 0.01, 'max_delta_step': 5.0, 'max_depth': 5.0, 'min_child_weight': 3.0, 'num_rounds': 1000, 'reg_alpha': 0.6, 'reg_lambda': 0.8}\n",
    "elif split == \"Brassicaceae\":\n",
    "    param = {\"random_state\": 42, 'learning_rate': 0.01, 'max_delta_step': 2.0, 'max_depth': 8.0, 'min_child_weight': 5.0, 'num_rounds': 500, 'reg_alpha': 0.4, 'reg_lambda': 0.0}\n",
    "elif split == \"wildtype\":\n",
    "    param = {\"random_state\": 42, 'learning_rate': 0.1, 'max_delta_step': 3.0, 'max_depth': 9.0, 'min_child_weight': 2.0, 'num_rounds': 100, 'reg_alpha': 0.8, 'reg_lambda': 0.2}\n",
    "else:\n",
    "    param = {\"random_state\": 42, 'learning_rate': 0.01, 'max_delta_step': 2.0, 'max_depth': 8.0, 'min_child_weight': 5.0, 'num_rounds': 500, 'reg_alpha': 0.4, 'reg_lambda': 0.0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c) Training and validating model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_round = param[\"num_rounds\"]\n",
    "param[\"max_depth\"] = int(np.round(param[\"max_depth\"]))\n",
    "\n",
    "del param[\"num_rounds\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R2 = []\n",
    "MSE = []\n",
    "Pearson = []\n",
    "MAE = []\n",
    "MedAE = []\n",
    "\n",
    "for i in range(5):\n",
    "    train_index, test_index  = train_indices[i], test_indices[i]\n",
    "    dtrain = xgb.DMatrix(train_X[train_index], label = train_Y[train_index])\n",
    "    dvalid = xgb.DMatrix(train_X[test_index])\n",
    "    \n",
    "    bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "    \n",
    "    y_valid_pred = bst.predict(dvalid)\n",
    "    MSE.append(np.sqrt(np.mean(abs(np.array([10**x for x in np.reshape(train_Y[test_index], (-1))]) - np.array([10**x for x in y_valid_pred]))**2)))\n",
    "    R2.append(r2_score([10**x for x in np.reshape(train_Y[test_index], (-1))], [10**x for x in y_valid_pred]))\n",
    "    Pearson.append(stats.pearsonr([10**x for x in np.reshape(train_Y[test_index], (-1))], [10**x for x in y_valid_pred])[0])\n",
    "    MAE.append(np.mean(abs(np.array([10**x for x in train_Y[test_index]]) - np.array([10**x for x in y_valid_pred]))))\n",
    "    MedAE.append(np.median(abs(np.array([10**x for x in train_Y[test_index]]) - np.array([10**x for x in y_valid_pred]))))\n",
    "\n",
    "    # MSE.append(np.mean(abs(np.reshape(train_Y[test_index], (-1)) - y_valid_pred)**2))\n",
    "    # R2.append(r2_score(np.reshape(train_Y[test_index], (-1)), y_valid_pred))\n",
    "    # Pearson.append(stats.pearsonr(np.reshape(train_Y[test_index], (-1)), y_valid_pred)[0])\n",
    "\n",
    "print(Pearson)\n",
    "print(MSE)\n",
    "print(R2)\n",
    "print(MAE)\n",
    "print(MedAE)\n",
    "\n",
    "np.save(join(\"..\", \"..\", \"data\", split, \"Pearson_CV_xgboost_diff_fp.npy\"), np.array(Pearson))\n",
    "np.save(join(\"..\", \"..\", \"data\", split, \"MSE_CV_xgboost_diff_fp.npy\"), np.array(MSE))\n",
    "np.save(join(\"..\", \"..\", \"data\", split, \"R2_CV_xgboost_diff_fp.npy\"), np.array(R2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(train_X, label = train_Y)\n",
    "dtest = xgb.DMatrix(test_X, label = test_Y)\n",
    "\n",
    "bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "\n",
    "y_test_pred = bst.predict(dtest)\n",
    "\n",
    "data_test[\"Estimate Km\"] = y_test_pred\n",
    "\n",
    "MSE_dif_fp_test = np.sqrt(np.mean(abs(np.array([10**x for x in test_Y]) - np.array([10**x for x in y_test_pred]))**2))\n",
    "R2_dif_fp_test = r2_score(np.array([10**x for x in test_Y]), np.array([10**x for x in y_test_pred]))\n",
    "Pearson = stats.pearsonr(np.array([10**x for x in test_Y]), np.array([10**x for x in y_test_pred]))\n",
    "MAE = np.mean(abs(np.array([10**x for x in test_Y]) - np.array([10**x for x in y_test_pred])))\n",
    "MedAE = np.median(abs(np.array([10**x for x in test_Y]) - np.array([10**x for x in y_test_pred])))\n",
    "\n",
    "# MSE_dif_fp_test = np.mean(abs(np.reshape(test_Y, (-1)) - y_test_pred)**2)\n",
    "# R2_dif_fp_test = r2_score(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "# Pearson = stats.pearsonr(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "# MAE = np.mean(abs(np.reshape(test_Y, (-1)) - y_test_pred))\n",
    "\n",
    "print(np.round(Pearson[0],3) , np.round(MSE_dif_fp_test, 10), np.round(R2_dif_fp_test,3), np.round(MAE, 10), np.round(MedAE, 10))\n",
    "\n",
    "\n",
    "np.save(join(\"..\", \"..\", \"data\", split, \"y_test_pred_xgboost_diff_fp.npy\"), bst.predict(dtest))\n",
    "np.save(join(\"..\", \"..\", \"data\", split,  \"y_test_true_xgboost_diff_fp.npy\"), test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred_esm1b_ts_drfp = y_test_pred\n",
    "# data_test[\"Estimate Km\"] = [10**x for x in data_test[\"Estimate Km\"]]\n",
    "# filtered_df = data_test[data_test['Uniprot IDs'].apply(lambda x: \"Q9LE06\" in x)]\n",
    "# filtered_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (d) Training model with test and train data for production mode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = np.array(list(data_train[\"difference_fp\"]))\n",
    "train_X = np.concatenate([train_X, np.array(list(data_train[\"Temperature\"]))[:, np.newaxis], np.array(list(data_train[\"pH\"]))[:, np.newaxis]], axis = 1)\n",
    "train_Y = np.array(list(data_train[\"log10_Km\"]))\n",
    "\n",
    "test_X = np.array(list(data_test[\"difference_fp\"]))\n",
    "test_X = np.concatenate([test_X, np.array(list(data_test[\"Temperature\"]))[:, np.newaxis], np.array(list(data_test[\"pH\"]))[:, np.newaxis]], axis = 1)\n",
    "test_Y = np.array(list(data_test[\"log10_Km\"]))\n",
    "\n",
    "train_X = np.concatenate([train_X, test_X])\n",
    "train_Y = np.concatenate([train_Y, test_Y])\n",
    "\n",
    "train_X = train_X.astype(float)\n",
    "test_X = test_X.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dtrain = xgb.DMatrix(train_X, label = train_Y)\n",
    "dtest = xgb.DMatrix(test_X)\n",
    "\n",
    "bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "\n",
    "y_test_pred = bst.predict(dtest)\n",
    "\n",
    "MSE_dif_fp_test = np.sqrt(np.mean(abs(np.array([10**x for x in np.reshape(test_Y, (-1))]) - np.array([10**x for x in y_test_pred]))**2))\n",
    "R2_dif_fp_test = r2_score(np.array([10**x for x in np.reshape(test_Y, (-1))]), np.array([10**x for x in y_test_pred]))\n",
    "Pearson = stats.pearsonr(np.array([10**x for x in np.reshape(test_Y, (-1))]), np.array([10**x for x in y_test_pred]))\n",
    "\n",
    "# MSE_dif_fp_test = np.mean(abs(np.reshape(test_Y, (-1)) - y_test_pred)**2)\n",
    "# R2_dif_fp_test = r2_score(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "# Pearson = stats.pearsonr(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "\n",
    "print(Pearson, MSE_dif_fp_test, R2_dif_fp_test)\n",
    "\n",
    "pickle.dump(bst, open(join(\"..\", \"..\", \"data\", \"saved_models\",\n",
    "                          \"xgboost_train_and_test.pkl\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = bst.get_score()\n",
    "print(len(data_train[\"ESM2\"][1]))\n",
    "print(len(data_train[\"MACCS FP\"][1]))\n",
    "for key, value in importances.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (e) Validating model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_X = np.array(list(data_val[\"difference_fp\"]))\n",
    "val_X = np.concatenate([val_X, np.array(list(data_val[\"Temperature\"]))[:, np.newaxis], np.array(list(data_val[\"pH\"]))[:, np.newaxis]], axis = 1)\n",
    "val_Y = np.array(list(data_val[\"log10_Km\"]))\n",
    "\n",
    "val_X = val_X.astype(float)\n",
    "\n",
    "dval = xgb.DMatrix(val_X)\n",
    "\n",
    "y_val_pred = bst.predict(dval)\n",
    "data_val[\"Estimate Km\"] = y_val_pred\n",
    "\n",
    "MSE_dif_fp_val = np.sqrt(np.mean(abs(np.array([10**x for x in np.reshape(val_Y, (-1))]) - np.array([10**x for x in y_val_pred]))**2))\n",
    "R2_dif_fp_val = r2_score(np.array([10**x for x in np.reshape(val_Y, (-1))]), np.array([10**x for x in y_val_pred]))\n",
    "Pearson = stats.pearsonr(np.array([10**x for x in np.reshape(val_Y, (-1))]), np.array([10**x for x in y_val_pred]))\n",
    "MAE = np.mean(abs(np.array([10**x for x in val_Y]) - np.array([10**x for x in y_val_pred])))\n",
    "MedAE = np.median(abs(np.array([10**x for x in val_Y]) - np.array([10**x for x in y_val_pred])))\n",
    "\n",
    "# MSE_dif_fp_test = np.mean(abs(np.reshape(test_Y, (-1)) - y_test_pred)**2)\n",
    "# R2_dif_fp_test = r2_score(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "# Pearson = stats.pearsonr(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "\n",
    "print(Pearson, MSE_dif_fp_val, R2_dif_fp_val, MAE, MedAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training a model with reaction and main substrate information (diff-fp/MACCS) + Temperature + pH:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Creating input matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = np.array(list(data_train[\"difference_fp\"]))\n",
    "train_X = np.concatenate([train_X, np.array(list(data_train[\"GNN FP\"])), np.array(list(data_train[\"Temperature\"]))[:, np.newaxis], np.array(list(data_train[\"pH\"]))[:, np.newaxis]], axis = 1)\n",
    "train_Y = np.array(list(data_train[\"log10_Km\"]))\n",
    "\n",
    "test_X = np.array(list(data_test[\"difference_fp\"]))\n",
    "test_X = np.concatenate([test_X, np.array(list(data_test[\"GNN FP\"])), np.array(list(data_test[\"Temperature\"]))[:, np.newaxis], np.array(list(data_test[\"pH\"]))[:, np.newaxis]], axis = 1)\n",
    "test_Y = np.array(list(data_test[\"log10_Km\"]))\n",
    "\n",
    "train_X = train_X.astype(float)\n",
    "test_X = test_X.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Hyperparameter optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [06:52<00:00, 20.64s/trial, best loss: -0.35898453987653944]\n",
      "{'learning_rate': 0.01, 'max_delta_step': 5.0, 'max_depth': 5.0, 'min_child_weight': 3.0, 'num_rounds': 1000, 'reg_alpha': 0.6000000000000001, 'reg_lambda': 0.8}\n"
     ]
    }
   ],
   "source": [
    "def cross_validation_mse_gradient_boosting(param):\n",
    "    num_round = param[\"num_rounds\"]\n",
    "    del param[\"num_rounds\"]\n",
    "    param[\"max_depth\"] = int(np.round(param[\"max_depth\"]))\n",
    "    # param[\"device\"] = \"cuda\"\n",
    "    param[\"tree_method\"] = \"hist\"\n",
    "    param[\"sampling_method\"] = \"gradient_based\"\n",
    "    \n",
    "    MSE = []\n",
    "    R2 = []\n",
    "    for i in range(5):\n",
    "        train_index, test_index  = train_indices[i], test_indices[i]\n",
    "        dtrain = xgb.DMatrix(train_X[train_index], label = train_Y[train_index])\n",
    "        dvalid = xgb.DMatrix(train_X[test_index])\n",
    "        bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "        y_valid_pred = bst.predict(dvalid)\n",
    "        MSE.append(np.mean(abs(np.reshape(train_Y[test_index], (-1)) - y_valid_pred)**2))\n",
    "        R2.append(r2_score(np.reshape(train_Y[test_index], (-1)),  y_valid_pred))\n",
    "    return(-np.mean(R2))\n",
    "\n",
    "space_gradient_boosting = {\n",
    "    \"learning_rate\": hp.choice(\"learning_rate\", [0.01,0.1,0.2]),\n",
    "    \"max_depth\": hp.quniform(\"max_depth\", 3, 10, 1),\n",
    "    # \"subsample\": hp.quniform(\"subsample\", 0.5, 1, 0.5),\n",
    "    \"reg_lambda\": hp.quniform(\"reg_lambda\", 0, 1, 0.2),\n",
    "    \"reg_alpha\": hp.quniform(\"reg_alpha\", 0, 1, 0.2),\n",
    "    \"max_delta_step\": hp.quniform(\"max_delta_step\", 1, 5, 1),\n",
    "    \"min_child_weight\": hp.quniform(\"min_child_weight\", 1, 6, 1),\n",
    "    \"num_rounds\":  hp.choice(\"num_rounds\", [100,250,500,1000])\n",
    "    }\n",
    "\n",
    "rstate = np.random.default_rng(42)\n",
    "trials = Trials()\n",
    "best = fmin(fn = cross_validation_mse_gradient_boosting, space = space_gradient_boosting,\n",
    "            algo=rand.suggest, max_evals = 20, trials=trials, return_argmin=False, rstate=rstate)\n",
    "\n",
    "print(best)\n",
    "param = best\n",
    "param[\"random_state\"] = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if split == \"full\":\n",
    "    param = {\"random_state\": 42, 'learning_rate': 0.01, 'max_delta_step': 2.0, 'max_depth': 8.0, 'min_child_weight': 5.0, 'num_rounds': 500, 'reg_alpha': 0.4, 'reg_lambda': 0.0}\n",
    "elif split == \"Arabidopsis\":\n",
    "    param = {\"random_state\": 42, 'learning_rate': 0.01, 'max_delta_step': 5.0, 'max_depth': 5.0, 'min_child_weight': 3.0, 'num_rounds': 1000, 'reg_alpha': 0.6, 'reg_lambda': 0.8}\n",
    "elif split == \"Brassicaceae\":\n",
    "    param = {\"random_state\": 42, 'learning_rate': 0.2, 'max_delta_step': 3.0, 'max_depth': 5.0, 'min_child_weight': 2.0, 'num_rounds': 500, 'reg_alpha': 0.8, 'reg_lambda': 0.4}\n",
    "elif split == \"wildtype\":\n",
    "    param = {\"random_state\": 42, 'learning_rate': 0.01, 'max_delta_step': 5.0, 'max_depth': 11.0, 'min_child_weight': 3.0, 'num_rounds': 1000, 'reg_alpha': 0.6, 'reg_lambda': 0.8}\n",
    "else:\n",
    "    param = {\"random_state\": 42, 'learning_rate': 0.1, 'max_delta_step': 3.0, 'max_depth': 4.0, 'min_child_weight': 5.0, 'num_rounds': 500, 'reg_alpha': 0.2, 'reg_lambda': 0.4}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c) Training and validating model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_round = param[\"num_rounds\"]\n",
    "param[\"max_depth\"] = int(np.round(param[\"max_depth\"]))\n",
    "\n",
    "del param[\"num_rounds\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R2 = []\n",
    "MSE = []\n",
    "Pearson = []\n",
    "MAE = []\n",
    "MedAE = []\n",
    "\n",
    "for i in range(5):\n",
    "    train_index, test_index  = train_indices[i], test_indices[i]\n",
    "    dtrain = xgb.DMatrix(train_X[train_index], label = train_Y[train_index])\n",
    "    dvalid = xgb.DMatrix(train_X[test_index])\n",
    "    \n",
    "    bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "    \n",
    "    y_valid_pred = bst.predict(dvalid)\n",
    "    MSE.append(np.sqrt(np.mean(abs(np.array([10**x for x in np.reshape(train_Y[test_index], (-1))]) - np.array([10**x for x in y_valid_pred]))**2)))\n",
    "    R2.append(r2_score([10**x for x in np.reshape(train_Y[test_index], (-1))], [10**x for x in y_valid_pred]))\n",
    "    Pearson.append(stats.pearsonr([10**x for x in np.reshape(train_Y[test_index], (-1))], [10**x for x in y_valid_pred])[0])\n",
    "    MAE.append(np.mean(abs(np.array([10**x for x in train_Y[test_index]]) - np.array([10**x for x in y_valid_pred]))))\n",
    "    MedAE.append(np.median(abs(np.array([10**x for x in train_Y[test_index]]) - np.array([10**x for x in y_valid_pred]))))\n",
    "\n",
    "    # MSE.append(np.mean(abs(np.reshape(train_Y[test_index], (-1)) - y_valid_pred)**2))\n",
    "    # R2.append(r2_score(np.reshape(train_Y[test_index], (-1)), y_valid_pred))\n",
    "    # Pearson.append(stats.pearsonr(np.reshape(train_Y[test_index], (-1)), y_valid_pred)[0])\n",
    "\n",
    "print(Pearson)\n",
    "print(MSE)\n",
    "print(R2)\n",
    "print(MAE)\n",
    "print(MedAE)\n",
    "\n",
    "np.save(join(\"..\", \"..\", \"data\", split, \"Pearson_CV_xgboost_gnn_fp_diff_fp.npy\"), np.array(Pearson))\n",
    "np.save(join(\"..\", \"..\", \"data\", split, \"MSE_CV_xgboost_gnn_fp_diff_fp.npy\"), np.array(MSE))\n",
    "np.save(join(\"..\", \"..\", \"data\", split, \"R2_CV_xgboost_gnn_fp_diff_fp.npy\"), np.array(R2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(train_X, label = train_Y)\n",
    "dtest = xgb.DMatrix(test_X, label = test_Y)\n",
    "\n",
    "bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "\n",
    "y_test_pred = bst.predict(dtest)\n",
    "\n",
    "data_test[\"Estimate Km\"] = y_test_pred\n",
    "\n",
    "MSE_dif_fp_test = np.sqrt(np.mean(abs(np.array([10**x for x in test_Y]) - np.array([10**x for x in y_test_pred]))**2))\n",
    "R2_dif_fp_test = r2_score(np.array([10**x for x in test_Y]), np.array([10**x for x in y_test_pred]))\n",
    "Pearson = stats.pearsonr(np.array([10**x for x in test_Y]), np.array([10**x for x in y_test_pred]))\n",
    "MAE = np.mean(abs(np.array([10**x for x in test_Y]) - np.array([10**x for x in y_test_pred])))\n",
    "MedAE = np.median(abs(np.array([10**x for x in test_Y]) - np.array([10**x for x in y_test_pred])))\n",
    "\n",
    "# MSE_dif_fp_test = np.mean(abs(np.reshape(test_Y, (-1)) - y_test_pred)**2)\n",
    "# R2_dif_fp_test = r2_score(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "# Pearson = stats.pearsonr(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "# MAE = np.mean(abs(np.reshape(test_Y, (-1)) - y_test_pred))\n",
    "\n",
    "print(np.round(Pearson[0],20) , np.round(MSE_dif_fp_test, 10), np.round(R2_dif_fp_test,3), np.round(MAE, 10), np.round(MedAE, 10))\n",
    "\n",
    "\n",
    "np.save(join(\"..\", \"..\", \"data\", split, \"y_test_pred_xgboost_gnn_fp_diff_fp.npy\"), bst.predict(dtest))\n",
    "np.save(join(\"..\", \"..\", \"data\", split,  \"y_test_true_xgboost_gnn_fp_diff_fp.npy\"), test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred_esm1b_ts_drfp = y_test_pred\n",
    "# data_test[\"Estimate Km\"] = [10**x for x in data_test[\"Estimate Km\"]]\n",
    "# filtered_df = data_test[data_test['Uniprot IDs'].apply(lambda x: \"Q9LE06\" in x)]\n",
    "# filtered_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (d) Training model with test and train data for production mode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = np.array(list(data_train[\"difference_fp\"]))\n",
    "train_X = np.concatenate([train_X, np.array(list(data_train[\"GNN FP\"])), np.array(list(data_train[\"Temperature\"]))[:, np.newaxis], np.array(list(data_train[\"pH\"]))[:, np.newaxis]], axis = 1)\n",
    "train_Y = np.array(list(data_train[\"log10_Km\"]))\n",
    "\n",
    "test_X = np.array(list(data_test[\"difference_fp\"]))\n",
    "test_X = np.concatenate([test_X, np.array(list(data_test[\"GNN FP\"])), np.array(list(data_test[\"Temperature\"]))[:, np.newaxis], np.array(list(data_test[\"pH\"]))[:, np.newaxis]], axis = 1)\n",
    "test_Y = np.array(list(data_test[\"log10_Km\"]))\n",
    "\n",
    "train_X = np.concatenate([train_X, test_X])\n",
    "train_Y = np.concatenate([train_Y, test_Y])\n",
    "\n",
    "train_X = train_X.astype(float)\n",
    "test_X = test_X.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dtrain = xgb.DMatrix(train_X, label = train_Y)\n",
    "dtest = xgb.DMatrix(test_X)\n",
    "\n",
    "bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "\n",
    "y_test_pred = bst.predict(dtest)\n",
    "\n",
    "MSE_dif_fp_test = np.sqrt(np.mean(abs(np.array([10**x for x in np.reshape(test_Y, (-1))]) - np.array([10**x for x in y_test_pred]))**2))\n",
    "R2_dif_fp_test = r2_score(np.array([10**x for x in np.reshape(test_Y, (-1))]), np.array([10**x for x in y_test_pred]))\n",
    "Pearson = stats.pearsonr(np.array([10**x for x in np.reshape(test_Y, (-1))]), np.array([10**x for x in y_test_pred]))\n",
    "\n",
    "# MSE_dif_fp_test = np.mean(abs(np.reshape(test_Y, (-1)) - y_test_pred)**2)\n",
    "# R2_dif_fp_test = r2_score(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "# Pearson = stats.pearsonr(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "\n",
    "print(Pearson, MSE_dif_fp_test, R2_dif_fp_test)\n",
    "\n",
    "pickle.dump(bst, open(join(\"..\", \"..\", \"data\", \"saved_models\",\n",
    "                          \"xgboost_train_and_test.pkl\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = bst.get_score()\n",
    "print(len(data_train[\"ESM2\"][1]))\n",
    "print(len(data_train[\"MACCS FP\"][1]))\n",
    "for key, value in importances.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (e) Validating model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_X = np.array(list(data_val[\"difference_fp\"]))\n",
    "val_X = np.concatenate([val_X, np.array(list(data_val[\"GNN FP\"])), np.array(list(data_val[\"Temperature\"]))[:, np.newaxis], np.array(list(data_val[\"pH\"]))[:, np.newaxis]], axis = 1)\n",
    "val_Y = np.array(list(data_val[\"log10_Km\"]))\n",
    "\n",
    "val_X = val_X.astype(float)\n",
    "\n",
    "dval = xgb.DMatrix(val_X)\n",
    "\n",
    "y_val_pred = bst.predict(dval)\n",
    "data_val[\"Estimate Km\"] = y_val_pred\n",
    "\n",
    "MSE_dif_fp_val = np.sqrt(np.mean(abs(np.array([10**x for x in np.reshape(val_Y, (-1))]) - np.array([10**x for x in y_val_pred]))**2))\n",
    "R2_dif_fp_val = r2_score(np.array([10**x for x in np.reshape(val_Y, (-1))]), np.array([10**x for x in y_val_pred]))\n",
    "Pearson = stats.pearsonr(np.array([10**x for x in np.reshape(val_Y, (-1))]), np.array([10**x for x in y_val_pred]))\n",
    "MAE = np.mean(abs(np.array([10**x for x in val_Y]) - np.array([10**x for x in y_val_pred])))\n",
    "MedAE = np.median(abs(np.array([10**x for x in val_Y]) - np.array([10**x for x in y_val_pred])))\n",
    "\n",
    "# MSE_dif_fp_test = np.mean(abs(np.reshape(test_Y, (-1)) - y_test_pred)**2)\n",
    "# R2_dif_fp_test = r2_score(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "# Pearson = stats.pearsonr(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "\n",
    "print(Pearson, MSE_dif_fp_val, R2_dif_fp_val, MAE, MedAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full = [9.141233079444068e-05, 7.185469398106053e-05, 6.779205666964775e-05, 5.96719217929302e-05, 7.806283319511387e-05, 0.00011095907130526096, 8.556763581546147e-05, 8.090209374754628e-05, 7.088546213628789e-05, 9.031643212413764e-05, 0.000106806798644459, 7.039732784429031e-05, 6.655172476534265e-05, 5.591630331957085e-05, 8.124002867417312e-05, 0.00010424289224102824, 8.6995878283547e-05, 9.44389948215191e-05, 7.551597968101089e-05, 9.822564879007027e-05, 8.861449467652537e-05, 6.956511460399432e-05, 6.233777065708902e-05, 7.836152608234311e-05, 7.90582900723007e-05, 7.591363653986654e-05, 8.024010564610054e-05, 6.551532660477316e-05, 7.240968464428222e-05, 5.994156070145586e-05, 7.706335402701757e-05, 8.957750723805123e-05, 6.158057943097745e-05, 6.33903481051542e-05, 7.143422110306562e-05]\n",
    "brassicaceae = [0.00012861803923586902, 0.00013592122642437635, 0.00016953472741893872, 0.00016544247917479126, 7.504736659537995e-05, 0.00011003290031336133, 0.00010734037406557226, 0.00015598826161281493, 0.00015940822333169823, 8.885073701780122e-05, 0.00012405584666604796, 8.922347907590305e-05, 0.00020498591132531223, 0.0001534025077799822, 6.117504662668341e-05, 0.00014033601713363744, 0.00012031620801492716, 0.00016438915737323592, 0.00018159250125045467, 7.950262291525943e-05, 0.0001496361145257905, 0.00010170893433303182, 0.0001331782321530527, 0.0001260355619003031, 8.849189405287189e-05, 0.0001218958574198203, 9.161366497610493e-05, 0.00022332754573037994, 0.00020031401594821182, 6.990201658702624e-05, 0.00011276266466130644, 7.814400632082885e-05, 0.00017219325129604095, 0.00020306271045665678, 7.024439733436467e-05]\n",
    "arabidopsis = [0.0001385939947244797, 0.0003225501988292827, 0.00010906913312004217, 0.00010887881318327493, 8.873553662629334e-05, 0.0001554828974106205, 0.00031584677057089577, 0.0001352365579731054, 0.00016969154532771563, 9.304810140266678e-05, 0.000142612658387255, 0.00035761387851831745, 0.00012801318773759019, 0.0001302173312618832, 0.00011026147455180972, 0.00011402768058943759, 0.0003401990237432721, 9.903906449737598e-05, 0.00015074928561894122, 7.99712648245992e-05, 0.00010148316662771834, 0.00044956245154539735, 0.00011820092940687042, 0.00010743870596746782, 0.00013668912943172378, 0.00013861294116593062, 0.00040941064574894875, 0.00015566865924278577, 0.00019631681032590786, 7.870489836642927e-05, 0.00013939675380045144, 0.0004235949986973696, 0.0001445173909740725, 0.00010300062674683532, 0.00011597935954225366]\n",
    "wildtype = [8.259021273648908e-05, 8.095321513092956e-05, 7.025546432255039e-05, 6.475155507260874e-05, 9.235940230751968e-05, 7.808948846054124e-05, 8.123819549625476e-05, 7.403454102200975e-05, 8.224451407764035e-05, 9.798687020432531e-05, 9.700267538664681e-05, 8.098409463363205e-05, 7.722589257511085e-05, 8.504068703586978e-05, 0.000105410984400949, 7.154745243175039e-05, 7.8405798765367e-05, 8.153640776648242e-05, 8.292243366385591e-05, 9.741348008576445e-05, 0.00010563756878303229, 8.76402174211743e-05, 0.00010087896126015466, 7.287416185368812e-05, 9.282946436420189e-05, 9.515820738318332e-05, 9.865915396106565e-05, 7.511079244226174e-05, 9.8249441459734e-05, 9.904110821267647e-05, 9.601109623909799e-05, 7.551026075169523e-05, 8.011270590605897e-05, 6.849589917136421e-05, 0.00011013776741499104]\n",
    "secondary = [4.1865121857082144e-05, 1.5248464212633977e-05, 1.762946309001216e-05, 2.490695027297198e-05, 1.9996619625720338e-05, 4.241157827784889e-05, 2.9426627425833845e-05, 1.9249999747656058e-05, 2.005881143658347e-05, 1.7905232721255534e-05, 4.752932485884811e-05, 1.6715754375953555e-05, 1.580756175020339e-05, 2.5520892292153867e-05, 1.6348933254160495e-05, 4.874477077021861e-05, 2.859975123930354e-05, 2.229612507654767e-05, 2.0910078665820617e-05, 2.0700582392129356e-05, 4.220708133420373e-05, 2.4574694116364415e-05, 2.189621952422675e-05, 2.8525490462280695e-05, 1.7582413293540527e-05, 3.8592798568759065e-05, 1.6754433531361163e-05, 2.5360049841022373e-05, 2.3044874792805162e-05, 1.4855876023789778e-05, 4.008848722349993e-05, 1.7643661023600644e-05, 2.186878365246612e-05, 2.7783601222524153e-05, 1.663535744708803e-05]\n",
    "\n",
    "data_frame = pd.DataFrame({\"Seed plants\": full, \"Brassicaceae\": brassicaceae, \"Arabidopsis\" : arabidopsis,\n",
    "                             \"Wildtype\" : wildtype, \"Secondary metabolites\" : secondary}).melt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotnine import (\n",
    "    ggplot,\n",
    "    aes,\n",
    "    geom_boxplot,\n",
    "    labs,\n",
    "    theme_classic,\n",
    "    scale_x_discrete\n",
    ")\n",
    "\n",
    "import textwrap\n",
    "\n",
    "(\n",
    "    ggplot(data_frame, aes(x = \"variable\", y=\"value\"))\n",
    "    + geom_boxplot(aes(fill=\"variable\"), notch=True) \n",
    "    + scale_x_discrete(labels= [textwrap.fill(label, width=9) for label in [\"Arabidopsis\",\"Brassicaceae\", \"Secondary metabolites\", \"Seed plants\", \"Wildtype\"]])\n",
    "    + labs(title=\"Median Absolute Errors of CV of different models\", x=\"Dataset\", y=\"MedAE\", fill=\"Dataset\")\n",
    "    + theme_classic(base_size=16)\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
