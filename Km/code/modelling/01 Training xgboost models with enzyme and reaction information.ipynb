{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import os\n",
    "from os.path import join\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy import stats\n",
    "import xgboost as xgb\n",
    "import random\n",
    "from hyperopt import fmin, rand, hp, Trials\n",
    "rstate = np.random.default_rng(42)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import matplotlib as mpl\n",
    "\n",
    "datasets_dir = \"../../data\"\n",
    "# plt.style.use('CCB_plot_style_0v4.mplstyle')\n",
    "# c_styles      = mpl.rcParams['axes.prop_cycle'].by_key()['color']   # fetch the defined color styles\n",
    "# high_contrast = ['#004488', '#DDAA33', '#BB5566', '#000000']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading training, testing and validation data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(679, 197, 20)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split = \"wildtype\"\n",
    "\n",
    "data_train = pd.read_pickle(join(datasets_dir, \"splits\", split, \"training_data.pkl\"))\n",
    "data_test = pd.read_pickle(join(datasets_dir, \"splits\", split, \"test_data.pkl\"))\n",
    "data_val = pd.read_pickle(join(datasets_dir, \"splits\", split, \"val_data.pkl\"))\n",
    "\n",
    "# data_train[\"geomean_Km\"] = np.log10(data_train[\"geomean_Km\"])\n",
    "# data_test[\"geomean_Km\"] = np.log10(data_test[\"geomean_Km\"])\n",
    "\n",
    "data_train[\"log10_Km\"] = np.log10(data_train[\"Km\"])\n",
    "data_test[\"log10_Km\"] = np.log10(data_test[\"Km\"])\n",
    "data_val[\"log10_Km\"] = np.log10(data_val[\"Km\"])\n",
    "\n",
    "data_train.rename(columns = {\"Enzyme rep\" : \"ESM2\"}, inplace = True)\n",
    "data_test.rename(columns = {\"Enzyme rep\" : \"ESM2\"}, inplace = True)\n",
    "data_val.rename(columns = {\"Enzyme rep\" : \"ESM2\"}, inplace = True)\n",
    "\n",
    "data_train['Temperature'] = data_train['Temperature'].replace('-', np.nan)\n",
    "data_test['Temperature'] = data_test['Temperature'].replace('-', np.nan)\n",
    "data_val['Temperature'] = data_val['Temperature'].replace('-', np.nan)\n",
    "data_train['pH'] = data_train['pH'].replace('-', np.nan)\n",
    "data_test['pH'] = data_test['pH'].replace('-', np.nan)\n",
    "data_val['pH'] = data_val['pH'].replace('-', np.nan)\n",
    "data_train['Type'] = data_train['Type'].replace('wildtype', 1)\n",
    "data_train['Type'] = data_train['Type'].replace('mutant', 2)\n",
    "data_test['Type'] = data_test['Type'].replace('wildtype', 1)\n",
    "data_test['Type'] = data_test['Type'].replace('mutant', 2)\n",
    "data_val['Type'] = data_val['Type'].replace('wildtype', 1)\n",
    "data_val['Type'] = data_val['Type'].replace('mutant', 2)\n",
    "\n",
    "data_train['MACCS FP'] = data_train['MACCS FP'].astype(str)\n",
    "data_test['MACCS FP'] = data_test['MACCS FP'].astype(str)\n",
    "data_val['MACCS FP'] = data_val['MACCS FP'].astype(str)\n",
    "\n",
    "len(data_train), len(data_test), len(data_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices = list(np.load(join(datasets_dir, \"splits\", split, \"CV_train_indices_Seed plants.npy\"), allow_pickle = True))\n",
    "test_indices = list(np.load(join(datasets_dir, \"splits\", split, \"CV_test_indices_Seed plants.npy\"), allow_pickle = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Training a model with enzyme and main substrate information (ESM-2/MACCS) + Temperature + pH :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Creating input matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = np.array(list(data_train[\"ESM2\"]))\n",
    "train_X = np.concatenate([train_X, np.array(list(data_train['GNN FP'])), np.array(list(data_train[\"Temperature\"]))[:, np.newaxis], np.array(list(data_train[\"pH\"]))[:, np.newaxis]], axis = 1)\n",
    "train_Y = np.array(list(data_train[\"log10_Km\"]))\n",
    "\n",
    "test_X = np.array(list(data_test[\"ESM2\"]))\n",
    "test_X = np.concatenate([test_X, np.array(list(data_test['GNN FP'])), np.array(list(data_test[\"Temperature\"]))[:, np.newaxis], np.array(list(data_test[\"pH\"]))[:, np.newaxis]], axis = 1)\n",
    "test_Y = np.array(list(data_test[\"log10_Km\"]))\n",
    "\n",
    "train_X = train_X.astype(float)\n",
    "test_X = test_X.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Hyperparameter optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_mse_gradient_boosting(param):\n",
    "    num_round = param[\"num_rounds\"]\n",
    "    del param[\"num_rounds\"]\n",
    "    param[\"max_depth\"] = int(np.round(param[\"max_depth\"]))\n",
    "    # param[\"device\"] = \"cuda\"\n",
    "    param[\"tree_method\"] = \"hist\"\n",
    "    param[\"sampling_method\"] = \"gradient_based\"\n",
    "    \n",
    "    MSE = []\n",
    "    R2 = []\n",
    "    for i in range(5):\n",
    "        train_index, test_index  = train_indices[i], test_indices[i]\n",
    "        dtrain = xgb.DMatrix(train_X[train_index], label = train_Y[train_index])\n",
    "        dvalid = xgb.DMatrix(train_X[test_index])\n",
    "        bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "        y_valid_pred = bst.predict(dvalid)\n",
    "        MSE.append(np.mean(abs(np.reshape(train_Y[test_index], (-1)) - y_valid_pred)**2))\n",
    "        R2.append(r2_score(np.reshape(train_Y[test_index], (-1)),  y_valid_pred))\n",
    "    return(-np.mean(R2))\n",
    "\n",
    "space_gradient_boosting = {\n",
    "    \"learning_rate\": hp.choice(\"learning_rate\", np.linspace(0.01, 0.1, 10)),\n",
    "    \"max_depth\": hp.choice(\"max_depth\", np.linspace(3,10,8)),\n",
    "    #\"subsample\": hp.quniform(\"subsample\", 0.5, 1),\n",
    "    \"reg_lambda\": hp.choice(\"reg_lambda\", np.linspace(0, 1, 11)),\n",
    "    \"reg_alpha\": hp.choice(\"reg_alpha\", np.linspace(0, 1, 11)),\n",
    "    \"max_delta_step\": hp.choice(\"max_delta_step\", np.linspace(1, 5, 5)),\n",
    "    \"min_child_weight\": hp.choice(\"min_child_weight\", np.linspace(1, 6, 6)),\n",
    "    \"num_rounds\":  hp.choice(\"num_rounds\", np.linspace(50, 200, 4))}\n",
    "\n",
    "\n",
    "trials = Trials()\n",
    "best = fmin(fn = cross_validation_mse_gradient_boosting, space = space_gradient_boosting,\n",
    "            algo=rand.suggest, max_evals = 10, trials=trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "full dataset : {'learning_rate': 0.1, 'max_delta_step': 2, 'max_depth': 5, 'min_child_weight': 1, 'num_rounds': 200, 'reg_alpha': 0.6, 'reg_lambda': 0.4}\n",
    "brassicaceae: {'learning_rate': 0.09, 'max_delta_step': 2, 'max_depth': 7, 'min_child_weight': 4, 'num_rounds': 100, 'reg_alpha': 0.4, 'reg_lambda': 0.4}\n",
    "arabidopsis : {'learning_rate': 0.05, 'max_delta_step': 5, 'max_depth': 3, 'min_child_weight': 6, 'num_rounds': 150, 'reg_alpha': 0.6, 'reg_lambda': 0.3}\n",
    "wildtype : {'learning_rate': 0.1, 'max_delta_step': 5, 'max_depth': 4, 'min_child_weight': 3, 'num_rounds': 100, 'reg_alpha': 0.4, 'reg_lambda': 0.6}\n",
    "secondary : {'learning_rate': 0.09, 'max_delta_step': 1, 'max_depth': 4, 'min_child_weight': 5, 'num_rounds': 200, 'reg_alpha': 0.9, 'reg_lambda': 0.2}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c) Training and validating model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\"random_state\": 42, 'learning_rate': 0.1, 'max_delta_step': 5, 'max_depth': 4, 'min_child_weight': 3, 'num_rounds': 100, 'reg_alpha': 0.4, 'reg_lambda': 0.6}\n",
    "\n",
    "num_round = param[\"num_rounds\"]\n",
    "param[\"max_depth\"] = int(np.round(param[\"max_depth\"]))\n",
    "\n",
    "del param[\"num_rounds\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R2 = []\n",
    "MSE = []\n",
    "Pearson = []\n",
    "MAE = []\n",
    "MedAE = []\n",
    "\n",
    "for i in range(5):\n",
    "    train_index, test_index  = train_indices[i], test_indices[i]\n",
    "    dtrain = xgb.DMatrix(train_X[train_index], label = train_Y[train_index])\n",
    "    dvalid = xgb.DMatrix(train_X[test_index])\n",
    "    \n",
    "    bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "    \n",
    "    y_valid_pred = bst.predict(dvalid)\n",
    "    MSE.append(np.sqrt(np.mean(abs(np.array([10**x for x in np.reshape(train_Y[test_index], (-1))]) - np.array([10**x for x in y_valid_pred]))**2)))\n",
    "    R2.append(r2_score([10**x for x in np.reshape(train_Y[test_index], (-1))], [10**x for x in y_valid_pred]))\n",
    "    Pearson.append(stats.pearsonr([10**x for x in np.reshape(train_Y[test_index], (-1))], [10**x for x in y_valid_pred])[0])\n",
    "    MAE.append(np.mean(np.array([10**x for x in train_Y[test_index]]) - np.array([10**x for x in y_valid_pred])))\n",
    "    MedAE.append(np.median(np.array([10**x for x in train_Y[test_index]]) - np.array([10**x for x in y_valid_pred])))\n",
    "\n",
    "    # MSE.append(np.mean(abs(np.reshape(train_Y[test_index], (-1)) - y_valid_pred)**2))\n",
    "    # R2.append(r2_score(np.reshape(train_Y[test_index], (-1)), y_valid_pred))\n",
    "    # Pearson.append(stats.pearsonr(np.reshape(train_Y[test_index], (-1)), y_valid_pred)[0])\n",
    "\n",
    "print(Pearson)\n",
    "print(MSE)\n",
    "print(R2)\n",
    "print(MAE)\n",
    "print(MedAE)\n",
    "\n",
    "np.save(join(\"..\", \"..\", \"data\", \"Pearson_CV_xgboost_ESM1b_ts_diff_fp.npy\"), np.array(Pearson))\n",
    "np.save(join(\"..\", \"..\", \"data\", \"MSE_CV_xgboost_ESM1b_ts_diff_fp.npy\"), np.array(MSE))\n",
    "np.save(join(\"..\", \"..\", \"data\", \"R2_CV_xgboost_ESM1b_ts_diff_fp.npy\"), np.array(R2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15562679597462648 0.0015712074 -0.082 0.0005632122 7.43082e-05\n"
     ]
    }
   ],
   "source": [
    "dtrain = xgb.DMatrix(train_X, label = train_Y)\n",
    "dtest = xgb.DMatrix(test_X, label = test_Y)\n",
    "\n",
    "bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "\n",
    "y_test_pred = bst.predict(dtest)\n",
    "\n",
    "data_test[\"Estimate Km\"] = y_test_pred\n",
    "\n",
    "MSE_dif_fp_test = np.sqrt(np.mean(abs(np.array([10**x for x in test_Y]) - np.array([10**x for x in y_test_pred]))**2))\n",
    "R2_dif_fp_test = r2_score(np.array([10**x for x in test_Y]), np.array([10**x for x in y_test_pred]))\n",
    "Pearson = np.corrcoef(np.array([10**x for x in test_Y]), np.array([10**x for x in y_test_pred]))\n",
    "MAE = np.mean(abs(np.array([10**x for x in test_Y]) - np.array([10**x for x in y_test_pred])))\n",
    "MedAE = np.median(abs(np.array([10**x for x in test_Y]) - np.array([10**x for x in y_test_pred])))\n",
    "\n",
    "# MSE_dif_fp_test = np.mean(abs(np.reshape(test_Y, (-1)) - y_test_pred)**2)\n",
    "# R2_dif_fp_test = r2_score(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "# Pearson = stats.pearsonr(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "# MAE = np.mean(abs(np.reshape(test_Y, (-1)) - y_test_pred))\n",
    "\n",
    "print(np.round(Pearson[0,1],20) , np.round(MSE_dif_fp_test, 10), np.round(R2_dif_fp_test,3), np.round(MAE, 10), np.round(MedAE, 10))\n",
    "\n",
    "\n",
    "np.save(join(\"..\", \"..\", \"data\", \"y_test_pred_xgboost_ESM1b_ts_diff_fp.npy\"), bst.predict(dtest))\n",
    "np.save(join(\"..\", \"..\", \"data\",  \"y_test_true_xgboost_ESM1b_ts_diff_fp.npy\"), test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred_esm1b_ts_drfp = y_test_pred\n",
    "# data_test[\"Estimate Km\"] = [10**x for x in data_test[\"Estimate Km\"]]\n",
    "# filtered_df = data_test[data_test['Uniprot IDs'].apply(lambda x: \"Q9LE06\" in x)]\n",
    "# filtered_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (d) Training model with test and train data for production mode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = np.array(list(data_train[\"ESM2\"]))\n",
    "train_X = np.concatenate([train_X, np.array(list(data_train['GNN FP'])), np.array(list(data_train[\"Temperature\"]))[:, np.newaxis], np.array(list(data_train[\"pH\"]))[:, np.newaxis]], axis = 1)\n",
    "train_Y = np.array(list(data_train[\"log10_Km\"]))\n",
    "\n",
    "test_X = np.array(list(data_test[\"ESM2\"]))\n",
    "test_X = np.concatenate([test_X, np.array(list(data_test['GNN FP'])), np.array(list(data_test[\"Temperature\"]))[:, np.newaxis], np.array(list(data_test[\"pH\"]))[:, np.newaxis]], axis = 1)\n",
    "test_Y = np.array(list(data_test[\"log10_Km\"]))\n",
    "\n",
    "train_X = np.concatenate([train_X, test_X])\n",
    "train_Y = np.concatenate([train_Y, test_Y])\n",
    "\n",
    "train_X = train_X.astype(float)\n",
    "test_X = test_X.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\"random_state\": 42, 'learning_rate': 0.09, 'max_delta_step': 1, 'max_depth': 4, 'min_child_weight': 5, 'num_rounds': 200, 'reg_alpha': 0.9, 'reg_lambda': 0.2}\n",
    "         \n",
    "num_round = param[\"num_rounds\"]\n",
    "param[\"max_depth\"] = int(np.round(param[\"max_depth\"]))\n",
    "\n",
    "del param[\"num_rounds\"]\n",
    "\n",
    "dtrain = xgb.DMatrix(train_X, label = train_Y)\n",
    "dtest = xgb.DMatrix(test_X)\n",
    "\n",
    "bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "\n",
    "y_test_pred = bst.predict(dtest)\n",
    "\n",
    "MSE_dif_fp_test = np.sqrt(np.mean(abs(np.array([10**x for x in np.reshape(test_Y, (-1))]) - np.array([10**x for x in y_test_pred]))**2))\n",
    "R2_dif_fp_test = r2_score(np.array([10**x for x in np.reshape(test_Y, (-1))]), np.array([10**x for x in y_test_pred]))\n",
    "Pearson = stats.pearsonr(np.array([10**x for x in np.reshape(test_Y, (-1))]), np.array([10**x for x in y_test_pred]))\n",
    "\n",
    "# MSE_dif_fp_test = np.mean(abs(np.reshape(test_Y, (-1)) - y_test_pred)**2)\n",
    "# R2_dif_fp_test = r2_score(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "# Pearson = stats.pearsonr(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "\n",
    "print(Pearson, MSE_dif_fp_test, R2_dif_fp_test)\n",
    "\n",
    "pickle.dump(bst, open(join(\"..\", \"..\", \"data\", \"saved_models\",\n",
    "                          \"xgboost_train_and_test.pkl\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = bst.get_score()\n",
    "print(len(data_train[\"ESM2\"][1]))\n",
    "print(len(data_train[\"MACCS FP\"][1]))\n",
    "for key, value in importances.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Validating model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_X = np.array(list(data_val[\"ESM2\"]))\n",
    "val_X = np.concatenate([val_X, np.array(list(data_val[\"GNN FP\"])), np.array(list(data_val[\"Temperature\"]))[:, np.newaxis], np.array(list(data_val[\"pH\"]))[:, np.newaxis]], axis = 1)\n",
    "val_Y = np.array(list(data_val[\"log10_Km\"]))\n",
    "\n",
    "val_X = val_X.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(list(chain(*data_val[\"Km_values\"]))).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dval = xgb.DMatrix(val_X)\n",
    "\n",
    "y_val_pred = bst.predict(dval)\n",
    "data_val[\"Estimate Km\"] = y_val_pred\n",
    "\n",
    "MSE_dif_fp_val = np.sqrt(np.mean(abs(np.array([10**x for x in np.reshape(val_Y, (-1))]) - np.array([10**x for x in y_val_pred]))**2))\n",
    "R2_dif_fp_val = r2_score(np.array([10**x for x in np.reshape(val_Y, (-1))]), np.array([10**x for x in y_val_pred]))\n",
    "Pearson = stats.pearsonr(np.array([10**x for x in np.reshape(val_Y, (-1))]), np.array([10**x for x in y_val_pred]))\n",
    "MAE = np.mean(abs(np.array([10**x for x in val_Y]) - np.array([10**x for x in y_val_pred])))\n",
    "MedAE = np.median(abs(np.array([10**x for x in val_Y]) - np.array([10**x for x in y_val_pred])))\n",
    "\n",
    "# MSE_dif_fp_test = np.mean(abs(np.reshape(test_Y, (-1)) - y_test_pred)**2)\n",
    "# R2_dif_fp_test = r2_score(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "# Pearson = stats.pearsonr(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "\n",
    "print(Pearson, MSE_dif_fp_val, R2_dif_fp_val, MAE, MedAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.cm as cmx\n",
    "\n",
    "plt.scatter(val_Y,y_val_pred, c='blue', edgecolors='black',s=15)\n",
    "    \n",
    "plt.xlim(-6,2)\n",
    "plt.ylim(-6,2)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.xlabel('Real value', fontsize=15)\n",
    "plt.ylabel('Estimated value', fontsize=15)\n",
    "plt.title('Predictions', fontsize=15)\n",
    "plt.axline((1, 1), slope=1, c='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_combined = pd.concat([data_train, data_test],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_identity_ignore_gaps(seq1, seq2):\n",
    "    identical_residues = sum([1 for x, y in zip(seq1, seq2) if x == y and x != \"-\"])\n",
    "    pid = identical_residues / sum([1 for x in seq1 if x != \"-\"]) \n",
    "    return pid\n",
    "\n",
    "from Bio import Align\n",
    "from Bio.Align import substitution_matrices\n",
    "\n",
    "data_val[\"max_identity\"] = np.nan\n",
    "\n",
    "aligner=Align.PairwiseAligner()\n",
    "aligner.substitution_matrix = substitution_matrices.load(\"BLOSUM62\")\n",
    "aligner.mode = \"global\"\n",
    "aligner.extend_gap_score = -0.5\n",
    "aligner.open_gap_score = -10\n",
    "\n",
    "for i in data_val.index:\n",
    "    identities = []\n",
    "    for j in data_combined.index:\n",
    "        seq1 = str(data_val[\"Sequence\"][i])\n",
    "        seq2 = str(data_combined[\"Sequence\"][j])\n",
    "        if 'U' in seq1:\n",
    "            seq1 = seq1.replace('U', 'C')\n",
    "        if 'U' in seq2:\n",
    "            seq2 = seq2.replace('U', 'C')\n",
    "        alignments = aligner.align(seq1, seq2)\n",
    "        identities.append(calculate_identity_ignore_gaps(alignments[0][0], alignments[0][1]))\n",
    "    data_val[\"max_identity\"][i] = max(identities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_val[\"max_identity\"] = data_val[\"max_identity\"]*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import sklearn.metrics as sk\n",
    "import math\n",
    "\n",
    "fig, ax = plt.subplots(figsize= (10,8))\n",
    "plt.rcParams.update({'font.size': 28})\n",
    "\n",
    "splits = [\"0-40%\", \"40-80%\", \"80-99%\", \"99-100%\"]\n",
    "lower_bounds = [0,40,80,99]\n",
    "upper_bounds = [40,80,99,100]\n",
    "\n",
    "points1 = []\n",
    "points2 = []\n",
    "n_points1, n_points2 = [], []\n",
    "\n",
    "for i, split in enumerate(splits):\n",
    "\n",
    "    lb, ub = lower_bounds[i], upper_bounds[i]\n",
    "    \n",
    "    help_df = data_val.loc[data_val[\"max_identity\"]>= lb].loc[data_val[\"max_identity\"]<= ub]\n",
    "    y_true = np.array([10**x for x in help_df[\"log10_Km\"]])\n",
    "    y_pred = np.array([10**x for x in help_df[\"Estimate Km\"]])\n",
    "    n_km = len(y_pred)\n",
    "    R2 =  sk.r2_score(y_true, y_pred)\n",
    "    abs_error = abs(y_true - y_pred)\n",
    "    rmse = math.sqrt(np.mean(abs(y_true - y_pred)**2))\n",
    "    print(len(y_true))\n",
    "    print(split, R2, rmse)\n",
    "    points1.append(R2)\n",
    "    points2.append(rmse)\n",
    "    n_points1.append(n_km)\n",
    "\n",
    "\n",
    "ticks2 = np.array(range(len(splits)))\n",
    "labs = splits\n",
    "ax.set_xticks(ticks2)\n",
    "ax.set_xticklabels(labs,  y= -0.03, fontsize=26)\n",
    "ax.tick_params(axis='x', length=0, rotation = 0)\n",
    "\n",
    "# plt.ylim((-0.1,2.5))\n",
    "# plt.xlim((-0.2, 3.2))\n",
    "plt.legend(loc = \"lower right\", fontsize=20)\n",
    "plt.ylabel('RMSE')\n",
    "plt.xlabel('Enzyme sequence identity')\n",
    "ax.yaxis.set_label_coords(-0.15, 0.5)\n",
    "ax.xaxis.set_label_coords(0.5,-0.13)\n",
    "\n",
    "plt.plot([-0.15,4], [0,0], color='grey', linestyle='dashed')\n",
    "\n",
    "\n",
    "plt.plot([0,1,2,3], points2, c= \"black\", linewidth=2)\n",
    "\n",
    "for i, split in enumerate(splits):\n",
    "    points1.append(R2)\n",
    "    \n",
    "    if i ==0:\n",
    "        plt.scatter(i, points2[i], c='black', marker=\"o\", linewidths= 8)\n",
    "        ax.annotate(n_points1[i], (i-0.08, points2[i]+0.08), fontsize=17, c= \"red\", weight = \"bold\")\n",
    "\n",
    "    else:\n",
    "        plt.scatter(i, points2[i], c='black', marker=\"o\", linewidths= 8)\n",
    "        ax.annotate(n_points1[i], (i-0.08, points2[i]+0.08), fontsize=17, c= \"red\", weight = \"bold\")\n",
    "            \n",
    "     \n",
    "plt.savefig(join(\"..\",\"..\", \"data\", \"sequence_identity.png\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EC_km_pred =[[] for _ in range(6)]\n",
    "EC_km =[[] for _ in range(6)]\n",
    "for ind in data_val.index:\n",
    "    try:\n",
    "        EC = int(data_val[\"ECs\"][ind][0][0])\n",
    "        EC_km[EC-1].append(data_val[\"log10_Km\"][ind])\n",
    "        EC_km_pred[EC-1].append(data_val[\"Estimate Km\"][ind])\n",
    "    except IndexError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize= (8,8))\n",
    "plt.rcParams.update({'font.size': 28})\n",
    "\n",
    "classes = [str(i) for i in range(1,7)]\n",
    "\n",
    "for i in range(len(EC_km)):\n",
    "    \n",
    "    circle = plt.Circle((np.mean(EC_km[i]), np.mean(EC_km_pred[i]) ),\n",
    "                        np.sqrt(len(EC_km_pred[i]))/300, color='navy', fill = True)\n",
    "    ax.add_artist(circle)\n",
    "    if i ==5:\n",
    "        ax.annotate(\"EC\"+ str(i+1), (np.mean(EC_km[i])+0.01, np.mean(EC_km_pred[i])-0.05), fontsize=17, c='red', weight = \"bold\")\n",
    "    else:\n",
    "        ax.annotate(\"EC\"+ str(i+1), (np.mean(EC_km[i])+0.03, np.mean(EC_km_pred[i])-0.01), fontsize=17, c='red', weight = \"bold\")\n",
    "    \n",
    "\n",
    "# ticks2 = [0.2, 0.6,1,1.4,1.8]\n",
    "# labs = ticks2\n",
    "# ax.set_xticks(ticks2)\n",
    "# ax.set_xticklabels(labs,  y= -0.03, fontsize=26)\n",
    "# ax.tick_params(axis='x', length=0, rotation = 0)\n",
    "\n",
    "# ax.set_yticks(ticks2)\n",
    "# ax.set_yticklabels(labs,  y= -0.03, fontsize=26)\n",
    "# ax.tick_params(axis='y', length=0, rotation = 0)\n",
    "\n",
    "plt.ylim((-6,-1))\n",
    "plt.xlim((-6, -1))\n",
    "plt.legend(loc = \"upper left\", fontsize=20)\n",
    "plt.xlabel('mean measured \\n $k_{m}$ value on $\\log_{10}$-scale')\n",
    "plt.ylabel('mean predicted \\n $k_{m}$ value on $\\log_{10}$-scale')\n",
    "ax.yaxis.set_label_coords(-0.15, 0.5)\n",
    "ax.xaxis.set_label_coords(0.5,-0.13)\n",
    "plt.axline((1, 1), slope=1, c='grey', alpha = 0.3, linestyle='dashed')\n",
    "plt.savefig(join(\"..\", \"..\", \"data\", \"EC_classes_mean_km.png\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "train_fps = [np.array(list(data_combined[\"MACCS FP\"][ind])).reshape(1,-1) for ind in data_combined.index]\n",
    "test_fps = [np.array(list(data_val[\"MACCS FP\"][ind])).reshape(1,-1) for ind in data_val.index]\n",
    "\n",
    "max_sim = []\n",
    "\n",
    "for fp in test_fps:\n",
    "    jaccard_sim = np.array([1 - scipy.spatial.distance.cdist(fp,train_fp, metric='jaccard')[0][0] for train_fp in train_fps])\n",
    "    max_sim.append(np.max(jaccard_sim))\n",
    "    \n",
    "data_val[\"substrate_sim\"] = max_sim\n",
    "\n",
    "data_val[\"substrate_sim\"]= (data_val[\"substrate_sim\"] - np.min(data_val[\"substrate_sim\"]))\n",
    "data_val[\"substrate_sim\"] = data_val[\"substrate_sim\"]/np.max(data_val[\"substrate_sim\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_val[\"global_sim\"] = (data_val[\"max_identity\"]/100)*data_val[\"substrate_sim\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_val.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import scipy as sci\n",
    "help_df = data_val\n",
    "\n",
    "sim_bins_lb = [0.0, 0.4, 0.8]\n",
    "sim_bins_ub = [0.4, 0.8, 1]\n",
    "r2_scores, n_points, pearson_r, rmse = [], [], [], []\n",
    "for i in range(len(sim_bins_lb)):\n",
    "    help_df2 = help_df.loc[help_df[\"global_sim\"] <= sim_bins_ub[i]].loc[help_df[\"global_sim\"] >= sim_bins_lb[i]]\n",
    "    pred = np.array([10**x for x in help_df2[\"log10_Km\"]])\n",
    "    true = np.array([10**x for x in help_df2[\"Estimate Km\"]])\n",
    "    r2_scores.append(sk.r2_score(true, pred))\n",
    "    pearson_r.append(sci.stats.pearsonr(true, pred)[0])\n",
    "    rmse.append(math.sqrt(np.mean(abs(true - pred)**2)))\n",
    "    n_points.append(len(pred))\n",
    "    print(\"%s - %s\" % (sim_bins_lb[i], sim_bins_ub[i]), r2_scores[-1], pearson_r[-1], rmse[-1], len(pred))\n",
    "    \n",
    "\n",
    "plt.rcParams.update({'font.size': 24})\n",
    "\n",
    "fig, ax = plt.subplots(figsize= (8,6))\n",
    "\n",
    "for i in range(len(sim_bins_lb)):    \n",
    "    plt.scatter(i, rmse[i], c='navy', marker=\"o\", linewidths= 8)\n",
    "    ax.annotate(n_points[i], (i-0.08, rmse[i]+0.05), fontsize=17, c= \"black\", weight = \"bold\")\n",
    "\n",
    "    \n",
    "plt.xlabel('Reaction similarity score')\n",
    "plt.ylabel('RMSE')\n",
    "ax.yaxis.set_label_coords(-0.2, 0.5)\n",
    "ax.xaxis.set_label_coords(0.5,-0.23)\n",
    "\n",
    "ticks2 = np.array(range(len(sim_bins_lb)))\n",
    "labs = [\"%s - %s\" % (sim_bins_lb[i], sim_bins_ub[i]) for i in range(len(sim_bins_lb))]\n",
    "ax.set_xticks(ticks2)\n",
    "ax.set_xticklabels(labs,  y= -0.03, fontsize=20)\n",
    "ax.tick_params(axis='x', length=0, rotation = 0)\n",
    "\n",
    "# plt.ylim((0.5,2))\n",
    "#plt.xlim((-0.5, 3.2))\n",
    "\n",
    "# plt.plot([-0.49, 4], [0,0], color='grey', linestyle='dashed')\n",
    "#plt.savefig(join(\"..\",\"..\", \"data\", \"figures\", \"Reaction_Similarity_Score.eps\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = data_test[data_test['Uniprot IDs'].apply(lambda x: \"Q9LE06\" in x)]\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training a model with enzyme and reaction information (ESM-2/diff_fp) + Temperature + pH :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Creating input matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = np.array(list(data_train[\"ESM2\"]))\n",
    "train_X = np.concatenate([train_X, np.array(list(data_train[\"difference_fp\"])), np.array(list(data_train[\"Temperature\"]))[:, np.newaxis], np.array(list(data_train[\"pH\"]))[:, np.newaxis]], axis = 1)\n",
    "train_Y = np.array(list(data_train[\"log10_Km\"]))\n",
    "\n",
    "test_X = np.array(list(data_test[\"ESM2\"]))\n",
    "test_X = np.concatenate([test_X, np.array(list(data_test[\"difference_fp\"])), np.array(list(data_test[\"Temperature\"]))[:, np.newaxis], np.array(list(data_test[\"pH\"]))[:, np.newaxis]], axis = 1)\n",
    "test_Y = np.array(list(data_test[\"log10_Km\"]))\n",
    "\n",
    "train_X = train_X.astype(float)\n",
    "test_X = test_X.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Hyperparameter optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_mse_gradient_boosting(param):\n",
    "    num_round = param[\"num_rounds\"]\n",
    "    del param[\"num_rounds\"]\n",
    "    param[\"max_depth\"] = int(np.round(param[\"max_depth\"]))\n",
    "    # param[\"device\"] = \"cuda\"\n",
    "    param[\"tree_method\"] = \"hist\"\n",
    "    param[\"sampling_method\"] = \"gradient_based\"\n",
    "    \n",
    "    MSE = []\n",
    "    R2 = []\n",
    "    for i in range(5):\n",
    "        train_index, test_index  = train_indices[i], test_indices[i]\n",
    "        dtrain = xgb.DMatrix(train_X[train_index], label = train_Y[train_index])\n",
    "        dvalid = xgb.DMatrix(train_X[test_index])\n",
    "        bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "        y_valid_pred = bst.predict(dvalid)\n",
    "        MSE.append(np.mean(abs(np.reshape(train_Y[test_index], (-1)) - y_valid_pred)**2))\n",
    "        R2.append(r2_score(np.reshape(train_Y[test_index], (-1)),  y_valid_pred))\n",
    "    return(-np.mean(R2))\n",
    "\n",
    "space_gradient_boosting = {\n",
    "    \"learning_rate\": hp.choice(\"learning_rate\", np.linspace(0.01, 0.1, 10)),\n",
    "    \"max_depth\": hp.choice(\"max_depth\", np.linspace(3,10,8)),\n",
    "    #\"subsample\": hp.quniform(\"subsample\", 0.5, 1),\n",
    "    \"reg_lambda\": hp.choice(\"reg_lambda\", np.linspace(0, 1, 11)),\n",
    "    \"reg_alpha\": hp.choice(\"reg_alpha\", np.linspace(0, 1, 11)),\n",
    "    \"max_delta_step\": hp.choice(\"max_delta_step\", np.linspace(1, 5, 5)),\n",
    "    \"min_child_weight\": hp.choice(\"min_child_weight\", np.linspace(1, 6, 6)),\n",
    "    \"num_rounds\":  hp.choice(\"num_rounds\", np.linspace(50, 200, 4))}\n",
    "\n",
    "\n",
    "trials = Trials()\n",
    "best = fmin(fn = cross_validation_mse_gradient_boosting, space = space_gradient_boosting,\n",
    "            algo=rand.suggest, max_evals = 10, trials=trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "full dataset : {'learning_rate': 0.1, 'max_delta_step': 2, 'max_depth': 5, 'min_child_weight': 4, 'num_rounds': 150, 'reg_alpha': 0.3, 'reg_lambda': 0.4}\n",
    "brassicaceae: {'learning_rate': 0.08, 'max_delta_step': 4, 'max_depth': 10, 'min_child_weight': 3, 'num_rounds': 150, 'reg_alpha': 0.3, 'reg_lambda': 0.5}\n",
    "arabidopsis : {'learning_rate': 0.1, 'max_delta_step': 4, 'max_depth': 5, 'min_child_weight': 2, 'num_rounds': 150, 'reg_alpha': 0.3, 'reg_lambda': 0.5}\n",
    "wildtype : {'learning_rate': 0.04, 'max_delta_step': 4, 'max_depth': 10, 'min_child_weight': 3, 'num_rounds': 150, 'reg_alpha': 0.4, 'reg_lambda': 0.8}\n",
    "secondary : {'learning_rate': 0.04, 'max_delta_step': 3, 'max_depth': 6, 'min_child_weight': 3, 'num_rounds': 150, 'reg_alpha': 0.5, 'reg_lambda': 0.9}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c) Training and validating model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\"random_state\": 42, 'learning_rate': 0.04, 'max_delta_step': 4, 'max_depth': 10, 'min_child_weight': 3, 'num_rounds': 150, 'reg_alpha': 0.4, 'reg_lambda': 0.8}\n",
    "\n",
    "num_round = param[\"num_rounds\"]\n",
    "param[\"max_depth\"] = int(np.round(param[\"max_depth\"]))\n",
    "\n",
    "del param[\"num_rounds\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "R2 = []\n",
    "MSE = []\n",
    "Pearson = []\n",
    "MAE = []\n",
    "MedAE = []\n",
    "\n",
    "for i in range(5):\n",
    "    train_index, test_index  = train_indices[i], test_indices[i]\n",
    "    dtrain = xgb.DMatrix(train_X[train_index], label = train_Y[train_index])\n",
    "    dvalid = xgb.DMatrix(train_X[test_index])\n",
    "    \n",
    "    bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "    \n",
    "    y_valid_pred = bst.predict(dvalid)\n",
    "    MSE.append(np.sqrt(np.mean(abs(np.array([10**x for x in np.reshape(train_Y[test_index], (-1))]) - np.array([10**x for x in y_valid_pred]))**2)))\n",
    "    R2.append(r2_score([10**x for x in np.reshape(train_Y[test_index], (-1))], [10**x for x in y_valid_pred]))\n",
    "    Pearson.append(stats.pearsonr([10**x for x in np.reshape(train_Y[test_index], (-1))], [10**x for x in y_valid_pred])[0])\n",
    "    MAE.append(np.mean(np.array([10**x for x in train_Y[test_index]]) - np.array([10**x for x in y_valid_pred])))\n",
    "    MedAE.append(np.median(np.array([10**x for x in train_Y[test_index]]) - np.array([10**x for x in y_valid_pred])))\n",
    "\n",
    "    # MSE.append(np.mean(abs(np.reshape(train_Y[test_index], (-1)) - y_valid_pred)**2))\n",
    "    # R2.append(r2_score(np.reshape(train_Y[test_index], (-1)), y_valid_pred))\n",
    "    # Pearson.append(stats.pearsonr(np.reshape(train_Y[test_index], (-1)), y_valid_pred)[0])\n",
    "\n",
    "print(Pearson)\n",
    "print(MSE)\n",
    "print(R2)\n",
    "print(MAE)\n",
    "print(MedAE)\n",
    "\n",
    "np.save(join(\"..\", \"..\", \"data\", \"Pearson_CV_xgboost_ESM1b_ts_diff_fp.npy\"), np.array(Pearson))\n",
    "np.save(join(\"..\", \"..\", \"data\", \"MSE_CV_xgboost_ESM1b_ts_diff_fp.npy\"), np.array(MSE))\n",
    "np.save(join(\"..\", \"..\", \"data\", \"R2_CV_xgboost_ESM1b_ts_diff_fp.npy\"), np.array(R2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.233 0.0015530007 -0.057 0.0005664986 9.97882e-05\n"
     ]
    }
   ],
   "source": [
    "dtrain = xgb.DMatrix(train_X, label = train_Y)\n",
    "dtest = xgb.DMatrix(test_X, label = test_Y)\n",
    "\n",
    "bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "\n",
    "y_test_pred = bst.predict(dtest)\n",
    "\n",
    "data_test[\"Estimate Km\"] = y_test_pred\n",
    "\n",
    "MSE_dif_fp_test = np.sqrt(np.mean(abs(np.array([10**x for x in test_Y]) - np.array([10**x for x in y_test_pred]))**2))\n",
    "R2_dif_fp_test = r2_score(np.array([10**x for x in test_Y]), np.array([10**x for x in y_test_pred]))\n",
    "Pearson = stats.pearsonr(np.array([10**x for x in test_Y]), np.array([10**x for x in y_test_pred]))\n",
    "MAE = np.mean(abs(np.array([10**x for x in test_Y]) - np.array([10**x for x in y_test_pred])))\n",
    "MedAE = np.median(abs(np.array([10**x for x in test_Y]) - np.array([10**x for x in y_test_pred])))\n",
    "\n",
    "# MSE_dif_fp_test = np.mean(abs(np.reshape(test_Y, (-1)) - y_test_pred)**2)\n",
    "# R2_dif_fp_test = r2_score(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "# Pearson = stats.pearsonr(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "# MAE = np.mean(abs(np.reshape(test_Y, (-1)) - y_test_pred))\n",
    "\n",
    "print(np.round(Pearson[0],3) , np.round(MSE_dif_fp_test, 10), np.round(R2_dif_fp_test,3), np.round(MAE, 10), np.round(MedAE, 10))\n",
    "\n",
    "\n",
    "np.save(join(\"..\", \"..\", \"data\", \"y_test_pred_xgboost_ESM1b_ts_diff_fp.npy\"), bst.predict(dtest))\n",
    "np.save(join(\"..\", \"..\", \"data\",  \"y_test_true_xgboost_ESM1b_ts_diff_fp.npy\"), test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred_esm1b_ts_drfp = y_test_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (d) Training model with test and train data for production mode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = np.array(list(data_train[\"ESM2\"]))\n",
    "train_X = np.concatenate([train_X, np.array(list(data_train[\"difference_fp\"])), np.array(list(data_train[\"Temperature\"]))[:, np.newaxis], np.array(list(data_train[\"pH\"]))[:, np.newaxis]], axis = 1)\n",
    "train_Y = np.array(list(data_train[\"log10_Km\"]))\n",
    "\n",
    "test_X = np.array(list(data_test[\"ESM2\"]))\n",
    "test_X = np.concatenate([test_X, np.array(list(data_test[\"difference_fp\"])), np.array(list(data_test[\"Temperature\"]))[:, np.newaxis], np.array(list(data_test[\"pH\"]))[:, np.newaxis]], axis = 1)\n",
    "test_Y = np.array(list(data_test[\"log10_Km\"]))\n",
    "\n",
    "train_X = np.concatenate([train_X, test_X])\n",
    "train_Y = np.concatenate([train_Y, test_Y])\n",
    "\n",
    "train_X = train_X.astype(float)\n",
    "test_X = test_X.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\"random_state\": 42, 'learning_rate': 0.04, 'max_delta_step': 3, 'max_depth': 6, 'min_child_weight': 3, 'num_rounds': 150, 'reg_alpha': 0.5, 'reg_lambda': 0.9}\n",
    "\n",
    "num_round = param[\"num_rounds\"]\n",
    "param[\"max_depth\"] = int(np.round(param[\"max_depth\"]))\n",
    "\n",
    "del param[\"num_rounds\"]\n",
    "\n",
    "dtrain = xgb.DMatrix(train_X, label = train_Y)\n",
    "dtest = xgb.DMatrix(test_X)\n",
    "\n",
    "bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "\n",
    "y_test_pred = bst.predict(dtest)\n",
    "\n",
    "MSE_dif_fp_test = np.sqrt(np.mean(abs(np.array([10**x for x in np.reshape(test_Y, (-1))]) - np.array([10**x for x in y_test_pred]))**2))\n",
    "R2_dif_fp_test = r2_score(np.array([10**x for x in np.reshape(test_Y, (-1))]), np.array([10**x for x in y_test_pred]))\n",
    "Pearson = stats.pearsonr(np.array([10**x for x in np.reshape(test_Y, (-1))]), np.array([10**x for x in y_test_pred]))\n",
    "\n",
    "# MSE_dif_fp_test = np.mean(abs(np.reshape(test_Y, (-1)) - y_test_pred)**2)\n",
    "# R2_dif_fp_test = r2_score(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "# Pearson = stats.pearsonr(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "\n",
    "print(Pearson, MSE_dif_fp_test, R2_dif_fp_test)\n",
    "\n",
    "pickle.dump(bst, open(join(\"..\", \"..\", \"data\", \"saved_models\",\n",
    "                          \"xgboost_train_and_test.pkl\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Validating model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_X = np.array(list(data_val[\"ESM2\"]))\n",
    "val_X = np.concatenate([val_X, np.array(list(data_val[\"difference_fp\"])), np.array(list(data_val[\"Temperature\"]))[:, np.newaxis], np.array(list(data_val[\"pH\"]))[:, np.newaxis]], axis = 1)\n",
    "val_Y = np.array(list(data_val[\"log10_Km\"]))\n",
    "\n",
    "val_X = val_X.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(list(chain(*data_val[\"Km_values\"]))).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dval = xgb.DMatrix(val_X)\n",
    "\n",
    "y_val_pred = bst.predict(dval)\n",
    "data_val[\"Estimate Km\"] = y_val_pred\n",
    "\n",
    "MSE_dif_fp_val = np.sqrt(np.mean(abs(np.array([10**x for x in np.reshape(val_Y, (-1))]) - np.array([10**x for x in y_val_pred]))**2))\n",
    "R2_dif_fp_val = r2_score(np.array([10**x for x in np.reshape(val_Y, (-1))]), np.array([10**x for x in y_val_pred]))\n",
    "Pearson = stats.pearsonr(np.array([10**x for x in np.reshape(val_Y, (-1))]), np.array([10**x for x in y_val_pred]))\n",
    "MAE = np.mean(abs(np.array([10**x for x in val_Y]) - np.array([10**x for x in y_val_pred])))\n",
    "MedAE = np.median(abs(np.array([10**x for x in val_Y]) - np.array([10**x for x in y_val_pred])))\n",
    "\n",
    "# MSE_dif_fp_test = np.mean(abs(np.reshape(test_Y, (-1)) - y_test_pred)**2)\n",
    "# R2_dif_fp_test = r2_score(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "# Pearson = stats.pearsonr(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "\n",
    "print(Pearson, MSE_dif_fp_val, R2_dif_fp_val, MAE, MedAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training a model with enzyme, main substrate (MACCS fp) and reaction information (ESM-2/diff_fp) + Temperature + pH :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Creating input matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = np.array(list(data_train[\"ESM2\"]))\n",
    "train_X = np.concatenate([train_X, np.array(list(data_train[\"difference_fp\"])), np.array(list(data_train[\"GNN FP\"])), np.array(list(data_train[\"Temperature\"]))[:, np.newaxis], np.array(list(data_train[\"pH\"]))[:, np.newaxis]], axis = 1)\n",
    "train_Y = np.array(list(data_train[\"log10_Km\"]))\n",
    "\n",
    "test_X = np.array(list(data_test[\"ESM2\"]))\n",
    "test_X = np.concatenate([test_X, np.array(list(data_test[\"difference_fp\"])), np.array(list(data_test[\"GNN FP\"])), np.array(list(data_test[\"Temperature\"]))[:, np.newaxis], np.array(list(data_test[\"pH\"]))[:, np.newaxis]], axis = 1)\n",
    "test_Y = np.array(list(data_test[\"log10_Km\"]))\n",
    "\n",
    "train_X = train_X.astype(float)\n",
    "test_X = test_X.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Hyperparameter optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [34:07<00:00, 204.80s/trial, best loss: -0.3765153600083859]\n"
     ]
    }
   ],
   "source": [
    "def cross_validation_mse_gradient_boosting(param):\n",
    "    num_round = param[\"num_rounds\"]\n",
    "    del param[\"num_rounds\"]\n",
    "    param[\"max_depth\"] = int(np.round(param[\"max_depth\"]))\n",
    "    # param[\"device\"] = \"cuda\"\n",
    "    param[\"tree_method\"] = \"hist\"\n",
    "    param[\"sampling_method\"] = \"gradient_based\"\n",
    "    \n",
    "    MSE = []\n",
    "    R2 = []\n",
    "    for i in range(5):\n",
    "        train_index, test_index  = train_indices[i], test_indices[i]\n",
    "        dtrain = xgb.DMatrix(train_X[train_index], label = train_Y[train_index])\n",
    "        dvalid = xgb.DMatrix(train_X[test_index])\n",
    "        bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "        y_valid_pred = bst.predict(dvalid)\n",
    "        MSE.append(np.mean(abs(np.reshape(train_Y[test_index], (-1)) - y_valid_pred)**2))\n",
    "        R2.append(r2_score(np.reshape(train_Y[test_index], (-1)),  y_valid_pred))\n",
    "    return(-np.mean(R2))\n",
    "\n",
    "\n",
    "space_gradient_boosting = {\n",
    "    \"learning_rate\": hp.choice(\"learning_rate\", np.linspace(0.01, 0.1, 10)),\n",
    "    \"max_depth\": hp.choice(\"max_depth\", np.linspace(3,10,8)),\n",
    "    #\"subsample\": hp.quniform(\"subsample\", 0.5, 1),\n",
    "    \"reg_lambda\": hp.choice(\"reg_lambda\", np.linspace(0, 1, 11)),\n",
    "    \"reg_alpha\": hp.choice(\"reg_alpha\", np.linspace(0, 1, 11)),\n",
    "    \"max_delta_step\": hp.choice(\"max_delta_step\", np.linspace(1, 5, 5)),\n",
    "    \"min_child_weight\": hp.choice(\"min_child_weight\", np.linspace(1, 6, 6)),\n",
    "    \"num_rounds\":  hp.choice(\"num_rounds\", np.linspace(50, 200, 4))}\n",
    "\n",
    "\n",
    "trials = Trials()\n",
    "best = fmin(fn = cross_validation_mse_gradient_boosting, space = space_gradient_boosting,\n",
    "            algo=rand.suggest, max_evals = 10, trials=trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 8, 'max_delta_step': 1, 'max_depth': 2, 'min_child_weight': 2, 'num_rounds': 1, 'reg_alpha': 5, 'reg_lambda': 1}\n"
     ]
    }
   ],
   "source": [
    "print(best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "full dataset : {'learning_rate': 0.09, 'max_delta_step': 1, 'max_depth': 4, 'min_child_weight': 6, 'num_rounds': 200, 'reg_alpha': 0.8, 'reg_lambda': 0.3}\n",
    "brassicaceae : {'learning_rate': 0.06, 'max_delta_step': 4, 'max_depth': 3, 'min_child_weight': 5, 'num_rounds': 200, 'reg_alpha': 0.2, 'reg_lambda': 0.2}\n",
    "arabidopsis : {'learning_rate': 0.1, 'max_delta_step': 4, 'max_depth': 5, 'min_child_weight': 2, 'num_rounds': 150, 'reg_alpha': 0.3, 'reg_lambda': 0.5}\n",
    "wildtype : {'learning_rate': 0.08, 'max_delta_step': 6, 'max_depth': 2, 'min_child_weight': 3, 'num_rounds': 100, 'reg_alpha': 0.5, 'reg_lambda': 0.2}\n",
    "secondary : {'learning_rate': 0.09, 'max_delta_step': 2, 'max_depth': 5, 'min_child_weight': 3, 'num_rounds': 100, 'reg_alpha': 0.5, 'reg_lambda': 0.1}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c) Training and validating model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\"random_state\": 42, 'learning_rate': 0.08, 'max_delta_step': 6, 'max_depth': 2, 'min_child_weight': 3, 'num_rounds': 100, 'reg_alpha': 0.5, 'reg_lambda': 0.2}\n",
    "\n",
    "num_round = param[\"num_rounds\"]\n",
    "param[\"max_depth\"] = int(np.round(param[\"max_depth\"]))\n",
    "\n",
    "del param[\"num_rounds\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R2 = []\n",
    "MSE = []\n",
    "Pearson = []\n",
    "MAE = []\n",
    "MedAE = []\n",
    "\n",
    "for i in range(5):\n",
    "    train_index, test_index  = train_indices[i], test_indices[i]\n",
    "    dtrain = xgb.DMatrix(train_X[train_index], label = train_Y[train_index])\n",
    "    dvalid = xgb.DMatrix(train_X[test_index])\n",
    "    \n",
    "    bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "    \n",
    "    y_valid_pred = bst.predict(dvalid)\n",
    "    MSE.append(np.sqrt(np.mean(abs(np.array([10**x for x in np.reshape(train_Y[test_index], (-1))]) - np.array([10**x for x in y_valid_pred]))**2)))\n",
    "    R2.append(r2_score([10**x for x in np.reshape(train_Y[test_index], (-1))], [10**x for x in y_valid_pred]))\n",
    "    Pearson.append(stats.pearsonr([10**x for x in np.reshape(train_Y[test_index], (-1))], [10**x for x in y_valid_pred])[0])\n",
    "    MAE.append(np.mean(np.array([10**x for x in train_Y[test_index]]) - np.array([10**x for x in y_valid_pred])))\n",
    "    MedAE.append(np.median(np.array([10**x for x in train_Y[test_index]]) - np.array([10**x for x in y_valid_pred])))\n",
    "\n",
    "    # MSE.append(np.mean(abs(np.reshape(train_Y[test_index], (-1)) - y_valid_pred)**2))\n",
    "    # R2.append(r2_score(np.reshape(train_Y[test_index], (-1)), y_valid_pred))\n",
    "    # Pearson.append(stats.pearsonr(np.reshape(train_Y[test_index], (-1)), y_valid_pred)[0])\n",
    "\n",
    "print(Pearson)\n",
    "print(MSE)\n",
    "print(R2)\n",
    "print(MAE)\n",
    "print(MedAE)\n",
    "\n",
    "np.save(join(\"..\", \"..\", \"data\", \"Pearson_CV_xgboost_ESM1b_ts_diff_fp.npy\"), np.array(Pearson))\n",
    "np.save(join(\"..\", \"..\", \"data\", \"MSE_CV_xgboost_ESM1b_ts_diff_fp.npy\"), np.array(MSE))\n",
    "np.save(join(\"..\", \"..\", \"data\", \"R2_CV_xgboost_ESM1b_ts_diff_fp.npy\"), np.array(R2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.27 0.0015611381 -0.068 0.0005626488 8.05193e-05\n"
     ]
    }
   ],
   "source": [
    "dtrain = xgb.DMatrix(train_X, label = train_Y)\n",
    "dtest = xgb.DMatrix(test_X, label = test_Y)\n",
    "\n",
    "bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "\n",
    "y_test_pred = bst.predict(dtest)\n",
    "\n",
    "data_test[\"Estimate Km\"] = y_test_pred\n",
    "\n",
    "MSE_dif_fp_test = np.sqrt(np.mean(abs(np.array([10**x for x in test_Y]) - np.array([10**x for x in y_test_pred]))**2))\n",
    "R2_dif_fp_test = r2_score(np.array([10**x for x in test_Y]), np.array([10**x for x in y_test_pred]))\n",
    "Pearson = stats.pearsonr(np.array([10**x for x in test_Y]), np.array([10**x for x in y_test_pred]))\n",
    "MAE = np.mean(abs(np.array([10**x for x in test_Y]) - np.array([10**x for x in y_test_pred])))\n",
    "MedAE = np.median(abs(np.array([10**x for x in test_Y]) - np.array([10**x for x in y_test_pred])))\n",
    "\n",
    "# MSE_dif_fp_test = np.mean(abs(np.reshape(test_Y, (-1)) - y_test_pred)**2)\n",
    "# R2_dif_fp_test = r2_score(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "# Pearson = stats.pearsonr(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "# MAE = np.mean(abs(np.reshape(test_Y, (-1)) - y_test_pred))\n",
    "\n",
    "print(np.round(Pearson[0],3) , np.round(MSE_dif_fp_test, 10), np.round(R2_dif_fp_test,3), np.round(MAE, 10), np.round(MedAE, 10))\n",
    "\n",
    "\n",
    "np.save(join(\"..\", \"..\", \"data\", \"y_test_pred_xgboost_ESM1b_ts_diff_fp.npy\"), bst.predict(dtest))\n",
    "np.save(join(\"..\", \"..\", \"data\",  \"y_test_true_xgboost_ESM1b_ts_diff_fp.npy\"), test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred_esm1b_ts_drfp = y_test_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (d) Training model with test and train data for production mode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = np.array(list(data_train[\"ESM2\"]))\n",
    "train_X = np.concatenate([train_X, np.array(list(data_train[\"difference_fp\"])), np.array(list(data_train[\"GNN FP\"])), np.array(list(data_train[\"Temperature\"]))[:, np.newaxis], np.array(list(data_train[\"pH\"]))[:, np.newaxis]], axis = 1)\n",
    "train_Y = np.array(list(data_train[\"log10_Km\"]))\n",
    "\n",
    "test_X = np.array(list(data_test[\"ESM2\"]))\n",
    "test_X = np.concatenate([test_X, np.array(list(data_test[\"difference_fp\"])), np.array(list(data_test[\"GNN FP\"])), np.array(list(data_test[\"Temperature\"]))[:, np.newaxis], np.array(list(data_test[\"pH\"]))[:, np.newaxis]], axis = 1)\n",
    "test_Y = np.array(list(data_test[\"log10_Km\"]))\n",
    "\n",
    "train_X = np.concatenate([train_X, test_X])\n",
    "train_Y = np.concatenate([train_Y, test_Y])\n",
    "\n",
    "train_X = train_X.astype(float)\n",
    "test_X = test_X.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8385781987345318, 7.471833505378463e-127) 0.0015214545949970627 0.5675224363211706\n"
     ]
    }
   ],
   "source": [
    "param = {\"random_state\": 42, 'learning_rate': 0.09, 'max_delta_step': 2, 'max_depth': 5, 'min_child_weight': 3, 'num_rounds': 100, 'reg_alpha': 0.5, 'reg_lambda': 0.1}\n",
    "\n",
    "num_round = param[\"num_rounds\"]\n",
    "param[\"max_depth\"] = int(np.round(param[\"max_depth\"]))\n",
    "\n",
    "del param[\"num_rounds\"]\n",
    "\n",
    "dtrain = xgb.DMatrix(train_X, label = train_Y)\n",
    "dtest = xgb.DMatrix(test_X)\n",
    "\n",
    "bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "\n",
    "y_test_pred = bst.predict(dtest)\n",
    "\n",
    "MSE_dif_fp_test = np.sqrt(np.mean(abs(np.array([10**x for x in np.reshape(test_Y, (-1))]) - np.array([10**x for x in y_test_pred]))**2))\n",
    "R2_dif_fp_test = r2_score(np.array([10**x for x in np.reshape(test_Y, (-1))]), np.array([10**x for x in y_test_pred]))\n",
    "Pearson = stats.pearsonr(np.array([10**x for x in np.reshape(test_Y, (-1))]), np.array([10**x for x in y_test_pred]))\n",
    "\n",
    "# MSE_dif_fp_test = np.mean(abs(np.reshape(test_Y, (-1)) - y_test_pred)**2)\n",
    "# R2_dif_fp_test = r2_score(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "# Pearson = stats.pearsonr(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "\n",
    "print(Pearson, MSE_dif_fp_test, R2_dif_fp_test)\n",
    "\n",
    "pickle.dump(bst, open(join(\"..\", \"..\", \"data\", \"saved_models\",\n",
    "                          \"xgboost_train_and_test.pkl\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Validating model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_X = np.array(list(data_val[\"ESM2\"]))\n",
    "val_X = np.concatenate([val_X, np.array(list(data_val[\"difference_fp\"])), np.array(list(data_val[\"GNN FP\"])), np.array(list(data_val[\"Temperature\"]))[:, np.newaxis], np.array(list(data_val[\"pH\"]))[:, np.newaxis]], axis = 1)\n",
    "val_Y = np.array(list(data_val[\"log10_Km\"]))\n",
    "\n",
    "val_X = val_X.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(list(chain(*data_val[\"Km_values\"]))).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-0.13675579132204868, 0.5653435306198017) 0.0021487085320582595 -0.14332519177718717 0.0008613460705452951 0.00021917412594406771\n"
     ]
    }
   ],
   "source": [
    "dval = xgb.DMatrix(val_X)\n",
    "\n",
    "y_val_pred = bst.predict(dval)\n",
    "data_val[\"Estimate Km\"] = y_val_pred\n",
    "\n",
    "MSE_dif_fp_val = np.sqrt(np.mean(abs(np.array([10**x for x in np.reshape(val_Y, (-1))]) - np.array([10**x for x in y_val_pred]))**2))\n",
    "R2_dif_fp_val = r2_score(np.array([10**x for x in np.reshape(val_Y, (-1))]), np.array([10**x for x in y_val_pred]))\n",
    "Pearson = stats.pearsonr(np.array([10**x for x in np.reshape(val_Y, (-1))]), np.array([10**x for x in y_val_pred]))\n",
    "MAE = np.mean(abs(np.array([10**x for x in val_Y]) - np.array([10**x for x in y_val_pred])))\n",
    "MedAE = np.median(abs(np.array([10**x for x in val_Y]) - np.array([10**x for x in y_val_pred])))\n",
    "\n",
    "# MSE_dif_fp_test = np.mean(abs(np.reshape(test_Y, (-1)) - y_test_pred)**2)\n",
    "# R2_dif_fp_test = r2_score(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "# Pearson = stats.pearsonr(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "\n",
    "print(Pearson, MSE_dif_fp_val, R2_dif_fp_val, MAE, MedAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training a model with enzyme information (ESM-2) + Temperature + pH:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Creating input matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = np.array(list(data_train[\"ESM2\"]))\n",
    "train_X = np.concatenate([train_X, np.array(list(data_train[\"Temperature\"]))[:, np.newaxis], np.array(list(data_train[\"pH\"]))[:, np.newaxis]], axis = 1)\n",
    "train_Y = np.array(list(data_train[\"log10_Km\"]))\n",
    "\n",
    "test_X = np.array(list(data_test[\"ESM2\"]))\n",
    "test_X = np.concatenate([test_X, np.array(list(data_test[\"Temperature\"]))[:, np.newaxis], np.array(list(data_test[\"pH\"]))[:, np.newaxis]], axis = 1)\n",
    "test_Y = np.array(list(data_test[\"log10_Km\"]))\n",
    "\n",
    "train_X = train_X.astype(float)\n",
    "test_X = test_X.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Hyperparameter optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [28:43<00:00, 172.38s/trial, best loss: -0.3024170717015792]\n"
     ]
    }
   ],
   "source": [
    "def cross_validation_mse_gradient_boosting(param):\n",
    "    num_round = param[\"num_rounds\"]\n",
    "    del param[\"num_rounds\"]\n",
    "    param[\"max_depth\"] = int(np.round(param[\"max_depth\"]))\n",
    "    # param[\"device\"] = \"cuda\"\n",
    "    param[\"tree_method\"] = \"hist\"\n",
    "    param[\"sampling_method\"] = \"gradient_based\"\n",
    "    \n",
    "    MSE = []\n",
    "    R2 = []\n",
    "    for i in range(5):\n",
    "        train_index, test_index  = train_indices[i], test_indices[i]\n",
    "        dtrain = xgb.DMatrix(train_X[train_index], label = train_Y[train_index])\n",
    "        dvalid = xgb.DMatrix(train_X[test_index])\n",
    "        bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "        y_valid_pred = bst.predict(dvalid)\n",
    "        MSE.append(np.mean(abs(np.reshape(train_Y[test_index], (-1)) - y_valid_pred)**2))\n",
    "        R2.append(r2_score(np.reshape(train_Y[test_index], (-1)),  y_valid_pred))\n",
    "    return(-np.mean(R2))\n",
    "\n",
    "\n",
    "space_gradient_boosting = {\n",
    "    \"learning_rate\": hp.choice(\"learning_rate\", np.linspace(0.01, 0.1, 10)),\n",
    "    \"max_depth\": hp.choice(\"max_depth\", np.linspace(3,10,8)),\n",
    "    #\"subsample\": hp.quniform(\"subsample\", 0.5, 1),\n",
    "    \"reg_lambda\": hp.choice(\"reg_lambda\", np.linspace(0, 1, 11)),\n",
    "    \"reg_alpha\": hp.choice(\"reg_alpha\", np.linspace(0, 1, 11)),\n",
    "    \"max_delta_step\": hp.choice(\"max_delta_step\", np.linspace(1, 5, 5)),\n",
    "    \"min_child_weight\": hp.choice(\"min_child_weight\", np.linspace(1, 6, 6)),\n",
    "    \"num_rounds\":  hp.choice(\"num_rounds\", np.linspace(50, 200, 4))}\n",
    "\n",
    "\n",
    "trials = Trials()\n",
    "best = fmin(fn = cross_validation_mse_gradient_boosting, space = space_gradient_boosting,\n",
    "            algo=rand.suggest, max_evals = 10, trials=trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 9, 'max_delta_step': 4, 'max_depth': 7, 'min_child_weight': 2, 'num_rounds': 1, 'reg_alpha': 4, 'reg_lambda': 6}\n"
     ]
    }
   ],
   "source": [
    "print(best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "full dataset : {'learning_rate': 0.08, 'max_delta_step': 5, 'max_depth': 6, 'min_child_weight': 4, 'num_rounds': 150, 'reg_alpha': 0.6, 'reg_lambda': 0.4}\n",
    "brassicaceae : {'learning_rate': 0.05, 'max_delta_step': 1, 'max_depth': 7, 'min_child_weight': 5, 'num_rounds': 200, 'reg_alpha': 0.6, 'reg_lambda': 0.4}\n",
    "arabidopsis :  {'learning_rate': 0.08, 'max_delta_step': 2, 'max_depth': 5, 'min_child_weight': 1, 'num_rounds': 200, 'reg_alpha': 1, 'reg_lambda': 0.2}\n",
    "wildtype : {'learning_rate': 0.08, 'max_delta_step': 4, 'max_depth': 5, 'min_child_weight': 1, 'num_rounds': 200, 'reg_alpha': 0.5, 'reg_lambda': 1}\n",
    "secondary : {'learning_rate': 0.1, 'max_delta_step': 5, 'max_depth': 10, 'min_child_weight': 3, 'num_rounds': 100, 'reg_alpha': 0.4, 'reg_lambda': 0.6}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c) Training and validating model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\"random_state\": 42, 'learning_rate': 0.08, 'max_delta_step': 2, 'max_depth': 5, 'min_child_weight': 1, 'num_rounds': 200, 'reg_alpha': 1, 'reg_lambda': 0.2}\n",
    "\n",
    "num_round = param[\"num_rounds\"]\n",
    "param[\"max_depth\"] = int(np.round(param[\"max_depth\"]))\n",
    "\n",
    "del param[\"num_rounds\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R2 = []\n",
    "MSE = []\n",
    "Pearson = []\n",
    "MAE = []\n",
    "MedAE = []\n",
    "\n",
    "for i in range(5):\n",
    "    train_index, test_index  = train_indices[i], test_indices[i]\n",
    "    dtrain = xgb.DMatrix(train_X[train_index], label = train_Y[train_index])\n",
    "    dvalid = xgb.DMatrix(train_X[test_index])\n",
    "    \n",
    "    bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "    \n",
    "    y_valid_pred = bst.predict(dvalid)\n",
    "    MSE.append(np.sqrt(np.mean(abs(np.array([10**x for x in np.reshape(train_Y[test_index], (-1))]) - np.array([10**x for x in y_valid_pred]))**2)))\n",
    "    R2.append(r2_score([10**x for x in np.reshape(train_Y[test_index], (-1))], [10**x for x in y_valid_pred]))\n",
    "    Pearson.append(stats.pearsonr([10**x for x in np.reshape(train_Y[test_index], (-1))], [10**x for x in y_valid_pred])[0])\n",
    "    MAE.append(np.mean(np.array([10**x for x in train_Y[test_index]]) - np.array([10**x for x in y_valid_pred])))\n",
    "    MedAE.append(np.median(np.array([10**x for x in train_Y[test_index]]) - np.array([10**x for x in y_valid_pred])))\n",
    "\n",
    "    # MSE.append(np.mean(abs(np.reshape(train_Y[test_index], (-1)) - y_valid_pred)**2))\n",
    "    # R2.append(r2_score(np.reshape(train_Y[test_index], (-1)), y_valid_pred))\n",
    "    # Pearson.append(stats.pearsonr(np.reshape(train_Y[test_index], (-1)), y_valid_pred)[0])\n",
    "\n",
    "print(Pearson)\n",
    "print(MSE)\n",
    "print(R2)\n",
    "print(MAE)\n",
    "print(MedAE)\n",
    "\n",
    "np.save(join(\"..\", \"..\", \"data\", \"Pearson_CV_xgboost_ESM1b_ts_diff_fp.npy\"), np.array(Pearson))\n",
    "np.save(join(\"..\", \"..\", \"data\", \"MSE_CV_xgboost_ESM1b_ts_diff_fp.npy\"), np.array(MSE))\n",
    "np.save(join(\"..\", \"..\", \"data\", \"R2_CV_xgboost_ESM1b_ts_diff_fp.npy\"), np.array(R2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.066 0.0011290639 -0.166 0.0005689072 0.000180507\n"
     ]
    }
   ],
   "source": [
    "dtrain = xgb.DMatrix(train_X, label = train_Y)\n",
    "dtest = xgb.DMatrix(test_X, label = test_Y)\n",
    "\n",
    "bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "\n",
    "y_test_pred = bst.predict(dtest)\n",
    "\n",
    "data_test[\"Estimate Km\"] = y_test_pred\n",
    "\n",
    "MSE_dif_fp_test = np.sqrt(np.mean(abs(np.array([10**x for x in test_Y]) - np.array([10**x for x in y_test_pred]))**2))\n",
    "R2_dif_fp_test = r2_score(np.array([10**x for x in test_Y]), np.array([10**x for x in y_test_pred]))\n",
    "Pearson = stats.pearsonr(np.array([10**x for x in test_Y]), np.array([10**x for x in y_test_pred]))\n",
    "MAE = np.mean(abs(np.array([10**x for x in test_Y]) - np.array([10**x for x in y_test_pred])))\n",
    "MedAE = np.median(abs(np.array([10**x for x in test_Y]) - np.array([10**x for x in y_test_pred])))\n",
    "\n",
    "# MSE_dif_fp_test = np.mean(abs(np.reshape(test_Y, (-1)) - y_test_pred)**2)\n",
    "# R2_dif_fp_test = r2_score(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "# Pearson = stats.pearsonr(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "# MAE = np.mean(abs(np.reshape(test_Y, (-1)) - y_test_pred))\n",
    "\n",
    "print(np.round(Pearson[0],3) , np.round(MSE_dif_fp_test, 10), np.round(R2_dif_fp_test,3), np.round(MAE, 10), np.round(MedAE, 10))\n",
    "\n",
    "\n",
    "np.save(join(\"..\", \"..\", \"data\", \"y_test_pred_xgboost_ESM1b_ts_diff_fp.npy\"), bst.predict(dtest))\n",
    "np.save(join(\"..\", \"..\", \"data\",  \"y_test_true_xgboost_ESM1b_ts_diff_fp.npy\"), test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred_esm1b_ts_drfp = y_test_pred\n",
    "# data_test[\"Estimate Km\"] = [10**x for x in data_test[\"Estimate Km\"]]\n",
    "# filtered_df = data_test[data_test['Uniprot IDs'].apply(lambda x: \"Q9LE06\" in x)]\n",
    "# filtered_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (d) Training model with test and train data for production mode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = np.array(list(data_train[\"ESM2\"]))\n",
    "train_X = np.concatenate([train_X, np.array(list(data_train[\"Temperature\"]))[:, np.newaxis], np.array(list(data_train[\"pH\"]))[:, np.newaxis]], axis = 1)\n",
    "train_Y = np.array(list(data_train[\"log10_Km\"]))\n",
    "\n",
    "test_X = np.array(list(data_test[\"ESM2\"]))\n",
    "test_X = np.concatenate([test_X, np.array(list(data_test[\"Temperature\"]))[:, np.newaxis], np.array(list(data_test[\"pH\"]))[:, np.newaxis]], axis = 1)\n",
    "test_Y = np.array(list(data_test[\"log10_Km\"]))\n",
    "\n",
    "train_X = np.concatenate([train_X, test_X])\n",
    "train_Y = np.concatenate([train_Y, test_Y])\n",
    "\n",
    "train_X = train_X.astype(float)\n",
    "test_X = test_X.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.5824884638972572, 1.0159724284692625e-13) 0.0014769208299797146 0.27595072404945364\n"
     ]
    }
   ],
   "source": [
    "param = {\"random_state\": 42, 'learning_rate': 0.05, 'max_delta_step': 1, 'max_depth': 7, 'min_child_weight': 5, 'num_rounds': 200, 'reg_alpha': 0.6, 'reg_lambda': 0.4}\n",
    "\n",
    "num_round = param[\"num_rounds\"]\n",
    "param[\"max_depth\"] = int(np.round(param[\"max_depth\"]))\n",
    "\n",
    "del param[\"num_rounds\"]\n",
    "\n",
    "dtrain = xgb.DMatrix(train_X, label = train_Y)\n",
    "dtest = xgb.DMatrix(test_X)\n",
    "\n",
    "bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "\n",
    "y_test_pred = bst.predict(dtest)\n",
    "\n",
    "MSE_dif_fp_test = np.sqrt(np.mean(abs(np.array([10**x for x in np.reshape(test_Y, (-1))]) - np.array([10**x for x in y_test_pred]))**2))\n",
    "R2_dif_fp_test = r2_score(np.array([10**x for x in np.reshape(test_Y, (-1))]), np.array([10**x for x in y_test_pred]))\n",
    "Pearson = stats.pearsonr(np.array([10**x for x in np.reshape(test_Y, (-1))]), np.array([10**x for x in y_test_pred]))\n",
    "\n",
    "# MSE_dif_fp_test = np.mean(abs(np.reshape(test_Y, (-1)) - y_test_pred)**2)\n",
    "# R2_dif_fp_test = r2_score(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "# Pearson = stats.pearsonr(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "\n",
    "print(Pearson, MSE_dif_fp_test, R2_dif_fp_test)\n",
    "\n",
    "pickle.dump(bst, open(join(\"..\", \"..\", \"data\", \"saved_models\",\n",
    "                          \"xgboost_train_and_test.pkl\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = bst.get_score()\n",
    "print(len(data_train[\"ESM2\"][1]))\n",
    "print(len(data_train[\"MACCS FP\"][1]))\n",
    "for key, value in importances.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (e) Validating model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.027275659770205275, 0.9091213853763981) 0.002105060915132823 -0.09734728019670702 0.0008231325291477231 0.00022142809884751722\n"
     ]
    }
   ],
   "source": [
    "val_X = np.array(list(data_val[\"ESM2\"]))\n",
    "val_X = np.concatenate([val_X, np.array(list(data_val[\"Temperature\"]))[:, np.newaxis], np.array(list(data_val[\"pH\"]))[:, np.newaxis]], axis = 1)\n",
    "val_Y = np.array(list(data_val[\"log10_Km\"]))\n",
    "\n",
    "val_X = val_X.astype(float)\n",
    "\n",
    "dval = xgb.DMatrix(val_X)\n",
    "\n",
    "y_val_pred = bst.predict(dval)\n",
    "data_val[\"Estimate Km\"] = y_val_pred\n",
    "\n",
    "MSE_dif_fp_val = np.sqrt(np.mean(abs(np.array([10**x for x in np.reshape(val_Y, (-1))]) - np.array([10**x for x in y_val_pred]))**2))\n",
    "R2_dif_fp_val = r2_score(np.array([10**x for x in np.reshape(val_Y, (-1))]), np.array([10**x for x in y_val_pred]))\n",
    "Pearson = stats.pearsonr(np.array([10**x for x in np.reshape(val_Y, (-1))]), np.array([10**x for x in y_val_pred]))\n",
    "MAE = np.mean(abs(np.array([10**x for x in val_Y]) - np.array([10**x for x in y_val_pred])))\n",
    "MedAE = np.median(abs(np.array([10**x for x in val_Y]) - np.array([10**x for x in y_val_pred])))\n",
    "\n",
    "# MSE_dif_fp_test = np.mean(abs(np.reshape(test_Y, (-1)) - y_test_pred)**2)\n",
    "# R2_dif_fp_test = r2_score(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "# Pearson = stats.pearsonr(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "\n",
    "print(Pearson, MSE_dif_fp_val, R2_dif_fp_val, MAE, MedAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training a model with main substrate information (MACCS) + Temperature + pH:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Creating input matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = np.array(list(data_train['GNN FP']))\n",
    "train_X = np.concatenate([train_X, np.array(list(data_train[\"Temperature\"]))[:, np.newaxis], np.array(list(data_train[\"pH\"]))[:, np.newaxis]], axis = 1)\n",
    "train_Y = np.array(list(data_train[\"log10_Km\"]))\n",
    "\n",
    "test_X = np.array(list(data_test['GNN FP']))\n",
    "test_X = np.concatenate([test_X, np.array(list(data_test[\"Temperature\"]))[:, np.newaxis], np.array(list(data_test[\"pH\"]))[:, np.newaxis]], axis = 1)\n",
    "test_Y = np.array(list(data_test[\"log10_Km\"]))\n",
    "\n",
    "train_X = train_X.astype(float)\n",
    "test_X = test_X.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Hyperparameter optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:15<00:00,  1.56s/trial, best loss: -0.26921031900871]  \n"
     ]
    }
   ],
   "source": [
    "def cross_validation_mse_gradient_boosting(param):\n",
    "    num_round = param[\"num_rounds\"]\n",
    "    del param[\"num_rounds\"]\n",
    "    param[\"max_depth\"] = int(np.round(param[\"max_depth\"]))\n",
    "    # param[\"device\"] = \"cuda\"\n",
    "    param[\"tree_method\"] = \"hist\"\n",
    "    param[\"sampling_method\"] = \"gradient_based\"\n",
    "    \n",
    "    MSE = []\n",
    "    R2 = []\n",
    "    for i in range(5):\n",
    "        train_index, test_index  = train_indices[i], test_indices[i]\n",
    "        dtrain = xgb.DMatrix(train_X[train_index], label = train_Y[train_index])\n",
    "        dvalid = xgb.DMatrix(train_X[test_index])\n",
    "        bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "        y_valid_pred = bst.predict(dvalid)\n",
    "        MSE.append(np.mean(abs(np.reshape(train_Y[test_index], (-1)) - y_valid_pred)**2))\n",
    "        R2.append(r2_score(np.reshape(train_Y[test_index], (-1)),  y_valid_pred))\n",
    "    return(-np.mean(R2))\n",
    "\n",
    "space_gradient_boosting = {\n",
    "    \"learning_rate\": hp.choice(\"learning_rate\", np.linspace(0.01, 0.1, 10)),\n",
    "    \"max_depth\": hp.choice(\"max_depth\", np.linspace(3,10,8)),\n",
    "    #\"subsample\": hp.quniform(\"subsample\", 0.5, 1),\n",
    "    \"reg_lambda\": hp.choice(\"reg_lambda\", np.linspace(0, 1, 11)),\n",
    "    \"reg_alpha\": hp.choice(\"reg_alpha\", np.linspace(0, 1, 11)),\n",
    "    \"max_delta_step\": hp.choice(\"max_delta_step\", np.linspace(1, 5, 5)),\n",
    "    \"min_child_weight\": hp.choice(\"min_child_weight\", np.linspace(1, 6, 6)),\n",
    "    \"num_rounds\":  hp.choice(\"num_rounds\", np.linspace(50, 200, 4))}\n",
    "\n",
    "trials = Trials()\n",
    "best = fmin(fn = cross_validation_mse_gradient_boosting, space = space_gradient_boosting,\n",
    "            algo=rand.suggest, max_evals = 10, trials=trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 6, 'max_delta_step': 4, 'max_depth': 2, 'min_child_weight': 2, 'num_rounds': 0, 'reg_alpha': 7, 'reg_lambda': 0}\n"
     ]
    }
   ],
   "source": [
    "print(best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "full dataset: {'learning_rate': 0.05, 'max_delta_step': 2, 'max_depth': 6, 'min_child_weight': 3, 'num_rounds': 200, 'reg_alpha': 0.6, 'reg_lambda': 0.5}\n",
    "brassicaceae : {'learning_rate': 0.06, 'max_delta_step': 1, 'max_depth': 9, 'min_child_weight': 5, 'num_rounds': 150, 'reg_alpha': 0.6, 'reg_lambda': 1}\n",
    "arabidopsis : {'learning_rate': 0.08, 'max_delta_step': 4, 'max_depth': 6, 'min_child_weight': 2, 'num_rounds': 50, 'reg_alpha': 1, 'reg_lambda': 0.5}\n",
    "wildtype : {'learning_rate': 0.05, 'max_delta_step': 1, 'max_depth': 4, 'min_child_weight': 1, 'num_rounds': 150, 'reg_alpha': 0.8, 'reg_lambda': 0.5}\n",
    "secondary : {'learning_rate': 0.07, 'max_delta_step': 5, 'max_depth': 5, 'min_child_weight': 3, 'num_rounds': 50, 'reg_alpha': 0.7, 'reg_lambda': 0}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c) Training and validating model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\"random_state\": 42, 'learning_rate': 0.08, 'max_delta_step': 4, 'max_depth': 6, 'min_child_weight': 2, 'num_rounds': 50, 'reg_alpha': 1, 'reg_lambda': 0.5}\n",
    "\n",
    "num_round = param[\"num_rounds\"]\n",
    "param[\"max_depth\"] = int(np.round(param[\"max_depth\"]))\n",
    "\n",
    "del param[\"num_rounds\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R2 = []\n",
    "MSE = []\n",
    "Pearson = []\n",
    "MAE = []\n",
    "MedAE = []\n",
    "\n",
    "for i in range(5):\n",
    "    train_index, test_index  = train_indices[i], test_indices[i]\n",
    "    dtrain = xgb.DMatrix(train_X[train_index], label = train_Y[train_index])\n",
    "    dvalid = xgb.DMatrix(train_X[test_index])\n",
    "    \n",
    "    bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "    \n",
    "    y_valid_pred = bst.predict(dvalid)\n",
    "    MSE.append(np.sqrt(np.mean(abs(np.array([10**x for x in np.reshape(train_Y[test_index], (-1))]) - np.array([10**x for x in y_valid_pred]))**2)))\n",
    "    R2.append(r2_score([10**x for x in np.reshape(train_Y[test_index], (-1))], [10**x for x in y_valid_pred]))\n",
    "    Pearson.append(stats.pearsonr([10**x for x in np.reshape(train_Y[test_index], (-1))], [10**x for x in y_valid_pred])[0])\n",
    "    MAE.append(np.mean(np.array([10**x for x in train_Y[test_index]]) - np.array([10**x for x in y_valid_pred])))\n",
    "    MedAE.append(np.median(np.array([10**x for x in train_Y[test_index]]) - np.array([10**x for x in y_valid_pred])))\n",
    "\n",
    "    # MSE.append(np.mean(abs(np.reshape(train_Y[test_index], (-1)) - y_valid_pred)**2))\n",
    "    # R2.append(r2_score(np.reshape(train_Y[test_index], (-1)), y_valid_pred))\n",
    "    # Pearson.append(stats.pearsonr(np.reshape(train_Y[test_index], (-1)), y_valid_pred)[0])\n",
    "\n",
    "print(Pearson)\n",
    "print(MSE)\n",
    "print(R2)\n",
    "print(MAE)\n",
    "print(MedAE)\n",
    "\n",
    "np.save(join(\"..\", \"..\", \"data\", \"Pearson_CV_xgboost_ESM1b_ts_diff_fp.npy\"), np.array(Pearson))\n",
    "np.save(join(\"..\", \"..\", \"data\", \"MSE_CV_xgboost_ESM1b_ts_diff_fp.npy\"), np.array(MSE))\n",
    "np.save(join(\"..\", \"..\", \"data\", \"R2_CV_xgboost_ESM1b_ts_diff_fp.npy\"), np.array(R2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.297 0.0010798995 -0.067 0.0004955612 0.0001841037\n"
     ]
    }
   ],
   "source": [
    "dtrain = xgb.DMatrix(train_X, label = train_Y)\n",
    "dtest = xgb.DMatrix(test_X, label = test_Y)\n",
    "\n",
    "bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "\n",
    "y_test_pred = bst.predict(dtest)\n",
    "\n",
    "data_test[\"Estimate Km\"] = y_test_pred\n",
    "\n",
    "MSE_dif_fp_test = np.sqrt(np.mean(abs(np.array([10**x for x in test_Y]) - np.array([10**x for x in y_test_pred]))**2))\n",
    "R2_dif_fp_test = r2_score(np.array([10**x for x in test_Y]), np.array([10**x for x in y_test_pred]))\n",
    "Pearson = stats.pearsonr(np.array([10**x for x in test_Y]), np.array([10**x for x in y_test_pred]))\n",
    "MAE = np.mean(abs(np.array([10**x for x in test_Y]) - np.array([10**x for x in y_test_pred])))\n",
    "MedAE = np.median(abs(np.array([10**x for x in test_Y]) - np.array([10**x for x in y_test_pred])))\n",
    "\n",
    "# MSE_dif_fp_test = np.mean(abs(np.reshape(test_Y, (-1)) - y_test_pred)**2)\n",
    "# R2_dif_fp_test = r2_score(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "# Pearson = stats.pearsonr(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "# MAE = np.mean(abs(np.reshape(test_Y, (-1)) - y_test_pred))\n",
    "\n",
    "print(np.round(Pearson[0],3) , np.round(MSE_dif_fp_test, 10), np.round(R2_dif_fp_test,3), np.round(MAE, 10), np.round(MedAE, 10))\n",
    "\n",
    "\n",
    "np.save(join(\"..\", \"..\", \"data\", \"y_test_pred_xgboost_ESM1b_ts_diff_fp.npy\"), bst.predict(dtest))\n",
    "np.save(join(\"..\", \"..\", \"data\",  \"y_test_true_xgboost_ESM1b_ts_diff_fp.npy\"), test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred_esm1b_ts_drfp = y_test_pred\n",
    "# data_test[\"Estimate Km\"] = [10**x for x in data_test[\"Estimate Km\"]]\n",
    "# filtered_df = data_test[data_test['Uniprot IDs'].apply(lambda x: \"Q9LE06\" in x)]\n",
    "# filtered_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (d) Training model with test and train data for production mode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = np.array(list(data_train[\"GNN FP\"]))\n",
    "train_X = np.concatenate([train_X, np.array(list(data_train[\"Temperature\"]))[:, np.newaxis], np.array(list(data_train[\"pH\"]))[:, np.newaxis]], axis = 1)\n",
    "train_Y = np.array(list(data_train[\"log10_Km\"]))\n",
    "\n",
    "test_X = np.array(list(data_test[\"GNN FP\"]))\n",
    "test_X = np.concatenate([test_X, np.array(list(data_test[\"Temperature\"]))[:, np.newaxis], np.array(list(data_test[\"pH\"]))[:, np.newaxis]], axis = 1)\n",
    "test_Y = np.array(list(data_test[\"log10_Km\"]))\n",
    "\n",
    "train_X = np.concatenate([train_X, test_X])\n",
    "train_Y = np.concatenate([train_Y, test_Y])\n",
    "\n",
    "train_X = train_X.astype(float)\n",
    "test_X = test_X.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.5263672057524258, 3.295903227144922e-35) 0.0021783004824522663 0.113495050268419\n"
     ]
    }
   ],
   "source": [
    "param = {\"random_state\": 42, 'learning_rate': 0.07, 'max_delta_step': 5, 'max_depth': 5, 'min_child_weight': 3, 'num_rounds': 50, 'reg_alpha': 0.7, 'reg_lambda': 0}\n",
    "\n",
    "num_round = param[\"num_rounds\"]\n",
    "param[\"max_depth\"] = int(np.round(param[\"max_depth\"]))\n",
    "\n",
    "del param[\"num_rounds\"]\n",
    "\n",
    "dtrain = xgb.DMatrix(train_X, label = train_Y)\n",
    "dtest = xgb.DMatrix(test_X)\n",
    "\n",
    "bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "\n",
    "y_test_pred = bst.predict(dtest)\n",
    "\n",
    "MSE_dif_fp_test = np.sqrt(np.mean(abs(np.array([10**x for x in np.reshape(test_Y, (-1))]) - np.array([10**x for x in y_test_pred]))**2))\n",
    "R2_dif_fp_test = r2_score(np.array([10**x for x in np.reshape(test_Y, (-1))]), np.array([10**x for x in y_test_pred]))\n",
    "Pearson = stats.pearsonr(np.array([10**x for x in np.reshape(test_Y, (-1))]), np.array([10**x for x in y_test_pred]))\n",
    "\n",
    "# MSE_dif_fp_test = np.mean(abs(np.reshape(test_Y, (-1)) - y_test_pred)**2)\n",
    "# R2_dif_fp_test = r2_score(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "# Pearson = stats.pearsonr(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "\n",
    "print(Pearson, MSE_dif_fp_test, R2_dif_fp_test)\n",
    "\n",
    "pickle.dump(bst, open(join(\"..\", \"..\", \"data\", \"saved_models\",\n",
    "                          \"xgboost_train_and_test.pkl\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = bst.get_score()\n",
    "print(len(data_train[\"ESM2\"][1]))\n",
    "print(len(data_train[\"MACCS FP\"][1]))\n",
    "for key, value in importances.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (e) Validating model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.13112654580345354, 0.5816025413059933) 0.0020553435625985496 -0.04612507100354435 0.0008882180237573709 0.00015993028175534865\n"
     ]
    }
   ],
   "source": [
    "val_X = np.array(list(data_val[\"GNN FP\"]))\n",
    "val_X = np.concatenate([val_X, np.array(list(data_val[\"Temperature\"]))[:, np.newaxis], np.array(list(data_val[\"pH\"]))[:, np.newaxis]], axis = 1)\n",
    "val_Y = np.array(list(data_val[\"log10_Km\"]))\n",
    "\n",
    "val_X = val_X.astype(float)\n",
    "\n",
    "dval = xgb.DMatrix(val_X)\n",
    "\n",
    "y_val_pred = bst.predict(dval)\n",
    "data_val[\"Estimate Km\"] = y_val_pred\n",
    "\n",
    "MSE_dif_fp_val = np.sqrt(np.mean(abs(np.array([10**x for x in np.reshape(val_Y, (-1))]) - np.array([10**x for x in y_val_pred]))**2))\n",
    "R2_dif_fp_val = r2_score(np.array([10**x for x in np.reshape(val_Y, (-1))]), np.array([10**x for x in y_val_pred]))\n",
    "Pearson = stats.pearsonr(np.array([10**x for x in np.reshape(val_Y, (-1))]), np.array([10**x for x in y_val_pred]))\n",
    "MAE = np.mean(abs(np.array([10**x for x in val_Y]) - np.array([10**x for x in y_val_pred])))\n",
    "MedAE = np.median(abs(np.array([10**x for x in val_Y]) - np.array([10**x for x in y_val_pred])))\n",
    "\n",
    "# MSE_dif_fp_test = np.mean(abs(np.reshape(test_Y, (-1)) - y_test_pred)**2)\n",
    "# R2_dif_fp_test = r2_score(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "# Pearson = stats.pearsonr(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "\n",
    "print(Pearson, MSE_dif_fp_val, R2_dif_fp_val, MAE,MedAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training a model with reaction information (diff-fp) + Temperature + pH:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Creating input matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = np.array(list(data_train[\"difference_fp\"]))\n",
    "train_X = np.concatenate([train_X, np.array(list(data_train[\"Temperature\"]))[:, np.newaxis], np.array(list(data_train[\"pH\"]))[:, np.newaxis]], axis = 1)\n",
    "train_Y = np.array(list(data_train[\"log10_Km\"]))\n",
    "\n",
    "test_X = np.array(list(data_test[\"difference_fp\"]))\n",
    "test_X = np.concatenate([test_X, np.array(list(data_test[\"Temperature\"]))[:, np.newaxis], np.array(list(data_test[\"pH\"]))[:, np.newaxis]], axis = 1)\n",
    "test_Y = np.array(list(data_test[\"log10_Km\"]))\n",
    "\n",
    "train_X = train_X.astype(float)\n",
    "test_X = test_X.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Hyperparameter optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [01:12<00:00,  7.29s/trial, best loss: -0.3460038200922663]\n"
     ]
    }
   ],
   "source": [
    "def cross_validation_mse_gradient_boosting(param):\n",
    "    num_round = param[\"num_rounds\"]\n",
    "    del param[\"num_rounds\"]\n",
    "    param[\"max_depth\"] = int(np.round(param[\"max_depth\"]))\n",
    "    # param[\"device\"] = \"cuda\"\n",
    "    param[\"tree_method\"] = \"hist\"\n",
    "    param[\"sampling_method\"] = \"gradient_based\"\n",
    "    \n",
    "    MSE = []\n",
    "    R2 = []\n",
    "    for i in range(5):\n",
    "        train_index, test_index  = train_indices[i], test_indices[i]\n",
    "        dtrain = xgb.DMatrix(train_X[train_index], label = train_Y[train_index])\n",
    "        dvalid = xgb.DMatrix(train_X[test_index])\n",
    "        bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "        y_valid_pred = bst.predict(dvalid)\n",
    "        MSE.append(np.mean(abs(np.reshape(train_Y[test_index], (-1)) - y_valid_pred)**2))\n",
    "        R2.append(r2_score(np.reshape(train_Y[test_index], (-1)),  y_valid_pred))\n",
    "    return(-np.mean(R2))\n",
    "\n",
    "\n",
    "space_gradient_boosting = {\n",
    "    \"learning_rate\": hp.choice(\"learning_rate\", np.linspace(0.01, 0.1, 10)),\n",
    "    \"max_depth\": hp.choice(\"max_depth\", np.linspace(3,10,8)),\n",
    "    #\"subsample\": hp.quniform(\"subsample\", 0.5, 1),\n",
    "    \"reg_lambda\": hp.choice(\"reg_lambda\", np.linspace(0, 1, 11)),\n",
    "    \"reg_alpha\": hp.choice(\"reg_alpha\", np.linspace(0, 1, 11)),\n",
    "    \"max_delta_step\": hp.choice(\"max_delta_step\", np.linspace(1, 5, 5)),\n",
    "    \"min_child_weight\": hp.choice(\"min_child_weight\", np.linspace(1, 6, 6)),\n",
    "    \"num_rounds\":  hp.choice(\"num_rounds\", np.linspace(50, 200, 4))}\n",
    "\n",
    "\n",
    "trials = Trials()\n",
    "best = fmin(fn = cross_validation_mse_gradient_boosting, space = space_gradient_boosting,\n",
    "            algo=rand.suggest, max_evals = 10, trials=trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 7, 'max_delta_step': 0, 'max_depth': 5, 'min_child_weight': 5, 'num_rounds': 1, 'reg_alpha': 8, 'reg_lambda': 6}\n"
     ]
    }
   ],
   "source": [
    "print(best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "full dataset: {'learning_rate': 0.03, 'max_delta_step': 2, 'max_depth': 9, 'min_child_weight': 4, 'num_rounds': 200, 'reg_alpha': 0.3, 'reg_lambda': 0.6}\n",
    "brassicaceae : {'learning_rate': 0.03, 'max_delta_step': 5, 'max_depth': 7, 'min_child_weight': 4, 'num_rounds': 200, 'reg_alpha': 0.5, 'reg_lambda': 0.6}\n",
    "arabidopsis : {'learning_rate': 0.09, 'max_delta_step': 5, 'max_depth': 8, 'min_child_weight': 3, 'num_rounds': 150, 'reg_alpha': 0.8, 'reg_lambda': 0.2}\n",
    "wildtype : {'learning_rate': 0.03, 'max_delta_step': 5, 'max_depth': 9, 'min_child_weight': 5, 'num_rounds': 200, 'reg_alpha': 0.2, 'reg_lambda': 0.3}\n",
    "secondary : {'learning_rate': 0.08, 'max_delta_step': 1, 'max_depth': 8, 'min_child_weight': 5, 'num_rounds': 100, 'reg_alpha': 0.8, 'reg_lambda': 0.6}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c) Training and validating model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\"random_state\": 42, 'learning_rate': 0.09, 'max_delta_step': 5, 'max_depth': 8, 'min_child_weight': 3, 'num_rounds': 150, 'reg_alpha': 0.8, 'reg_lambda': 0.2}\n",
    "\n",
    "\n",
    "num_round = param[\"num_rounds\"]\n",
    "param[\"max_depth\"] = int(np.round(param[\"max_depth\"]))\n",
    "\n",
    "del param[\"num_rounds\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R2 = []\n",
    "MSE = []\n",
    "Pearson = []\n",
    "MAE = []\n",
    "MedAE = []\n",
    "\n",
    "for i in range(5):\n",
    "    train_index, test_index  = train_indices[i], test_indices[i]\n",
    "    dtrain = xgb.DMatrix(train_X[train_index], label = train_Y[train_index])\n",
    "    dvalid = xgb.DMatrix(train_X[test_index])\n",
    "    \n",
    "    bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "    \n",
    "    y_valid_pred = bst.predict(dvalid)\n",
    "    MSE.append(np.sqrt(np.mean(abs(np.array([10**x for x in np.reshape(train_Y[test_index], (-1))]) - np.array([10**x for x in y_valid_pred]))**2)))\n",
    "    R2.append(r2_score([10**x for x in np.reshape(train_Y[test_index], (-1))], [10**x for x in y_valid_pred]))\n",
    "    Pearson.append(stats.pearsonr([10**x for x in np.reshape(train_Y[test_index], (-1))], [10**x for x in y_valid_pred])[0])\n",
    "    MAE.append(np.mean(np.array([10**x for x in train_Y[test_index]]) - np.array([10**x for x in y_valid_pred])))\n",
    "    MedAE.append(np.median(np.array([10**x for x in train_Y[test_index]]) - np.array([10**x for x in y_valid_pred])))\n",
    "\n",
    "    # MSE.append(np.mean(abs(np.reshape(train_Y[test_index], (-1)) - y_valid_pred)**2))\n",
    "    # R2.append(r2_score(np.reshape(train_Y[test_index], (-1)), y_valid_pred))\n",
    "    # Pearson.append(stats.pearsonr(np.reshape(train_Y[test_index], (-1)), y_valid_pred)[0])\n",
    "\n",
    "print(Pearson)\n",
    "print(MSE)\n",
    "print(R2)\n",
    "print(MAE)\n",
    "print(MedAE)\n",
    "\n",
    "np.save(join(\"..\", \"..\", \"data\", \"Pearson_CV_xgboost_ESM1b_ts_diff_fp.npy\"), np.array(Pearson))\n",
    "np.save(join(\"..\", \"..\", \"data\", \"MSE_CV_xgboost_ESM1b_ts_diff_fp.npy\"), np.array(MSE))\n",
    "np.save(join(\"..\", \"..\", \"data\", \"R2_CV_xgboost_ESM1b_ts_diff_fp.npy\"), np.array(R2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.272 0.0010577434 -0.024 0.0005314292 0.0002554804\n"
     ]
    }
   ],
   "source": [
    "dtrain = xgb.DMatrix(train_X, label = train_Y)\n",
    "dtest = xgb.DMatrix(test_X, label = test_Y)\n",
    "\n",
    "bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "\n",
    "y_test_pred = bst.predict(dtest)\n",
    "\n",
    "data_test[\"Estimate Km\"] = y_test_pred\n",
    "\n",
    "MSE_dif_fp_test = np.sqrt(np.mean(abs(np.array([10**x for x in test_Y]) - np.array([10**x for x in y_test_pred]))**2))\n",
    "R2_dif_fp_test = r2_score(np.array([10**x for x in test_Y]), np.array([10**x for x in y_test_pred]))\n",
    "Pearson = stats.pearsonr(np.array([10**x for x in test_Y]), np.array([10**x for x in y_test_pred]))\n",
    "MAE = np.mean(abs(np.array([10**x for x in test_Y]) - np.array([10**x for x in y_test_pred])))\n",
    "MedAE = np.median(abs(np.array([10**x for x in test_Y]) - np.array([10**x for x in y_test_pred])))\n",
    "\n",
    "# MSE_dif_fp_test = np.mean(abs(np.reshape(test_Y, (-1)) - y_test_pred)**2)\n",
    "# R2_dif_fp_test = r2_score(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "# Pearson = stats.pearsonr(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "# MAE = np.mean(abs(np.reshape(test_Y, (-1)) - y_test_pred))\n",
    "\n",
    "print(np.round(Pearson[0],3) , np.round(MSE_dif_fp_test, 10), np.round(R2_dif_fp_test,3), np.round(MAE, 10), np.round(MedAE, 10))\n",
    "\n",
    "\n",
    "np.save(join(\"..\", \"..\", \"data\", \"y_test_pred_xgboost_ESM1b_ts_diff_fp.npy\"), bst.predict(dtest))\n",
    "np.save(join(\"..\", \"..\", \"data\",  \"y_test_true_xgboost_ESM1b_ts_diff_fp.npy\"), test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred_esm1b_ts_drfp = y_test_pred\n",
    "# data_test[\"Estimate Km\"] = [10**x for x in data_test[\"Estimate Km\"]]\n",
    "# filtered_df = data_test[data_test['Uniprot IDs'].apply(lambda x: \"Q9LE06\" in x)]\n",
    "# filtered_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (d) Training model with test and train data for production mode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = np.array(list(data_train[\"difference_fp\"]))\n",
    "train_X = np.concatenate([train_X, np.array(list(data_train[\"Temperature\"]))[:, np.newaxis], np.array(list(data_train[\"pH\"]))[:, np.newaxis]], axis = 1)\n",
    "train_Y = np.array(list(data_train[\"log10_Km\"]))\n",
    "\n",
    "test_X = np.array(list(data_test[\"difference_fp\"]))\n",
    "test_X = np.concatenate([test_X, np.array(list(data_test[\"Temperature\"]))[:, np.newaxis], np.array(list(data_test[\"pH\"]))[:, np.newaxis]], axis = 1)\n",
    "test_Y = np.array(list(data_test[\"log10_Km\"]))\n",
    "\n",
    "train_X = np.concatenate([train_X, test_X])\n",
    "train_Y = np.concatenate([train_Y, test_Y])\n",
    "\n",
    "train_X = train_X.astype(float)\n",
    "test_X = test_X.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.74845661212888, 2.100027498481725e-86) 0.001993069675058591 0.257851947553457\n"
     ]
    }
   ],
   "source": [
    "param = {\"random_state\": 42, 'learning_rate': 0.08, 'max_delta_step': 1, 'max_depth': 8, 'min_child_weight': 5, 'num_rounds': 100, 'reg_alpha': 0.8, 'reg_lambda': 0.6}\n",
    "\n",
    "num_round = param[\"num_rounds\"]\n",
    "param[\"max_depth\"] = int(np.round(param[\"max_depth\"]))\n",
    "\n",
    "del param[\"num_rounds\"]\n",
    "\n",
    "dtrain = xgb.DMatrix(train_X, label = train_Y)\n",
    "dtest = xgb.DMatrix(test_X)\n",
    "\n",
    "bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "\n",
    "y_test_pred = bst.predict(dtest)\n",
    "\n",
    "MSE_dif_fp_test = np.sqrt(np.mean(abs(np.array([10**x for x in np.reshape(test_Y, (-1))]) - np.array([10**x for x in y_test_pred]))**2))\n",
    "R2_dif_fp_test = r2_score(np.array([10**x for x in np.reshape(test_Y, (-1))]), np.array([10**x for x in y_test_pred]))\n",
    "Pearson = stats.pearsonr(np.array([10**x for x in np.reshape(test_Y, (-1))]), np.array([10**x for x in y_test_pred]))\n",
    "\n",
    "# MSE_dif_fp_test = np.mean(abs(np.reshape(test_Y, (-1)) - y_test_pred)**2)\n",
    "# R2_dif_fp_test = r2_score(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "# Pearson = stats.pearsonr(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "\n",
    "print(Pearson, MSE_dif_fp_test, R2_dif_fp_test)\n",
    "\n",
    "pickle.dump(bst, open(join(\"..\", \"..\", \"data\", \"saved_models\",\n",
    "                          \"xgboost_train_and_test.pkl\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = bst.get_score()\n",
    "print(len(data_train[\"ESM2\"][1]))\n",
    "print(len(data_train[\"MACCS FP\"][1]))\n",
    "for key, value in importances.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (e) Validating model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.2949571966789343, 0.20678004814470266) 0.002031214959140672 -0.02170737599768824 0.0008037316701600746 0.00021590418632816844\n"
     ]
    }
   ],
   "source": [
    "val_X = np.array(list(data_val[\"difference_fp\"]))\n",
    "val_X = np.concatenate([val_X, np.array(list(data_val[\"Temperature\"]))[:, np.newaxis], np.array(list(data_val[\"pH\"]))[:, np.newaxis]], axis = 1)\n",
    "val_Y = np.array(list(data_val[\"log10_Km\"]))\n",
    "\n",
    "val_X = val_X.astype(float)\n",
    "\n",
    "dval = xgb.DMatrix(val_X)\n",
    "\n",
    "y_val_pred = bst.predict(dval)\n",
    "data_val[\"Estimate Km\"] = y_val_pred\n",
    "\n",
    "MSE_dif_fp_val = np.sqrt(np.mean(abs(np.array([10**x for x in np.reshape(val_Y, (-1))]) - np.array([10**x for x in y_val_pred]))**2))\n",
    "R2_dif_fp_val = r2_score(np.array([10**x for x in np.reshape(val_Y, (-1))]), np.array([10**x for x in y_val_pred]))\n",
    "Pearson = stats.pearsonr(np.array([10**x for x in np.reshape(val_Y, (-1))]), np.array([10**x for x in y_val_pred]))\n",
    "MAE = np.mean(abs(np.array([10**x for x in val_Y]) - np.array([10**x for x in y_val_pred])))\n",
    "MedAE = np.median(abs(np.array([10**x for x in val_Y]) - np.array([10**x for x in y_val_pred])))\n",
    "\n",
    "# MSE_dif_fp_test = np.mean(abs(np.reshape(test_Y, (-1)) - y_test_pred)**2)\n",
    "# R2_dif_fp_test = r2_score(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "# Pearson = stats.pearsonr(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "\n",
    "print(Pearson, MSE_dif_fp_val, R2_dif_fp_val, MAE, MedAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training a model with reaction and main substrate information (diff-fp/MACCS) + Temperature + pH:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Creating input matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = np.array(list(data_train[\"difference_fp\"]))\n",
    "train_X = np.concatenate([train_X, np.array(list(data_train[\"GNN FP\"])), np.array(list(data_train[\"Temperature\"]))[:, np.newaxis], np.array(list(data_train[\"pH\"]))[:, np.newaxis]], axis = 1)\n",
    "train_Y = np.array(list(data_train[\"log10_Km\"]))\n",
    "\n",
    "test_X = np.array(list(data_test[\"difference_fp\"]))\n",
    "test_X = np.concatenate([test_X, np.array(list(data_test[\"GNN FP\"])), np.array(list(data_test[\"Temperature\"]))[:, np.newaxis], np.array(list(data_test[\"pH\"]))[:, np.newaxis]], axis = 1)\n",
    "test_Y = np.array(list(data_test[\"log10_Km\"]))\n",
    "\n",
    "train_X = train_X.astype(float)\n",
    "test_X = test_X.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Hyperparameter optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [01:46<00:00, 10.63s/trial, best loss: -0.37758133671742733]\n"
     ]
    }
   ],
   "source": [
    "def cross_validation_mse_gradient_boosting(param):\n",
    "    num_round = param[\"num_rounds\"]\n",
    "    del param[\"num_rounds\"]\n",
    "    param[\"max_depth\"] = int(np.round(param[\"max_depth\"]))\n",
    "    # param[\"device\"] = \"cuda\"\n",
    "    param[\"tree_method\"] = \"hist\"\n",
    "    param[\"sampling_method\"] = \"gradient_based\"\n",
    "    \n",
    "    MSE = []\n",
    "    R2 = []\n",
    "    for i in range(5):\n",
    "        train_index, test_index  = train_indices[i], test_indices[i]\n",
    "        dtrain = xgb.DMatrix(train_X[train_index], label = train_Y[train_index])\n",
    "        dvalid = xgb.DMatrix(train_X[test_index])\n",
    "        bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "        y_valid_pred = bst.predict(dvalid)\n",
    "        MSE.append(np.mean(abs(np.reshape(train_Y[test_index], (-1)) - y_valid_pred)**2))\n",
    "        R2.append(r2_score(np.reshape(train_Y[test_index], (-1)),  y_valid_pred))\n",
    "    return(-np.mean(R2))\n",
    "\n",
    "\n",
    "space_gradient_boosting = {\n",
    "    \"learning_rate\": hp.choice(\"learning_rate\", np.linspace(0.01, 0.1, 10)),\n",
    "    \"max_depth\": hp.choice(\"max_depth\", np.linspace(3,10,8)),\n",
    "    #\"subsample\": hp.quniform(\"subsample\", 0.5, 1),\n",
    "    \"reg_lambda\": hp.choice(\"reg_lambda\", np.linspace(0, 1, 11)),\n",
    "    \"reg_alpha\": hp.choice(\"reg_alpha\", np.linspace(0, 1, 11)),\n",
    "    \"max_delta_step\": hp.choice(\"max_delta_step\", np.linspace(1, 5, 5)),\n",
    "    \"min_child_weight\": hp.choice(\"min_child_weight\", np.linspace(1, 6, 6)),\n",
    "    \"num_rounds\":  hp.choice(\"num_rounds\", np.linspace(50, 200, 4))}\n",
    "\n",
    "\n",
    "trials = Trials()\n",
    "best = fmin(fn = cross_validation_mse_gradient_boosting, space = space_gradient_boosting,\n",
    "            algo=rand.suggest, max_evals = 10, trials=trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 5, 'max_delta_step': 1, 'max_depth': 7, 'min_child_weight': 1, 'num_rounds': 3, 'reg_alpha': 2, 'reg_lambda': 5}\n"
     ]
    }
   ],
   "source": [
    "print(best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "full dataset: {'learning_rate': 0.1, 'max_delta_step': 4, 'max_depth': 7, 'min_child_weight': 2, 'num_rounds': 3, 'reg_alpha': 0.4, 'reg_lambda': 0.4}\n",
    "brassicaceae : {'learning_rate': 0.06, 'max_delta_step': 5, 'max_depth': 6, 'min_child_weight': 4, 'num_rounds': 3, 'reg_alpha': 0.4, 'reg_lambda': 0.9}\n",
    "arabidopsis: {'learning_rate': 0.07, 'max_delta_step': 5, 'max_depth': 5, 'min_child_weight': 5, 'num_rounds': 3, 'reg_alpha': 0.8, 'reg_lambda': 0.4}\n",
    "wildtype : {'learning_rate': 0.1, 'max_delta_step': 1, 'max_depth': 4, 'min_child_weight': 2, 'num_rounds': 3, 'reg_alpha': 0.6, 'reg_lambda': 0.9}\n",
    "secondary : {'learning_rate': 0.06, 'max_delta_step': 2, 'max_depth': 10, 'min_child_weight': 2, 'num_rounds': 200, 'reg_alpha': 0.2, 'reg_lambda': 0.5}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c) Training and validating model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\"random_state\": 42, 'learning_rate': 0.07, 'max_delta_step': 5, 'max_depth': 5, 'min_child_weight': 5, 'num_rounds': 3, 'reg_alpha': 0.8, 'reg_lambda': 0.4}\n",
    "\n",
    "num_round = param[\"num_rounds\"]\n",
    "param[\"max_depth\"] = int(np.round(param[\"max_depth\"]))\n",
    "\n",
    "del param[\"num_rounds\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R2 = []\n",
    "MSE = []\n",
    "Pearson = []\n",
    "MAE = []\n",
    "MedAE = []\n",
    "\n",
    "for i in range(5):\n",
    "    train_index, test_index  = train_indices[i], test_indices[i]\n",
    "    dtrain = xgb.DMatrix(train_X[train_index], label = train_Y[train_index])\n",
    "    dvalid = xgb.DMatrix(train_X[test_index])\n",
    "    \n",
    "    bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "    \n",
    "    y_valid_pred = bst.predict(dvalid)\n",
    "    MSE.append(np.sqrt(np.mean(abs(np.array([10**x for x in np.reshape(train_Y[test_index], (-1))]) - np.array([10**x for x in y_valid_pred]))**2)))\n",
    "    R2.append(r2_score([10**x for x in np.reshape(train_Y[test_index], (-1))], [10**x for x in y_valid_pred]))\n",
    "    Pearson.append(stats.pearsonr([10**x for x in np.reshape(train_Y[test_index], (-1))], [10**x for x in y_valid_pred])[0])\n",
    "    MAE.append(np.mean(np.array([10**x for x in train_Y[test_index]]) - np.array([10**x for x in y_valid_pred])))\n",
    "    MedAE.append(np.median(np.array([10**x for x in train_Y[test_index]]) - np.array([10**x for x in y_valid_pred])))\n",
    "\n",
    "    # MSE.append(np.mean(abs(np.reshape(train_Y[test_index], (-1)) - y_valid_pred)**2))\n",
    "    # R2.append(r2_score(np.reshape(train_Y[test_index], (-1)), y_valid_pred))\n",
    "    # Pearson.append(stats.pearsonr(np.reshape(train_Y[test_index], (-1)), y_valid_pred)[0])\n",
    "\n",
    "print(Pearson)\n",
    "print(MSE)\n",
    "print(R2)\n",
    "print(MAE)\n",
    "print(MedAE)\n",
    "\n",
    "np.save(join(\"..\", \"..\", \"data\", \"Pearson_CV_xgboost_ESM1b_ts_diff_fp.npy\"), np.array(Pearson))\n",
    "np.save(join(\"..\", \"..\", \"data\", \"MSE_CV_xgboost_ESM1b_ts_diff_fp.npy\"), np.array(MSE))\n",
    "np.save(join(\"..\", \"..\", \"data\", \"R2_CV_xgboost_ESM1b_ts_diff_fp.npy\"), np.array(R2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3250982320778769 0.5152965751 -242945.233 0.5050440058 0.5684470836\n"
     ]
    }
   ],
   "source": [
    "dtrain = xgb.DMatrix(train_X, label = train_Y)\n",
    "dtest = xgb.DMatrix(test_X, label = test_Y)\n",
    "\n",
    "bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "\n",
    "y_test_pred = bst.predict(dtest)\n",
    "\n",
    "data_test[\"Estimate Km\"] = y_test_pred\n",
    "\n",
    "MSE_dif_fp_test = np.sqrt(np.mean(abs(np.array([10**x for x in test_Y]) - np.array([10**x for x in y_test_pred]))**2))\n",
    "R2_dif_fp_test = r2_score(np.array([10**x for x in test_Y]), np.array([10**x for x in y_test_pred]))\n",
    "Pearson = np.corrcoef(np.array([10**x for x in test_Y]), np.array([10**x for x in y_test_pred]))\n",
    "MAE = np.mean(abs(np.array([10**x for x in test_Y]) - np.array([10**x for x in y_test_pred])))\n",
    "MedAE = np.median(abs(np.array([10**x for x in test_Y]) - np.array([10**x for x in y_test_pred])))\n",
    "\n",
    "# MSE_dif_fp_test = np.mean(abs(np.reshape(test_Y, (-1)) - y_test_pred)**2)\n",
    "# R2_dif_fp_test = r2_score(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "# Pearson = stats.pearsonr(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "# MAE = np.mean(abs(np.reshape(test_Y, (-1)) - y_test_pred))\n",
    "\n",
    "print(np.round(Pearson[1,0],20) , np.round(MSE_dif_fp_test, 10), np.round(R2_dif_fp_test,3), np.round(MAE, 10), np.round(MedAE, 10))\n",
    "\n",
    "\n",
    "np.save(join(\"..\", \"..\", \"data\", \"y_test_pred_xgboost_ESM1b_ts_diff_fp.npy\"), bst.predict(dtest))\n",
    "np.save(join(\"..\", \"..\", \"data\",  \"y_test_true_xgboost_ESM1b_ts_diff_fp.npy\"), test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred_esm1b_ts_drfp = y_test_pred\n",
    "# data_test[\"Estimate Km\"] = [10**x for x in data_test[\"Estimate Km\"]]\n",
    "# filtered_df = data_test[data_test['Uniprot IDs'].apply(lambda x: \"Q9LE06\" in x)]\n",
    "# filtered_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (d) Training model with test and train data for production mode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = np.array(list(data_train[\"difference_fp\"]))\n",
    "train_X = np.concatenate([train_X, np.array(list(data_train[\"GNN FP\"])), np.array(list(data_train[\"Temperature\"]))[:, np.newaxis], np.array(list(data_train[\"pH\"]))[:, np.newaxis]], axis = 1)\n",
    "train_Y = np.array(list(data_train[\"log10_Km\"]))\n",
    "\n",
    "test_X = np.array(list(data_test[\"difference_fp\"]))\n",
    "test_X = np.concatenate([test_X, np.array(list(data_test[\"GNN FP\"])), np.array(list(data_test[\"Temperature\"]))[:, np.newaxis], np.array(list(data_test[\"pH\"]))[:, np.newaxis]], axis = 1)\n",
    "test_Y = np.array(list(data_test[\"log10_Km\"]))\n",
    "\n",
    "train_X = np.concatenate([train_X, test_X])\n",
    "train_Y = np.concatenate([train_Y, test_Y])\n",
    "\n",
    "train_X = train_X.astype(float)\n",
    "test_X = test_X.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8697062014019628, 3.788618891222201e-147) 0.0012939506959099548 0.6871896987875892\n"
     ]
    }
   ],
   "source": [
    "param = {\"random_state\": 42, 'learning_rate': 0.06, 'max_delta_step': 2, 'max_depth': 10, 'min_child_weight': 2, 'num_rounds': 200, 'reg_alpha': 0.2, 'reg_lambda': 0.5}\n",
    "\n",
    "num_round = param[\"num_rounds\"]\n",
    "param[\"max_depth\"] = int(np.round(param[\"max_depth\"]))\n",
    "\n",
    "del param[\"num_rounds\"]\n",
    "\n",
    "dtrain = xgb.DMatrix(train_X, label = train_Y)\n",
    "dtest = xgb.DMatrix(test_X)\n",
    "\n",
    "bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "\n",
    "y_test_pred = bst.predict(dtest)\n",
    "\n",
    "MSE_dif_fp_test = np.sqrt(np.mean(abs(np.array([10**x for x in np.reshape(test_Y, (-1))]) - np.array([10**x for x in y_test_pred]))**2))\n",
    "R2_dif_fp_test = r2_score(np.array([10**x for x in np.reshape(test_Y, (-1))]), np.array([10**x for x in y_test_pred]))\n",
    "Pearson = stats.pearsonr(np.array([10**x for x in np.reshape(test_Y, (-1))]), np.array([10**x for x in y_test_pred]))\n",
    "\n",
    "# MSE_dif_fp_test = np.mean(abs(np.reshape(test_Y, (-1)) - y_test_pred)**2)\n",
    "# R2_dif_fp_test = r2_score(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "# Pearson = stats.pearsonr(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "\n",
    "print(Pearson, MSE_dif_fp_test, R2_dif_fp_test)\n",
    "\n",
    "pickle.dump(bst, open(join(\"..\", \"..\", \"data\", \"saved_models\",\n",
    "                          \"xgboost_train_and_test.pkl\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = bst.get_score()\n",
    "print(len(data_train[\"ESM2\"][1]))\n",
    "print(len(data_train[\"MACCS FP\"][1]))\n",
    "for key, value in importances.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (e) Validating model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.009875956202451878 0.002091805566082048 -0.08357102823186424 0.0009291326453864157 0.00021330710213197273\n"
     ]
    }
   ],
   "source": [
    "val_X = np.array(list(data_val[\"difference_fp\"]))\n",
    "val_X = np.concatenate([val_X, np.array(list(data_val[\"GNN FP\"])), np.array(list(data_val[\"Temperature\"]))[:, np.newaxis], np.array(list(data_val[\"pH\"]))[:, np.newaxis]], axis = 1)\n",
    "val_Y = np.array(list(data_val[\"log10_Km\"]))\n",
    "\n",
    "val_X = val_X.astype(float)\n",
    "\n",
    "dval = xgb.DMatrix(val_X)\n",
    "\n",
    "y_val_pred = bst.predict(dval)\n",
    "data_val[\"Estimate Km\"] = y_val_pred\n",
    "\n",
    "MSE_dif_fp_val = np.sqrt(np.mean(abs(np.array([10**x for x in np.reshape(val_Y, (-1))]) - np.array([10**x for x in y_val_pred]))**2))\n",
    "R2_dif_fp_val = r2_score(np.array([10**x for x in np.reshape(val_Y, (-1))]), np.array([10**x for x in y_val_pred]))\n",
    "Pearson = np.corrcoef(np.array([10**x for x in np.reshape(val_Y, (-1))]), np.array([10**x for x in y_val_pred]))\n",
    "MAE = np.mean(abs(np.array([10**x for x in val_Y]) - np.array([10**x for x in y_val_pred])))\n",
    "MedAE = np.median(abs(np.array([10**x for x in val_Y]) - np.array([10**x for x in y_val_pred])))\n",
    "\n",
    "# MSE_dif_fp_test = np.mean(abs(np.reshape(test_Y, (-1)) - y_test_pred)**2)\n",
    "# R2_dif_fp_test = r2_score(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "# Pearson = stats.pearsonr(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "\n",
    "print(Pearson[1,0], MSE_dif_fp_val, R2_dif_fp_val, MAE, MedAE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
