{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from os.path import join\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy import stats\n",
    "import xgboost as xgb\n",
    "from hyperopt import fmin, rand, hp, Trials, tpe\n",
    "from plotnine import *\n",
    "import shap\n",
    "rstate = np.random.default_rng(42)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import matplotlib as mpl\n",
    "\n",
    "datasets_dir = \"../../data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading training, testing and validation data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250, 47, 21)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split = \"secondary\"\n",
    "\n",
    "data_train = pd.read_pickle(join(datasets_dir, \"splits\", split, \"training_data.pkl\"))\n",
    "data_test = pd.read_pickle(join(datasets_dir, \"splits\", split, \"test_data.pkl\"))\n",
    "data_val = pd.read_pickle(join(datasets_dir, \"splits\", split, \"val_data.pkl\"))\n",
    "\n",
    "data_train[\"log10_Km\"] = np.log10(data_train[\"Km\"])\n",
    "data_test[\"log10_Km\"] = np.log10(data_test[\"Km\"])\n",
    "data_val[\"log10_Km\"] = np.log10(data_val[\"Km\"])\n",
    "\n",
    "data_train.rename(columns = {\"Enzyme rep\" : \"ESM2\"}, inplace = True)\n",
    "data_test.rename(columns = {\"Enzyme rep\" : \"ESM2\"}, inplace = True)\n",
    "data_val.rename(columns = {\"Enzyme rep\" : \"ESM2\"}, inplace = True)\n",
    "\n",
    "data_train['Temperature'] = data_train['Temperature'].replace('-', np.nan)\n",
    "data_test['Temperature'] = data_test['Temperature'].replace('-', np.nan)\n",
    "data_val['Temperature'] = data_val['Temperature'].replace('-', np.nan)\n",
    "data_train['pH'] = data_train['pH'].replace('-', np.nan)\n",
    "data_test['pH'] = data_test['pH'].replace('-', np.nan)\n",
    "data_val['pH'] = data_val['pH'].replace('-', np.nan)\n",
    "data_train['Type'] = data_train['Type'].replace('wildtype', 1)\n",
    "data_train['Type'] = data_train['Type'].replace('mutant', 2)\n",
    "data_test['Type'] = data_test['Type'].replace('wildtype', 1)\n",
    "data_test['Type'] = data_test['Type'].replace('mutant', 2)\n",
    "data_val['Type'] = data_val['Type'].replace('wildtype', 1)\n",
    "data_val['Type'] = data_val['Type'].replace('mutant', 2)\n",
    "\n",
    "data_train['MACCS FP'] = data_train['MACCS FP'].astype(str)\n",
    "data_test['MACCS FP'] = data_test['MACCS FP'].astype(str)\n",
    "data_val['MACCS FP'] = data_val['MACCS FP'].astype(str)\n",
    "\n",
    "len(data_train), len(data_test), len(data_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices = list(np.load(join(datasets_dir, \"splits\", split, \"CV_train_indices_Seed plants.npy\"), allow_pickle = True))\n",
    "test_indices = list(np.load(join(datasets_dir, \"splits\", split, \"CV_test_indices_Seed plants.npy\"), allow_pickle = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "data_test = data_test[~data_test['GNN FP'].isnull()]\n",
    "\n",
    "nan_rows = data_train[data_train['GNN FP'].apply(lambda x: not isinstance(x, np.ndarray))]\n",
    "\n",
    "indices_with_nan = nan_rows.index.tolist()\n",
    "print(indices_with_nan)\n",
    "\n",
    "for ind, sub_list in enumerate(train_indices):\n",
    "    for elem in sub_list:\n",
    "        if elem in indices_with_nan:\n",
    "            sub_list.remove(elem)\n",
    "\n",
    "for ind, sub_list in enumerate(train_indices):\n",
    "    for num in indices_with_nan:\n",
    "        for i, elem in enumerate(sub_list):\n",
    "            if elem > num:\n",
    "                train_indices[ind][i] = elem-1\n",
    "\n",
    "for ind, sub_list in enumerate(test_indices):\n",
    "    for elem in sub_list:\n",
    "        if elem in indices_with_nan:\n",
    "            sub_list.remove(elem)\n",
    "\n",
    "for ind, sub_list in enumerate(test_indices):\n",
    "    for num in indices_with_nan:\n",
    "        for i, elem in enumerate(sub_list):\n",
    "            if elem > num:\n",
    "                test_indices[ind][i] = elem-1  \n",
    "\n",
    "data_train = data_train[data_train['GNN FP'].apply(lambda x: isinstance(x, np.ndarray))]\n",
    "data_train.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_combined = pd.concat([data_train,data_test])\n",
    "data_combined.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_val.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Training a model with enzyme and main substrate information (ESM-2/GNN) + Temperature + pH :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Creating input matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = np.array(list(data_train[\"ESM2\"]))\n",
    "train_X = np.concatenate([train_X, np.array(list(data_train['GNN FP'])), np.array(list(data_train[\"Temperature\"]))[:, np.newaxis], np.array(list(data_train[\"pH\"]))[:, np.newaxis]], axis = 1)\n",
    "train_Y = np.array(list(data_train[\"log10_Km\"]))\n",
    "\n",
    "test_X = np.array(list(data_test[\"ESM2\"]))\n",
    "test_X = np.concatenate([test_X, np.array(list(data_test['GNN FP'])), np.array(list(data_test[\"Temperature\"]))[:, np.newaxis], np.array(list(data_test[\"pH\"]))[:, np.newaxis]], axis = 1)\n",
    "test_Y = np.array(list(data_test[\"log10_Km\"]))\n",
    "\n",
    "train_X = train_X.astype(float)\n",
    "test_X = test_X.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Hyperparameter optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def cross_validation_mse_gradient_boosting(param):\n",
    "#     num_round = param[\"num_rounds\"]\n",
    "#     del param[\"num_rounds\"]\n",
    "#     param[\"max_depth\"] = int(np.round(param[\"max_depth\"]))\n",
    "#     # param[\"device\"] = \"cuda\"\n",
    "#     param[\"tree_method\"] = \"hist\"\n",
    "#     param[\"sampling_method\"] = \"gradient_based\"\n",
    "    \n",
    "#     MSE = []\n",
    "#     R2 = []\n",
    "#     for i in range(5):\n",
    "#         train_index, test_index  = train_indices[i], test_indices[i]\n",
    "#         dtrain = xgb.DMatrix(train_X[train_index], label = train_Y[train_index])\n",
    "#         dvalid = xgb.DMatrix(train_X[test_index])\n",
    "#         bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "#         y_valid_pred = bst.predict(dvalid)\n",
    "#         MSE.append(np.mean(abs(np.reshape(train_Y[test_index], (-1)) - y_valid_pred)**2))\n",
    "#         R2.append(r2_score(np.reshape(train_Y[test_index], (-1)),  y_valid_pred))\n",
    "#     return(-np.mean(R2))\n",
    "\n",
    "# space_gradient_boosting = {\n",
    "#     \"learning_rate\": hp.choice(\"learning_rate\", [0.01,0.1,0.2]),\n",
    "#     \"max_depth\": hp.quniform(\"max_depth\", 3, 10, 1),\n",
    "#     # \"subsample\": hp.quniform(\"subsample\", 0.5, 1, 0.5),\n",
    "#     \"reg_lambda\": hp.quniform(\"reg_lambda\", 0, 1, 0.2),\n",
    "#     \"reg_alpha\": hp.quniform(\"reg_alpha\", 0, 1, 0.2),\n",
    "#     \"max_delta_step\": hp.quniform(\"max_delta_step\", 1, 5, 1),\n",
    "#     \"min_child_weight\": hp.quniform(\"min_child_weight\", 1, 6, 1),\n",
    "#     \"num_rounds\":  hp.choice(\"num_rounds\", [100,250,500,1000])\n",
    "#     }\n",
    "\n",
    "# rstate = np.random.default_rng(42)\n",
    "# trials = Trials()\n",
    "# best = fmin(fn = cross_validation_mse_gradient_boosting, space = space_gradient_boosting,\n",
    "#             algo=rand.suggest, max_evals = 20, trials=trials, return_argmin=False, rstate=rstate)\n",
    "\n",
    "# print(best)\n",
    "# param = best\n",
    "# param[\"random_state\"] = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if split == \"full\":\n",
    "    param = {\"random_state\": 42, 'learning_rate': 0.1, 'max_delta_step': 3.0, 'max_depth': 4.0, 'min_child_weight': 5.0, 'num_rounds': 500, 'reg_alpha': 0.2, 'reg_lambda': 0.4}\n",
    "elif split == \"Arabidopsis\":\n",
    "    param = {\"random_state\": 42, 'learning_rate': 0.1, 'max_delta_step': 2.0, 'max_depth': 4.0, 'min_child_weight': 6.0, 'num_rounds': 1000, 'reg_alpha': 0.2, 'reg_lambda': 0.4}\n",
    "elif split == \"Brassicaceae\":\n",
    "    param = {\"random_state\": 42, 'learning_rate': 0.1, 'max_delta_step': 2.0, 'max_depth': 4.0, 'min_child_weight': 6.0, 'num_rounds': 1000, 'reg_alpha': 0.2, 'reg_lambda': 0.4}\n",
    "elif split == \"wildtype\":\n",
    "    param = {\"random_state\": 42, 'learning_rate': 0.1, 'max_delta_step': 3.0, 'max_depth': 4.0, 'min_child_weight': 5.0, 'num_rounds': 500, 'reg_alpha': 0.2, 'reg_lambda': 0.4}\n",
    "else:\n",
    "    param = {\"random_state\": 42, 'learning_rate': 0.2, 'max_delta_step': 1.0, 'max_depth': 7.0, 'min_child_weight': 1.0, 'num_rounds': 1000, 'reg_alpha': 0.6, 'reg_lambda': 0.4}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c) Training and validating model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_round = param[\"num_rounds\"]\n",
    "param[\"max_depth\"] = int(np.round(param[\"max_depth\"]))\n",
    "param[\"tree_method\"] = \"gpu_hist\"\n",
    "param[\"sampling_method\"] = \"gradient_based\"\n",
    "del param[\"num_rounds\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R2 = []\n",
    "MSE = []\n",
    "Pearson = []\n",
    "MAE = []\n",
    "MedAE = []\n",
    "\n",
    "for i in range(5):\n",
    "    train_index, test_index  = train_indices[i], test_indices[i]\n",
    "    dtrain = xgb.DMatrix(train_X[train_index], label = train_Y[train_index])\n",
    "    dvalid = xgb.DMatrix(train_X[test_index])\n",
    "    \n",
    "    bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "    \n",
    "    y_valid_pred = bst.predict(dvalid)\n",
    "    MSE.append(np.sqrt(np.mean(abs(np.array([10**x for x in np.reshape(train_Y[test_index], (-1))]) - np.array([10**x for x in y_valid_pred]))**2)))\n",
    "    R2.append(r2_score([10**x for x in np.reshape(train_Y[test_index], (-1))], [10**x for x in y_valid_pred]))\n",
    "    Pearson.append(stats.pearsonr([10**x for x in np.reshape(train_Y[test_index], (-1))], [10**x for x in y_valid_pred])[0])\n",
    "    MAE.append(np.mean(abs(np.array([10**x for x in train_Y[test_index]]) - np.array([10**x for x in y_valid_pred]))))\n",
    "    MedAE.append(np.median(abs(np.array([10**x for x in train_Y[test_index]]) - np.array([10**x for x in y_valid_pred]))))\n",
    "\n",
    "print(Pearson)\n",
    "print(MSE)\n",
    "print(R2)\n",
    "print(MAE)\n",
    "print(MedAE)\n",
    "\n",
    "np.save(join(\"..\", \"..\", \"results\", split, \"Pearson_CV_xgboost_ESM2_gnn_fp.npy\"), np.array(Pearson))\n",
    "np.save(join(\"..\", \"..\", \"results\", split, \"MSE_CV_xgboost_ESM2_gnn_fp.npy\"), np.array(MSE))\n",
    "np.save(join(\"..\", \"..\", \"results\", split, \"R2_CV_xgboost_ESM2_gnn_fp.npy\"), np.array(R2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(train_X, label = train_Y)\n",
    "dtest = xgb.DMatrix(test_X, label = test_Y)\n",
    "\n",
    "bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "\n",
    "y_test_pred = bst.predict(dtest)\n",
    "\n",
    "\n",
    "\n",
    "MSE_dif_fp_test = np.sqrt(np.mean(abs(np.array([10**x for x in test_Y]) - np.array([10**x for x in y_test_pred]))**2))\n",
    "R2_dif_fp_test = r2_score(np.array([10**x for x in test_Y]), np.array([10**x for x in y_test_pred]))\n",
    "Pearson = stats.pearsonr(np.array([10**x for x in test_Y]), np.array([10**x for x in y_test_pred]))\n",
    "MAE = np.mean(abs(np.array([10**x for x in test_Y]) - np.array([10**x for x in y_test_pred])))\n",
    "MedAE = np.median(abs(np.array([10**x for x in test_Y]) - np.array([10**x for x in y_test_pred])))\n",
    "\n",
    "print(np.round(Pearson[0],20) , np.round(MSE_dif_fp_test, 10), np.round(R2_dif_fp_test,3), np.round(MAE, 10), np.round(MedAE, 10))\n",
    "\n",
    "np.save(join(\"..\", \"..\", \"results\", split, \"y_test_pred_xgboost_ESM2_gnn_fp.npy\"), bst.predict(dtest))\n",
    "np.save(join(\"..\", \"..\", \"results\", split,  \"y_test_true_xgboost_ESM2_gnn_fp.npy\"), test_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (d) Training model with test and train data for production mode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = np.array(list(data_train[\"ESM2\"]))\n",
    "train_X = np.concatenate([train_X, np.array(list(data_train['GNN FP'])), np.array(list(data_train[\"Temperature\"]))[:, np.newaxis], np.array(list(data_train[\"pH\"]))[:, np.newaxis]], axis = 1)\n",
    "train_Y = np.array(list(data_train[\"log10_Km\"]))\n",
    "\n",
    "test_X = np.array(list(data_test[\"ESM2\"]))\n",
    "test_X = np.concatenate([test_X, np.array(list(data_test['GNN FP'])), np.array(list(data_test[\"Temperature\"]))[:, np.newaxis], np.array(list(data_test[\"pH\"]))[:, np.newaxis]], axis = 1)\n",
    "test_Y = np.array(list(data_test[\"log10_Km\"]))\n",
    "\n",
    "train_X = np.concatenate([train_X, test_X])\n",
    "train_Y = np.concatenate([train_Y, test_Y])\n",
    "\n",
    "train_X = train_X.astype(float)\n",
    "test_X = test_X.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(train_X, label = train_Y)\n",
    "dtest = xgb.DMatrix(test_X)\n",
    "\n",
    "bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "\n",
    "y_test_pred = bst.predict(dtest)\n",
    "\n",
    "MSE_dif_fp_test = np.sqrt(np.mean(abs(np.array([10**x for x in np.reshape(test_Y, (-1))]) - np.array([10**x for x in y_test_pred]))**2))\n",
    "R2_dif_fp_test = r2_score(np.array([10**x for x in np.reshape(test_Y, (-1))]), np.array([10**x for x in y_test_pred]))\n",
    "Pearson = stats.pearsonr(np.array([10**x for x in np.reshape(test_Y, (-1))]), np.array([10**x for x in y_test_pred]))\n",
    "\n",
    "print(Pearson, MSE_dif_fp_test, R2_dif_fp_test)\n",
    "\n",
    "pickle.dump(bst, open(join(\"..\", \"..\", \"results\", split, \"xgboost_ESM2_gnn_fp.pkl\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Validating model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_X = np.array(list(data_val[\"ESM2\"]))\n",
    "val_X = np.concatenate([val_X, np.array(list(data_val[\"GNN FP\"])), np.array(list(data_val[\"Temperature\"]))[:, np.newaxis], np.array(list(data_val[\"pH\"]))[:, np.newaxis]], axis = 1)\n",
    "val_Y = np.array(list(data_val[\"log10_Km\"]))\n",
    "\n",
    "val_X = val_X.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dval = xgb.DMatrix(val_X)\n",
    "\n",
    "y_val_pred = bst.predict(dval)\n",
    "\n",
    "MSE_dif_fp_val = np.sqrt(np.mean(abs(np.array([10**x for x in np.reshape(val_Y, (-1))]) - np.array([10**x for x in y_val_pred]))**2))\n",
    "R2_dif_fp_val = r2_score(np.array([10**x for x in np.reshape(val_Y, (-1))]), np.array([10**x for x in y_val_pred]))\n",
    "Pearson = stats.pearsonr(np.array([10**x for x in np.reshape(val_Y, (-1))]), np.array([10**x for x in y_val_pred]))\n",
    "MAE = np.mean(abs(np.array([10**x for x in val_Y]) - np.array([10**x for x in y_val_pred])))\n",
    "MedAE = np.median(abs(np.array([10**x for x in val_Y]) - np.array([10**x for x in y_val_pred])))\n",
    "\n",
    "print(Pearson, MSE_dif_fp_val, R2_dif_fp_val, MAE, MedAE)\n",
    "\n",
    "np.save(join(\"..\", \"..\", \"results\", split, \"y_test_true_xgboost_esm2_gnn_fp.npy\"), val_Y)\n",
    "np.save(join(\"..\", \"..\", \"results\", split, \"y_val_pred_xgboost_esm2_gnn_fp.npy\"), y_val_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training a model with enzyme and reaction information (ESM-2/diff_fp) + Temperature + pH :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Creating input matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = np.array(list(data_train[\"ESM2\"]))\n",
    "train_X = np.concatenate([train_X, np.array(list(data_train[\"difference_fp\"])), np.array(list(data_train[\"Temperature\"]))[:, np.newaxis], np.array(list(data_train[\"pH\"]))[:, np.newaxis]], axis = 1)\n",
    "train_Y = np.array(list(data_train[\"log10_Km\"]))\n",
    "\n",
    "test_X = np.array(list(data_test[\"ESM2\"]))\n",
    "test_X = np.concatenate([test_X, np.array(list(data_test[\"difference_fp\"])), np.array(list(data_test[\"Temperature\"]))[:, np.newaxis], np.array(list(data_test[\"pH\"]))[:, np.newaxis]], axis = 1)\n",
    "test_Y = np.array(list(data_test[\"log10_Km\"]))\n",
    "\n",
    "train_X = train_X.astype(float)\n",
    "test_X = test_X.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Hyperparameter optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def cross_validation_mse_gradient_boosting(param):\n",
    "#     num_round = param[\"num_rounds\"]\n",
    "#     del param[\"num_rounds\"]\n",
    "#     param[\"max_depth\"] = int(np.round(param[\"max_depth\"]))\n",
    "#     # param[\"device\"] = \"cuda\"\n",
    "#     param[\"tree_method\"] = \"hist\"\n",
    "#     param[\"sampling_method\"] = \"gradient_based\"\n",
    "    \n",
    "#     MSE = []\n",
    "#     R2 = []\n",
    "#     for i in range(5):\n",
    "#         train_index, test_index  = train_indices[i], test_indices[i]\n",
    "#         dtrain = xgb.DMatrix(train_X[train_index], label = train_Y[train_index])\n",
    "#         dvalid = xgb.DMatrix(train_X[test_index])\n",
    "#         bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "#         y_valid_pred = bst.predict(dvalid)\n",
    "#         MSE.append(np.mean(abs(np.reshape(train_Y[test_index], (-1)) - y_valid_pred)**2))\n",
    "#         R2.append(r2_score(np.reshape(train_Y[test_index], (-1)),  y_valid_pred))\n",
    "#     return(-np.mean(R2))\n",
    "\n",
    "# space_gradient_boosting = {\n",
    "#     \"learning_rate\": hp.choice(\"learning_rate\", [0.01,0.1,0.2]),\n",
    "#     \"max_depth\": hp.quniform(\"max_depth\", 3, 10, 1),\n",
    "#     # \"subsample\": hp.quniform(\"subsample\", 0.5, 1, 0.5),\n",
    "#     \"reg_lambda\": hp.quniform(\"reg_lambda\", 0, 1, 0.2),\n",
    "#     \"reg_alpha\": hp.quniform(\"reg_alpha\", 0, 1, 0.2),\n",
    "#     \"max_delta_step\": hp.quniform(\"max_delta_step\", 1, 5, 1),\n",
    "#     \"min_child_weight\": hp.quniform(\"min_child_weight\", 1, 6, 1),\n",
    "#     \"num_rounds\":  hp.choice(\"num_rounds\", [100,250,500,1000])\n",
    "#     }\n",
    "\n",
    "# rstate = np.random.default_rng(42)\n",
    "# trials = Trials()\n",
    "# best = fmin(fn = cross_validation_mse_gradient_boosting, space = space_gradient_boosting,\n",
    "#             algo=rand.suggest, max_evals = 20, trials=trials, return_argmin=False, rstate=rstate)\n",
    "\n",
    "# print(best)\n",
    "# param = best\n",
    "# param[\"random_state\"] = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if split == \"full\":\n",
    "    param = {\"random_state\": 42, 'learning_rate': 0.01, 'max_delta_step': 5.0, 'max_depth': 5.0, 'min_child_weight': 3.0, 'num_rounds': 1000, 'reg_alpha': 0.6, 'reg_lambda': 0.8}\n",
    "elif split == \"Arabidopsis\":\n",
    "    param = {\"random_state\": 42, 'learning_rate': 0.1, 'max_delta_step': 3.0, 'max_depth': 4.0, 'min_child_weight': 5.0, 'num_rounds': 500, 'reg_alpha': 0.2, 'reg_lambda': 0.4}\n",
    "elif split == \"Brassicaceae\":\n",
    "    param = {\"random_state\": 42, 'learning_rate': 0.1, 'max_delta_step': 2.0, 'max_depth': 4.0, 'min_child_weight': 6.0, 'num_rounds': 1000, 'reg_alpha': 0.2, 'reg_lambda': 0.4}\n",
    "elif split == \"wildtype\":\n",
    "    param = {\"random_state\": 42, 'learning_rate': 0.1, 'max_delta_step': 3.0, 'max_depth': 4.0, 'min_child_weight': 5.0, 'num_rounds': 500, 'reg_alpha': 0.2, 'reg_lambda': 0.4}\n",
    "else:\n",
    "    param = {\"random_state\": 42, 'learning_rate': 0.2, 'max_delta_step': 4.0, 'max_depth': 4.0, 'min_child_weight': 3.0, 'num_rounds': 100, 'reg_alpha': 0.8, 'reg_lambda': 0.0}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c) Training and validating model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_round = param[\"num_rounds\"]\n",
    "param[\"max_depth\"] = int(np.round(param[\"max_depth\"]))\n",
    "param[\"tree_method\"] = \"gpu_hist\"\n",
    "param[\"sampling_method\"] = \"gradient_based\"\n",
    "del param[\"num_rounds\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R2 = []\n",
    "MSE = []\n",
    "Pearson = []\n",
    "MAE = []\n",
    "MedAE = []\n",
    "\n",
    "for i in range(5):\n",
    "    train_index, test_index  = train_indices[i], test_indices[i]\n",
    "    dtrain = xgb.DMatrix(train_X[train_index], label = train_Y[train_index])\n",
    "    dvalid = xgb.DMatrix(train_X[test_index])\n",
    "    \n",
    "    bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "    \n",
    "    y_valid_pred = bst.predict(dvalid)\n",
    "    MSE.append(np.sqrt(np.mean(abs(np.array([10**x for x in np.reshape(train_Y[test_index], (-1))]) - np.array([10**x for x in y_valid_pred]))**2)))\n",
    "    R2.append(r2_score([10**x for x in np.reshape(train_Y[test_index], (-1))], [10**x for x in y_valid_pred]))\n",
    "    Pearson.append(stats.pearsonr([10**x for x in np.reshape(train_Y[test_index], (-1))], [10**x for x in y_valid_pred])[0])\n",
    "    MAE.append(np.mean(abs(np.array([10**x for x in train_Y[test_index]]) - np.array([10**x for x in y_valid_pred]))))\n",
    "    MedAE.append(np.median(abs(np.array([10**x for x in train_Y[test_index]]) - np.array([10**x for x in y_valid_pred]))))\n",
    "\n",
    "print(Pearson)\n",
    "print(MSE)\n",
    "print(R2)\n",
    "print(MAE)\n",
    "print(MedAE)\n",
    "\n",
    "np.save(join(\"..\", \"..\", \"results\", split, \"Pearson_CV_xgboost_ESM2_diff_fp.npy\"), np.array(Pearson))\n",
    "np.save(join(\"..\", \"..\", \"results\", split, \"MSE_CV_xgboost_ESM2_diff_fp.npy\"), np.array(MSE))\n",
    "np.save(join(\"..\", \"..\", \"results\", split, \"R2_CV_xgboost_ESM2_diff_fp.npy\"), np.array(R2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(train_X, label = train_Y)\n",
    "dtest = xgb.DMatrix(test_X, label = test_Y)\n",
    "\n",
    "bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "\n",
    "y_test_pred = bst.predict(dtest)\n",
    "\n",
    "MSE_dif_fp_test = np.sqrt(np.mean(abs(np.array([10**x for x in test_Y]) - np.array([10**x for x in y_test_pred]))**2))\n",
    "R2_dif_fp_test = r2_score(np.array([10**x for x in test_Y]), np.array([10**x for x in y_test_pred]))\n",
    "Pearson = stats.pearsonr(np.array([10**x for x in test_Y]), np.array([10**x for x in y_test_pred]))\n",
    "MAE = np.mean(abs(np.array([10**x for x in test_Y]) - np.array([10**x for x in y_test_pred])))\n",
    "MedAE = np.median(abs(np.array([10**x for x in test_Y]) - np.array([10**x for x in y_test_pred])))\n",
    "\n",
    "print(np.round(Pearson[0],3) , np.round(MSE_dif_fp_test, 10), np.round(R2_dif_fp_test,3), np.round(MAE, 10), np.round(MedAE, 10))\n",
    "\n",
    "\n",
    "np.save(join(\"..\", \"..\", \"results\", split, \"y_test_pred_xgboost_ESM2_diff_fp.npy\"), bst.predict(dtest))\n",
    "np.save(join(\"..\", \"..\", \"results\", split,  \"y_test_true_xgboost_ESM2_diff_fp.npy\"), test_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (d) Training model with test and train data for production mode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = np.array(list(data_train[\"ESM2\"]))\n",
    "train_X = np.concatenate([train_X, np.array(list(data_train[\"difference_fp\"])), np.array(list(data_train[\"Temperature\"]))[:, np.newaxis], np.array(list(data_train[\"pH\"]))[:, np.newaxis]], axis = 1)\n",
    "train_Y = np.array(list(data_train[\"log10_Km\"]))\n",
    "\n",
    "test_X = np.array(list(data_test[\"ESM2\"]))\n",
    "test_X = np.concatenate([test_X, np.array(list(data_test[\"difference_fp\"])), np.array(list(data_test[\"Temperature\"]))[:, np.newaxis], np.array(list(data_test[\"pH\"]))[:, np.newaxis]], axis = 1)\n",
    "test_Y = np.array(list(data_test[\"log10_Km\"]))\n",
    "\n",
    "train_X = np.concatenate([train_X, test_X])\n",
    "train_Y = np.concatenate([train_Y, test_Y])\n",
    "\n",
    "train_X = train_X.astype(float)\n",
    "test_X = test_X.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(train_X, label = train_Y)\n",
    "dtest = xgb.DMatrix(test_X)\n",
    "\n",
    "bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "\n",
    "y_test_pred = bst.predict(dtest)\n",
    "\n",
    "MSE_dif_fp_test = np.sqrt(np.mean(abs(np.array([10**x for x in np.reshape(test_Y, (-1))]) - np.array([10**x for x in y_test_pred]))**2))\n",
    "R2_dif_fp_test = r2_score(np.array([10**x for x in np.reshape(test_Y, (-1))]), np.array([10**x for x in y_test_pred]))\n",
    "Pearson = stats.pearsonr(np.array([10**x for x in np.reshape(test_Y, (-1))]), np.array([10**x for x in y_test_pred]))\n",
    "\n",
    "print(Pearson, MSE_dif_fp_test, R2_dif_fp_test)\n",
    "\n",
    "pickle.dump(bst, open(join(\"..\", \"..\", \"results\", split, \"xgboost_ESM2_diff_fp.pkl\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Validating model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_X = np.array(list(data_val[\"ESM2\"]))\n",
    "val_X = np.concatenate([val_X, np.array(list(data_val[\"difference_fp\"])), np.array(list(data_val[\"Temperature\"]))[:, np.newaxis], np.array(list(data_val[\"pH\"]))[:, np.newaxis]], axis = 1)\n",
    "val_Y = np.array(list(data_val[\"log10_Km\"]))\n",
    "\n",
    "val_X = val_X.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dval = xgb.DMatrix(val_X)\n",
    "\n",
    "y_val_pred = bst.predict(dval)\n",
    "\n",
    "MSE_dif_fp_val = np.sqrt(np.mean(abs(np.array([10**x for x in np.reshape(val_Y, (-1))]) - np.array([10**x for x in y_val_pred]))**2))\n",
    "R2_dif_fp_val = r2_score(np.array([10**x for x in np.reshape(val_Y, (-1))]), np.array([10**x for x in y_val_pred]))\n",
    "Pearson = stats.pearsonr(np.array([10**x for x in np.reshape(val_Y, (-1))]), np.array([10**x for x in y_val_pred]))\n",
    "MAE = np.mean(abs(np.array([10**x for x in val_Y]) - np.array([10**x for x in y_val_pred])))\n",
    "MedAE = np.median(abs(np.array([10**x for x in val_Y]) - np.array([10**x for x in y_val_pred])))\n",
    "\n",
    "print(Pearson, MSE_dif_fp_val, R2_dif_fp_val, MAE, MedAE)\n",
    "np.save(join(\"..\", \"..\", \"results\", split, \"y_val_true_xgboost_esm2_diff_fp.npy\"), val_Y)\n",
    "np.save(join(\"..\", \"..\", \"results\", split, \"y_val_pred_xgboost_esm2_diff_fp.npy\"), y_val_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training a model with enzyme, main substrate (GNN fp) and reaction information (ESM-2/diff_fp) + Temperature + pH :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Creating input matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = np.array(list(data_train[\"ESM2\"]))\n",
    "train_X = np.concatenate([train_X, np.array(list(data_train[\"difference_fp\"])), np.array(list(data_train[\"GNN FP\"])), np.array(list(data_train[\"Temperature\"]))[:, np.newaxis], np.array(list(data_train[\"pH\"]))[:, np.newaxis]], axis = 1)\n",
    "train_Y = np.array(list(data_train[\"log10_Km\"]))\n",
    "\n",
    "test_X = np.array(list(data_test[\"ESM2\"]))\n",
    "test_X = np.concatenate([test_X, np.array(list(data_test[\"difference_fp\"])), np.array(list(data_test[\"GNN FP\"])), np.array(list(data_test[\"Temperature\"]))[:, np.newaxis], np.array(list(data_test[\"pH\"]))[:, np.newaxis]], axis = 1)\n",
    "test_Y = np.array(list(data_test[\"log10_Km\"]))\n",
    "\n",
    "train_X = train_X.astype(float)\n",
    "test_X = test_X.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Hyperparameter optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def cross_validation_mse_gradient_boosting(param):\n",
    "#     num_round = param[\"num_rounds\"]\n",
    "#     del param[\"num_rounds\"]\n",
    "#     param[\"max_depth\"] = int(np.round(param[\"max_depth\"]))\n",
    "#     # param[\"device\"] = \"cuda\"\n",
    "#     param[\"tree_method\"] = \"hist\"\n",
    "#     param[\"sampling_method\"] = \"gradient_based\"\n",
    "    \n",
    "#     MSE = []\n",
    "#     R2 = []\n",
    "#     for i in range(5):\n",
    "#         train_index, test_index  = train_indices[i], test_indices[i]\n",
    "#         dtrain = xgb.DMatrix(train_X[train_index], label = train_Y[train_index])\n",
    "#         dvalid = xgb.DMatrix(train_X[test_index])\n",
    "#         bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "#         y_valid_pred = bst.predict(dvalid)\n",
    "#         MSE.append(np.mean(abs(np.reshape(train_Y[test_index], (-1)) - y_valid_pred)**2))\n",
    "#         R2.append(r2_score(np.reshape(train_Y[test_index], (-1)),  y_valid_pred))\n",
    "#     return(-np.mean(R2))\n",
    "\n",
    "# space_gradient_boosting = {\n",
    "#     \"learning_rate\": hp.choice(\"learning_rate\", [0.01,0.1,0.2]),\n",
    "#     \"max_depth\": hp.quniform(\"max_depth\", 3, 10, 1),\n",
    "#     # \"subsample\": hp.quniform(\"subsample\", 0.5, 1, 0.5),\n",
    "#     \"reg_lambda\": hp.quniform(\"reg_lambda\", 0, 1, 0.2),\n",
    "#     \"reg_alpha\": hp.quniform(\"reg_alpha\", 0, 1, 0.2),\n",
    "#     \"max_delta_step\": hp.quniform(\"max_delta_step\", 1, 5, 1),\n",
    "#     \"min_child_weight\": hp.quniform(\"min_child_weight\", 1, 6, 1),\n",
    "#     \"num_rounds\":  hp.choice(\"num_rounds\", [100,250,500,1000])\n",
    "#     }\n",
    "\n",
    "# rstate = np.random.default_rng(42)\n",
    "# trials = Trials()\n",
    "# best = fmin(fn = cross_validation_mse_gradient_boosting, space = space_gradient_boosting,\n",
    "#             algo=rand.suggest, max_evals = 20, trials=trials, return_argmin=False, rstate=rstate)\n",
    "\n",
    "# print(best)\n",
    "# param = best\n",
    "# param[\"random_state\"] = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if split == \"full\":\n",
    "    param = {\"random_state\": 42, 'learning_rate': 0.01, 'max_delta_step': 5.0, 'max_depth': 5.0, 'min_child_weight': 3.0, 'num_rounds': 1000, 'reg_alpha': 0.6, 'reg_lambda': 0.8}\n",
    "elif split == \"Arabidopsis\":\n",
    "    param = {\"random_state\": 42, 'learning_rate': 0.01, 'max_delta_step': 5.0, 'max_depth': 5.0, 'min_child_weight': 3.0, 'num_rounds': 1000, 'reg_alpha': 0.6, 'reg_lambda': 0.8}\n",
    "elif split == \"Brassicaceae\":\n",
    "    param = {\"random_state\": 42, 'learning_rate': 0.1, 'max_delta_step': 3.0, 'max_depth': 4.0, 'min_child_weight': 5.0, 'num_rounds': 500, 'reg_alpha': 0.2, 'reg_lambda': 0.4}\n",
    "elif split == \"wildtype\":\n",
    "    param = {\"random_state\": 42, 'learning_rate': 0.1, 'max_delta_step': 2.0, 'max_depth': 4.0, 'min_child_weight': 6.0, 'num_rounds': 1000, 'reg_alpha': 0.2, 'reg_lambda': 0.4}\n",
    "else:\n",
    "    param = {\"random_state\": 42, 'learning_rate': 0.2, 'max_delta_step': 1.0, 'max_depth': 7.0, 'min_child_weight': 1.0, 'num_rounds': 1000, 'reg_alpha': 0.6, 'reg_lambda': 0.4}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c) Training and validating model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_round = param[\"num_rounds\"]\n",
    "param[\"max_depth\"] = int(np.round(param[\"max_depth\"]))\n",
    "param[\"tree_method\"] = \"gpu_hist\"\n",
    "param[\"sampling_method\"] = \"gradient_based\"\n",
    "del param[\"num_rounds\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R2 = []\n",
    "MSE = []\n",
    "Pearson = []\n",
    "MAE = []\n",
    "MedAE = []\n",
    "\n",
    "for i in range(5):\n",
    "    train_index, test_index  = train_indices[i], test_indices[i]\n",
    "    dtrain = xgb.DMatrix(train_X[train_index], label = train_Y[train_index])\n",
    "    dvalid = xgb.DMatrix(train_X[test_index])\n",
    "    \n",
    "    bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "    \n",
    "    y_valid_pred = bst.predict(dvalid)\n",
    "    MSE.append(np.sqrt(np.mean(abs(np.array([10**x for x in np.reshape(train_Y[test_index], (-1))]) - np.array([10**x for x in y_valid_pred]))**2)))\n",
    "    R2.append(r2_score([10**x for x in np.reshape(train_Y[test_index], (-1))], [10**x for x in y_valid_pred]))\n",
    "    Pearson.append(stats.pearsonr([10**x for x in np.reshape(train_Y[test_index], (-1))], [10**x for x in y_valid_pred])[0])\n",
    "    MAE.append(np.mean(abs(np.array([10**x for x in train_Y[test_index]]) - np.array([10**x for x in y_valid_pred]))))\n",
    "    MedAE.append(np.median(abs(np.array([10**x for x in train_Y[test_index]]) - np.array([10**x for x in y_valid_pred]))))\n",
    "\n",
    "print(Pearson)\n",
    "print(MSE)\n",
    "print(R2)\n",
    "print(MAE)\n",
    "print(MedAE)\n",
    "\n",
    "np.save(join(\"..\", \"..\", \"results\", split, \"Pearson_CV_xgboost_ESM2_gnn_fp_diff_fp.npy\"), np.array(Pearson))\n",
    "np.save(join(\"..\", \"..\", \"results\", split, \"MSE_CV_xgboost_ESM2_gnn_fp_diff_fp.npy\"), np.array(MSE))\n",
    "np.save(join(\"..\", \"..\", \"results\", split, \"R2_CV_xgboost_ESM2_gnn_fp_diff_fp.npy\"), np.array(R2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(train_X, label = train_Y)\n",
    "dtest = xgb.DMatrix(test_X, label = test_Y)\n",
    "\n",
    "bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "\n",
    "y_test_pred = bst.predict(dtest)\n",
    "\n",
    "MSE_dif_fp_test = np.sqrt(np.mean(abs(np.array([10**x for x in test_Y]) - np.array([10**x for x in y_test_pred]))**2))\n",
    "R2_dif_fp_test = r2_score(np.array([10**x for x in test_Y]), np.array([10**x for x in y_test_pred]))\n",
    "Pearson = stats.pearsonr(np.array([10**x for x in test_Y]), np.array([10**x for x in y_test_pred]))\n",
    "MAE = np.mean(abs(np.array([10**x for x in test_Y]) - np.array([10**x for x in y_test_pred])))\n",
    "MedAE = np.median(abs(np.array([10**x for x in test_Y]) - np.array([10**x for x in y_test_pred])))\n",
    "\n",
    "print(np.round(Pearson[0],3) , np.round(MSE_dif_fp_test, 10), np.round(R2_dif_fp_test,3), np.round(MAE, 10), np.round(MedAE, 10))\n",
    "\n",
    "np.save(join(\"..\", \"..\", \"results\", split, \"y_test_pred_xgboost_ESM2_gnn_fp_diff_fp.npy\"), bst.predict(dtest))\n",
    "np.save(join(\"..\", \"..\", \"results\", split,  \"y_test_true_xgboost_ESM2_gnn_fp_diff_fp.npy\"), test_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (d) Training model with test and train data for production mode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = np.array(list(data_train[\"ESM2\"]))\n",
    "train_X = np.concatenate([train_X, np.array(list(data_train[\"difference_fp\"])), np.array(list(data_train[\"GNN FP\"])), np.array(list(data_train[\"Temperature\"]))[:, np.newaxis], np.array(list(data_train[\"pH\"]))[:, np.newaxis]], axis = 1)\n",
    "train_Y = np.array(list(data_train[\"log10_Km\"]))\n",
    "\n",
    "test_X = np.array(list(data_test[\"ESM2\"]))\n",
    "test_X = np.concatenate([test_X, np.array(list(data_test[\"difference_fp\"])), np.array(list(data_test[\"GNN FP\"])), np.array(list(data_test[\"Temperature\"]))[:, np.newaxis], np.array(list(data_test[\"pH\"]))[:, np.newaxis]], axis = 1)\n",
    "test_Y = np.array(list(data_test[\"log10_Km\"]))\n",
    "\n",
    "train_X = np.concatenate([train_X, test_X])\n",
    "train_Y = np.concatenate([train_Y, test_Y])\n",
    "\n",
    "train_X = train_X.astype(float)\n",
    "test_X = test_X.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(train_X, label = train_Y)\n",
    "dtest = xgb.DMatrix(test_X)\n",
    "\n",
    "bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "\n",
    "y_test_pred = bst.predict(dtest)\n",
    "\n",
    "MSE_dif_fp_test = np.sqrt(np.mean(abs(np.array([10**x for x in np.reshape(test_Y, (-1))]) - np.array([10**x for x in y_test_pred]))**2))\n",
    "R2_dif_fp_test = r2_score(np.array([10**x for x in np.reshape(test_Y, (-1))]), np.array([10**x for x in y_test_pred]))\n",
    "Pearson = stats.pearsonr(np.array([10**x for x in np.reshape(test_Y, (-1))]), np.array([10**x for x in y_test_pred]))\n",
    "\n",
    "print(Pearson, MSE_dif_fp_test, R2_dif_fp_test)\n",
    "\n",
    "pickle.dump(bst, open(join(\"..\", \"..\", \"results\", split, \"xgboost_ESM2_gnn_fp_diff_fp.pkl\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Validating model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_X = np.array(list(data_val[\"ESM2\"]))\n",
    "val_X = np.concatenate([val_X, np.array(list(data_val[\"difference_fp\"])), np.array(list(data_val[\"GNN FP\"])), np.array(list(data_val[\"Temperature\"]))[:, np.newaxis], np.array(list(data_val[\"pH\"]))[:, np.newaxis]], axis = 1)\n",
    "val_Y = np.array(list(data_val[\"log10_Km\"]))\n",
    "\n",
    "val_X = val_X.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dval = xgb.DMatrix(val_X)\n",
    "\n",
    "y_val_pred = bst.predict(dval)\n",
    "\n",
    "MSE_dif_fp_val = np.sqrt(np.mean(abs(np.array([10**x for x in np.reshape(val_Y, (-1))]) - np.array([10**x for x in y_val_pred]))**2))\n",
    "R2_dif_fp_val = r2_score(np.array([10**x for x in np.reshape(val_Y, (-1))]), np.array([10**x for x in y_val_pred]))\n",
    "Pearson = stats.pearsonr(np.array([10**x for x in np.reshape(val_Y, (-1))]), np.array([10**x for x in y_val_pred]))\n",
    "MAE = np.mean(abs(np.array([10**x for x in val_Y]) - np.array([10**x for x in y_val_pred])))\n",
    "MedAE = np.median(abs(np.array([10**x for x in val_Y]) - np.array([10**x for x in y_val_pred])))\n",
    "\n",
    "print(Pearson, MSE_dif_fp_val, R2_dif_fp_val, MAE, MedAE)\n",
    "\n",
    "np.save(join(\"..\", \"..\", \"results\", split, \"y_val_true_xgboost_esm2_gnn_fp_diff_fp.npy\"), val_Y)\n",
    "np.save(join(\"..\", \"..\", \"results\", split, \"y_val_pred_xgboost_esm2_gnn_fp_diff_fp.npy\"), y_val_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training a model with enzyme information (ESM-2) + Temperature + pH:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Creating input matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = np.array(list(data_train[\"ESM2\"]))\n",
    "train_X = np.concatenate([train_X, np.array(list(data_train[\"Temperature\"]))[:, np.newaxis], np.array(list(data_train[\"pH\"]))[:, np.newaxis]], axis = 1)\n",
    "train_Y = np.array(list(data_train[\"log10_Km\"]))\n",
    "\n",
    "test_X = np.array(list(data_test[\"ESM2\"]))\n",
    "test_X = np.concatenate([test_X, np.array(list(data_test[\"Temperature\"]))[:, np.newaxis], np.array(list(data_test[\"pH\"]))[:, np.newaxis]], axis = 1)\n",
    "test_Y = np.array(list(data_test[\"log10_Km\"]))\n",
    "\n",
    "train_X = train_X.astype(float)\n",
    "test_X = test_X.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Hyperparameter optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def cross_validation_mse_gradient_boosting(param):\n",
    "#     num_round = param[\"num_rounds\"]\n",
    "#     del param[\"num_rounds\"]\n",
    "#     param[\"max_depth\"] = int(np.round(param[\"max_depth\"]))\n",
    "#     # param[\"device\"] = \"cuda\"\n",
    "#     param[\"tree_method\"] = \"hist\"\n",
    "#     param[\"sampling_method\"] = \"gradient_based\"\n",
    "    \n",
    "#     MSE = []\n",
    "#     R2 = []\n",
    "#     for i in range(5):\n",
    "#         train_index, test_index  = train_indices[i], test_indices[i]\n",
    "#         dtrain = xgb.DMatrix(train_X[train_index], label = train_Y[train_index])\n",
    "#         dvalid = xgb.DMatrix(train_X[test_index])\n",
    "#         bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "#         y_valid_pred = bst.predict(dvalid)\n",
    "#         MSE.append(np.mean(abs(np.reshape(train_Y[test_index], (-1)) - y_valid_pred)**2))\n",
    "#         R2.append(r2_score(np.reshape(train_Y[test_index], (-1)),  y_valid_pred))\n",
    "#     return(-np.mean(R2))\n",
    "\n",
    "# space_gradient_boosting = {\n",
    "#     \"learning_rate\": hp.choice(\"learning_rate\", [0.01,0.1,0.2]),\n",
    "#     \"max_depth\": hp.quniform(\"max_depth\", 3, 10, 1),\n",
    "#     # \"subsample\": hp.quniform(\"subsample\", 0.5, 1, 0.5),\n",
    "#     \"reg_lambda\": hp.quniform(\"reg_lambda\", 0, 1, 0.2),\n",
    "#     \"reg_alpha\": hp.quniform(\"reg_alpha\", 0, 1, 0.2),\n",
    "#     \"max_delta_step\": hp.quniform(\"max_delta_step\", 1, 5, 1),\n",
    "#     \"min_child_weight\": hp.quniform(\"min_child_weight\", 1, 6, 1),\n",
    "#     \"num_rounds\":  hp.choice(\"num_rounds\", [100,250,500,1000])\n",
    "#     }\n",
    "\n",
    "# rstate = np.random.default_rng(42)\n",
    "# trials = Trials()\n",
    "# best = fmin(fn = cross_validation_mse_gradient_boosting, space = space_gradient_boosting,\n",
    "#             algo=rand.suggest, max_evals = 20, trials=trials, return_argmin=False, rstate=rstate)\n",
    "\n",
    "# print(best)\n",
    "# param = best\n",
    "# param[\"random_state\"] = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if split == \"full\":\n",
    "    param = {\"random_state\": 42, 'learning_rate': 0.01, 'max_delta_step': 5.0, 'max_depth': 5.0, 'min_child_weight': 3.0, 'num_rounds': 1000, 'reg_alpha': 0.6, 'reg_lambda': 0.8}\n",
    "elif split == \"Arabidopsis\":\n",
    "    param = {\"random_state\": 42, 'learning_rate': 0.1, 'max_delta_step': 2.0, 'max_depth': 4.0, 'min_child_weight': 6.0, 'num_rounds': 1000, 'reg_alpha': 0.2, 'reg_lambda': 0.4}\n",
    "elif split == \"Brassicaceae\":\n",
    "    param = {\"random_state\": 42, 'learning_rate': 0.1, 'max_delta_step': 2.0, 'max_depth': 4.0, 'min_child_weight': 6.0, 'num_rounds': 1000, 'reg_alpha': 0.2, 'reg_lambda': 0.4}\n",
    "elif split == \"wildtype\":\n",
    "    param = {\"random_state\": 42, 'learning_rate': 0.1, 'max_delta_step': 2.0, 'max_depth': 4.0, 'min_child_weight': 6.0, 'num_rounds': 1000, 'reg_alpha': 0.2, 'reg_lambda': 0.4}\n",
    "else:\n",
    "    param = {\"random_state\": 42, 'learning_rate': 0.1, 'max_delta_step': 3.0, 'max_depth': 9.0, 'min_child_weight': 2.0, 'num_rounds': 100, 'reg_alpha': 0.8, 'reg_lambda': 0.2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c) Training and validating model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_round = param[\"num_rounds\"]\n",
    "param[\"max_depth\"] = int(np.round(param[\"max_depth\"]))\n",
    "param[\"tree_method\"] = \"gpu_hist\"\n",
    "param[\"sampling_method\"] = \"gradient_based\"\n",
    "del param[\"num_rounds\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R2 = []\n",
    "MSE = []\n",
    "Pearson = []\n",
    "MAE = []\n",
    "MedAE = []\n",
    "\n",
    "for i in range(5):\n",
    "    train_index, test_index  = train_indices[i], test_indices[i]\n",
    "    dtrain = xgb.DMatrix(train_X[train_index], label = train_Y[train_index])\n",
    "    dvalid = xgb.DMatrix(train_X[test_index])\n",
    "    \n",
    "    bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "    \n",
    "    y_valid_pred = bst.predict(dvalid)\n",
    "    MSE.append(np.sqrt(np.mean(abs(np.array([10**x for x in np.reshape(train_Y[test_index], (-1))]) - np.array([10**x for x in y_valid_pred]))**2)))\n",
    "    R2.append(r2_score([10**x for x in np.reshape(train_Y[test_index], (-1))], [10**x for x in y_valid_pred]))\n",
    "    Pearson.append(stats.pearsonr([10**x for x in np.reshape(train_Y[test_index], (-1))], [10**x for x in y_valid_pred])[0])\n",
    "    MAE.append(np.mean(abs(np.array([10**x for x in train_Y[test_index]]) - np.array([10**x for x in y_valid_pred]))))\n",
    "    MedAE.append(np.median(abs(np.array([10**x for x in train_Y[test_index]]) - np.array([10**x for x in y_valid_pred]))))\n",
    "\n",
    "print(Pearson)\n",
    "print(MSE)\n",
    "print(R2)\n",
    "print(MAE)\n",
    "print(MedAE)\n",
    "\n",
    "np.save(join(\"..\", \"..\", \"results\", split, \"Pearson_CV_xgboost_ESM2.npy\"), np.array(Pearson))\n",
    "np.save(join(\"..\", \"..\", \"results\", split, \"MSE_CV_xgboost_ESM2.npy\"), np.array(MSE))\n",
    "np.save(join(\"..\", \"..\", \"results\", split, \"R2_CV_xgboost_ESM2.npy\"), np.array(R2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(train_X, label = train_Y)\n",
    "dtest = xgb.DMatrix(test_X, label = test_Y)\n",
    "\n",
    "bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "\n",
    "y_test_pred = bst.predict(dtest)\n",
    "\n",
    "MSE_dif_fp_test = np.sqrt(np.mean(abs(np.array([10**x for x in test_Y]) - np.array([10**x for x in y_test_pred]))**2))\n",
    "R2_dif_fp_test = r2_score(np.array([10**x for x in test_Y]), np.array([10**x for x in y_test_pred]))\n",
    "Pearson = stats.pearsonr(np.array([10**x for x in test_Y]), np.array([10**x for x in y_test_pred]))\n",
    "MAE = np.mean(abs(np.array([10**x for x in test_Y]) - np.array([10**x for x in y_test_pred])))\n",
    "MedAE = np.median(abs(np.array([10**x for x in test_Y]) - np.array([10**x for x in y_test_pred])))\n",
    "\n",
    "print(np.round(Pearson[0],3) , np.round(MSE_dif_fp_test, 10), np.round(R2_dif_fp_test,3), np.round(MAE, 10), np.round(MedAE, 10))\n",
    "\n",
    "np.save(join(\"..\", \"..\", \"results\", split, \"y_test_pred_xgboost_ESM2.npy\"), bst.predict(dtest))\n",
    "np.save(join(\"..\", \"..\", \"results\", split,  \"y_test_true_xgboost_ESM2.npy\"), test_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (d) Training model with test and train data for production mode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = np.array(list(data_train[\"ESM2\"]))\n",
    "train_X = np.concatenate([train_X, np.array(list(data_train[\"Temperature\"]))[:, np.newaxis], np.array(list(data_train[\"pH\"]))[:, np.newaxis]], axis = 1)\n",
    "train_Y = np.array(list(data_train[\"log10_Km\"]))\n",
    "\n",
    "test_X = np.array(list(data_test[\"ESM2\"]))\n",
    "test_X = np.concatenate([test_X, np.array(list(data_test[\"Temperature\"]))[:, np.newaxis], np.array(list(data_test[\"pH\"]))[:, np.newaxis]], axis = 1)\n",
    "test_Y = np.array(list(data_test[\"log10_Km\"]))\n",
    "\n",
    "train_X = np.concatenate([train_X, test_X])\n",
    "train_Y = np.concatenate([train_Y, test_Y])\n",
    "\n",
    "train_X = train_X.astype(float)\n",
    "test_X = test_X.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(train_X, label = train_Y)\n",
    "dtest = xgb.DMatrix(test_X)\n",
    "\n",
    "bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "\n",
    "y_test_pred = bst.predict(dtest)\n",
    "\n",
    "MSE_dif_fp_test = np.sqrt(np.mean(abs(np.array([10**x for x in np.reshape(test_Y, (-1))]) - np.array([10**x for x in y_test_pred]))**2))\n",
    "R2_dif_fp_test = r2_score(np.array([10**x for x in np.reshape(test_Y, (-1))]), np.array([10**x for x in y_test_pred]))\n",
    "Pearson = stats.pearsonr(np.array([10**x for x in np.reshape(test_Y, (-1))]), np.array([10**x for x in y_test_pred]))\n",
    "\n",
    "print(Pearson, MSE_dif_fp_test, R2_dif_fp_test)\n",
    "\n",
    "pickle.dump(bst, open(join(\"..\", \"..\", \"results\", split, \"xgboost_ESM2.pkl\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (e) Validating model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_X = np.array(list(data_val[\"ESM2\"]))\n",
    "val_X = np.concatenate([val_X, np.array(list(data_val[\"Temperature\"]))[:, np.newaxis], np.array(list(data_val[\"pH\"]))[:, np.newaxis]], axis = 1)\n",
    "val_Y = np.array(list(data_val[\"log10_Km\"]))\n",
    "\n",
    "val_X = val_X.astype(float)\n",
    "\n",
    "dval = xgb.DMatrix(val_X)\n",
    "\n",
    "y_val_pred = bst.predict(dval)\n",
    "\n",
    "MSE_dif_fp_val = np.sqrt(np.mean(abs(np.array([10**x for x in np.reshape(val_Y, (-1))]) - np.array([10**x for x in y_val_pred]))**2))\n",
    "R2_dif_fp_val = r2_score(np.array([10**x for x in np.reshape(val_Y, (-1))]), np.array([10**x for x in y_val_pred]))\n",
    "Pearson = stats.pearsonr(np.array([10**x for x in np.reshape(val_Y, (-1))]), np.array([10**x for x in y_val_pred]))\n",
    "MAE = np.mean(abs(np.array([10**x for x in val_Y]) - np.array([10**x for x in y_val_pred])))\n",
    "MedAE = np.median(abs(np.array([10**x for x in val_Y]) - np.array([10**x for x in y_val_pred])))\n",
    "\n",
    "print(Pearson, MSE_dif_fp_val, R2_dif_fp_val, MAE, MedAE)\n",
    "\n",
    "np.save(join(\"..\", \"..\", \"results\", split, \"y_val_true_xgboost_esm2.npy\"), val_Y)\n",
    "np.save(join(\"..\", \"..\", \"results\", split, \"y_val_pred_xgboost_esm2.npy\"), y_val_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training a model with main substrate information (GNN) + Temperature + pH:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Creating input matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = np.array(list(data_train['GNN FP']))\n",
    "train_X = np.concatenate([train_X, np.array(list(data_train[\"Temperature\"]))[:, np.newaxis], np.array(list(data_train[\"pH\"]))[:, np.newaxis]], axis = 1)\n",
    "train_Y = np.array(list(data_train[\"log10_Km\"]))\n",
    "\n",
    "test_X = np.array(list(data_test['GNN FP']))\n",
    "test_X = np.concatenate([test_X, np.array(list(data_test[\"Temperature\"]))[:, np.newaxis], np.array(list(data_test[\"pH\"]))[:, np.newaxis]], axis = 1)\n",
    "test_Y = np.array(list(data_test[\"log10_Km\"]))\n",
    "\n",
    "train_X = train_X.astype(float)\n",
    "test_X = test_X.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Hyperparameter optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def cross_validation_mse_gradient_boosting(param):\n",
    "#     num_round = param[\"num_rounds\"]\n",
    "#     del param[\"num_rounds\"]\n",
    "#     param[\"max_depth\"] = int(np.round(param[\"max_depth\"]))\n",
    "#     # param[\"device\"] = \"cuda\"\n",
    "#     param[\"tree_method\"] = \"hist\"\n",
    "#     param[\"sampling_method\"] = \"gradient_based\"\n",
    "    \n",
    "#     MSE = []\n",
    "#     R2 = []\n",
    "#     for i in range(5):\n",
    "#         train_index, test_index  = train_indices[i], test_indices[i]\n",
    "#         dtrain = xgb.DMatrix(train_X[train_index], label = train_Y[train_index])\n",
    "#         dvalid = xgb.DMatrix(train_X[test_index])\n",
    "#         bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "#         y_valid_pred = bst.predict(dvalid)\n",
    "#         MSE.append(np.mean(abs(np.reshape(train_Y[test_index], (-1)) - y_valid_pred)**2))\n",
    "#         R2.append(r2_score(np.reshape(train_Y[test_index], (-1)),  y_valid_pred))\n",
    "#     return(-np.mean(R2))\n",
    "\n",
    "# space_gradient_boosting = {\n",
    "#     \"learning_rate\": hp.choice(\"learning_rate\", [0.01,0.1,0.2]),\n",
    "#     \"max_depth\": hp.quniform(\"max_depth\", 3, 10, 1),\n",
    "#     # \"subsample\": hp.quniform(\"subsample\", 0.5, 1, 0.5),\n",
    "#     \"reg_lambda\": hp.quniform(\"reg_lambda\", 0, 1, 0.2),\n",
    "#     \"reg_alpha\": hp.quniform(\"reg_alpha\", 0, 1, 0.2),\n",
    "#     \"max_delta_step\": hp.quniform(\"max_delta_step\", 1, 5, 1),\n",
    "#     \"min_child_weight\": hp.quniform(\"min_child_weight\", 1, 6, 1),\n",
    "#     \"num_rounds\":  hp.choice(\"num_rounds\", [100,250,500,1000])\n",
    "#     }\n",
    "\n",
    "# rstate = np.random.default_rng(42)\n",
    "# trials = Trials()\n",
    "# best = fmin(fn = cross_validation_mse_gradient_boosting, space = space_gradient_boosting,\n",
    "#             algo=rand.suggest, max_evals = 20, trials=trials, return_argmin=False, rstate=rstate)\n",
    "\n",
    "# print(best)\n",
    "# param = best\n",
    "# param[\"random_state\"] = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if split == \"full\":\n",
    "    param = {\"random_state\": 42, 'learning_rate': 0.01, 'max_delta_step': 5.0, 'max_depth': 5.0, 'min_child_weight': 3.0, 'num_rounds': 1000, 'reg_alpha': 0.6, 'reg_lambda': 0.8}\n",
    "elif split == \"Arabidopsis\":\n",
    "    param = {\"random_state\": 42, 'learning_rate': 0.2, 'max_delta_step': 4.0, 'max_depth': 4.0, 'min_child_weight': 3.0, 'num_rounds': 100, 'reg_alpha': 0.8, 'reg_lambda': 0.0}\n",
    "elif split == \"Brassicaceae\":\n",
    "    param = {\"random_state\": 42, 'learning_rate': 0.01, 'max_delta_step': 4.0, 'max_depth': 5.0, 'min_child_weight': 3.0, 'num_rounds': 250, 'reg_alpha': 0.8, 'reg_lambda': 0.2}\n",
    "elif split == \"wildtype\":\n",
    "    param = {\"random_state\": 42, 'learning_rate': 0.01, 'max_delta_step': 5.0, 'max_depth': 5.0, 'min_child_weight': 3.0, 'num_rounds': 1000, 'reg_alpha': 0.6, 'reg_lambda': 0.8}\n",
    "else:\n",
    "    param = {\"random_state\": 42, 'learning_rate': 0.1, 'max_delta_step': 3.0, 'max_depth': 9.0, 'min_child_weight': 2.0, 'num_rounds': 100, 'reg_alpha': 0.8, 'reg_lambda': 0.2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c) Training and validating model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_round = param[\"num_rounds\"]\n",
    "param[\"max_depth\"] = int(np.round(param[\"max_depth\"]))\n",
    "param[\"tree_method\"] = \"gpu_hist\"\n",
    "param[\"sampling_method\"] = \"gradient_based\"\n",
    "del param[\"num_rounds\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R2 = []\n",
    "MSE = []\n",
    "Pearson = []\n",
    "MAE = []\n",
    "MedAE = []\n",
    "\n",
    "for i in range(5):\n",
    "    train_index, test_index  = train_indices[i], test_indices[i]\n",
    "    dtrain = xgb.DMatrix(train_X[train_index], label = train_Y[train_index])\n",
    "    dvalid = xgb.DMatrix(train_X[test_index])\n",
    "    \n",
    "    bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "    \n",
    "    y_valid_pred = bst.predict(dvalid)\n",
    "    MSE.append(np.sqrt(np.mean(abs(np.array([10**x for x in np.reshape(train_Y[test_index], (-1))]) - np.array([10**x for x in y_valid_pred]))**2)))\n",
    "    R2.append(r2_score([10**x for x in np.reshape(train_Y[test_index], (-1))], [10**x for x in y_valid_pred]))\n",
    "    Pearson.append(stats.pearsonr([10**x for x in np.reshape(train_Y[test_index], (-1))], [10**x for x in y_valid_pred])[0])\n",
    "    MAE.append(np.mean(abs(np.array([10**x for x in train_Y[test_index]]) - np.array([10**x for x in y_valid_pred]))))\n",
    "    MedAE.append(np.median(abs(np.array([10**x for x in train_Y[test_index]]) - np.array([10**x for x in y_valid_pred]))))\n",
    "\n",
    "print(Pearson)\n",
    "print(MSE)\n",
    "print(R2)\n",
    "print(MAE)\n",
    "print(MedAE)\n",
    "\n",
    "np.save(join(\"..\", \"..\", \"results\", split, \"Pearson_CV_xgboost_gnn_fp.npy\"), np.array(Pearson))\n",
    "np.save(join(\"..\", \"..\", \"results\", split, \"MSE_CV_xgboost_gnn_fp.npy\"), np.array(MSE))\n",
    "np.save(join(\"..\", \"..\", \"results\", split, \"R2_CV_xgboost_gnn_fp.npy\"), np.array(R2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(train_X, label = train_Y)\n",
    "dtest = xgb.DMatrix(test_X, label = test_Y)\n",
    "\n",
    "bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "\n",
    "y_test_pred = bst.predict(dtest)\n",
    "\n",
    "MSE_dif_fp_test = np.sqrt(np.mean(abs(np.array([10**x for x in test_Y]) - np.array([10**x for x in y_test_pred]))**2))\n",
    "R2_dif_fp_test = r2_score(np.array([10**x for x in test_Y]), np.array([10**x for x in y_test_pred]))\n",
    "Pearson = stats.pearsonr(np.array([10**x for x in test_Y]), np.array([10**x for x in y_test_pred]))\n",
    "MAE = np.mean(abs(np.array([10**x for x in test_Y]) - np.array([10**x for x in y_test_pred])))\n",
    "MedAE = np.median(abs(np.array([10**x for x in test_Y]) - np.array([10**x for x in y_test_pred])))\n",
    "\n",
    "print(np.round(Pearson[0],3) , np.round(MSE_dif_fp_test, 10), np.round(R2_dif_fp_test,3), np.round(MAE, 10), np.round(MedAE, 10))\n",
    "\n",
    "np.save(join(\"..\", \"..\", \"results\", split, \"y_test_pred_xgboost_gnn_fp.npy\"), bst.predict(dtest))\n",
    "np.save(join(\"..\", \"..\", \"results\", split,  \"y_test_true_xgboost_gnn_fp.npy\"), test_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (d) Training model with test and train data for production mode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = np.array(list(data_train[\"GNN FP\"]))\n",
    "train_X = np.concatenate([train_X, np.array(list(data_train[\"Temperature\"]))[:, np.newaxis], np.array(list(data_train[\"pH\"]))[:, np.newaxis]], axis = 1)\n",
    "train_Y = np.array(list(data_train[\"log10_Km\"]))\n",
    "\n",
    "test_X = np.array(list(data_test[\"GNN FP\"]))\n",
    "test_X = np.concatenate([test_X, np.array(list(data_test[\"Temperature\"]))[:, np.newaxis], np.array(list(data_test[\"pH\"]))[:, np.newaxis]], axis = 1)\n",
    "test_Y = np.array(list(data_test[\"log10_Km\"]))\n",
    "\n",
    "train_X = np.concatenate([train_X, test_X])\n",
    "train_Y = np.concatenate([train_Y, test_Y])\n",
    "\n",
    "train_X = train_X.astype(float)\n",
    "test_X = test_X.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(train_X, label = train_Y)\n",
    "dtest = xgb.DMatrix(test_X)\n",
    "\n",
    "bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "\n",
    "y_test_pred = bst.predict(dtest)\n",
    "\n",
    "MSE_dif_fp_test = np.sqrt(np.mean(abs(np.array([10**x for x in np.reshape(test_Y, (-1))]) - np.array([10**x for x in y_test_pred]))**2))\n",
    "R2_dif_fp_test = r2_score(np.array([10**x for x in np.reshape(test_Y, (-1))]), np.array([10**x for x in y_test_pred]))\n",
    "Pearson = stats.pearsonr(np.array([10**x for x in np.reshape(test_Y, (-1))]), np.array([10**x for x in y_test_pred]))\n",
    "\n",
    "print(Pearson, MSE_dif_fp_test, R2_dif_fp_test)\n",
    "\n",
    "pickle.dump(bst, open(join(\"..\", \"..\", \"results\", split, \"xgboost_gnn_fp.pkl\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (e) Validating model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_X = np.array(list(data_val[\"GNN FP\"]))\n",
    "val_X = np.concatenate([val_X, np.array(list(data_val[\"Temperature\"]))[:, np.newaxis], np.array(list(data_val[\"pH\"]))[:, np.newaxis]], axis = 1)\n",
    "val_Y = np.array(list(data_val[\"log10_Km\"]))\n",
    "\n",
    "val_X = val_X.astype(float)\n",
    "\n",
    "dval = xgb.DMatrix(val_X)\n",
    "\n",
    "y_val_pred = bst.predict(dval)\n",
    "\n",
    "MSE_dif_fp_val = np.sqrt(np.mean(abs(np.array([10**x for x in np.reshape(val_Y, (-1))]) - np.array([10**x for x in y_val_pred]))**2))\n",
    "R2_dif_fp_val = r2_score(np.array([10**x for x in np.reshape(val_Y, (-1))]), np.array([10**x for x in y_val_pred]))\n",
    "Pearson = stats.pearsonr(np.array([10**x for x in np.reshape(val_Y, (-1))]), np.array([10**x for x in y_val_pred]))\n",
    "MAE = np.mean(abs(np.array([10**x for x in val_Y]) - np.array([10**x for x in y_val_pred])))\n",
    "MedAE = np.median(abs(np.array([10**x for x in val_Y]) - np.array([10**x for x in y_val_pred])))\n",
    "\n",
    "print(Pearson, MSE_dif_fp_val, R2_dif_fp_val, MAE,MedAE)\n",
    "\n",
    "np.save(join(\"..\", \"..\", \"results\", split, \"y_val_true_xgboost_gnn_fp.npy\"), val_Y)\n",
    "np.save(join(\"..\", \"..\", \"results\", split, \"y_val_pred_xgboost_gnn_fp.npy\"), y_val_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training a model with reaction information (diff-fp) + Temperature + pH:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Creating input matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = np.array(list(data_train[\"difference_fp\"]))\n",
    "train_X = np.concatenate([train_X, np.array(list(data_train[\"Temperature\"]))[:, np.newaxis], np.array(list(data_train[\"pH\"]))[:, np.newaxis]], axis = 1)\n",
    "train_Y = np.array(list(data_train[\"log10_Km\"]))\n",
    "\n",
    "test_X = np.array(list(data_test[\"difference_fp\"]))\n",
    "test_X = np.concatenate([test_X, np.array(list(data_test[\"Temperature\"]))[:, np.newaxis], np.array(list(data_test[\"pH\"]))[:, np.newaxis]], axis = 1)\n",
    "test_Y = np.array(list(data_test[\"log10_Km\"]))\n",
    "\n",
    "train_X = train_X.astype(float)\n",
    "test_X = test_X.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Hyperparameter optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def cross_validation_mse_gradient_boosting(param):\n",
    "#     num_round = param[\"num_rounds\"]\n",
    "#     del param[\"num_rounds\"]\n",
    "#     param[\"max_depth\"] = int(np.round(param[\"max_depth\"]))\n",
    "#     # param[\"device\"] = \"cuda\"\n",
    "#     param[\"tree_method\"] = \"hist\"\n",
    "#     param[\"sampling_method\"] = \"gradient_based\"\n",
    "    \n",
    "#     MSE = []\n",
    "#     R2 = []\n",
    "#     for i in range(5):\n",
    "#         train_index, test_index  = train_indices[i], test_indices[i]\n",
    "#         dtrain = xgb.DMatrix(train_X[train_index], label = train_Y[train_index])\n",
    "#         dvalid = xgb.DMatrix(train_X[test_index])\n",
    "#         bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "#         y_valid_pred = bst.predict(dvalid)\n",
    "#         MSE.append(np.mean(abs(np.reshape(train_Y[test_index], (-1)) - y_valid_pred)**2))\n",
    "#         R2.append(r2_score(np.reshape(train_Y[test_index], (-1)),  y_valid_pred))\n",
    "#     return(-np.mean(R2))\n",
    "\n",
    "# space_gradient_boosting = {\n",
    "#     \"learning_rate\": hp.choice(\"learning_rate\", [0.01,0.1,0.2]),\n",
    "#     \"max_depth\": hp.quniform(\"max_depth\", 3, 10, 1),\n",
    "#     # \"subsample\": hp.quniform(\"subsample\", 0.5, 1, 0.5),\n",
    "#     \"reg_lambda\": hp.quniform(\"reg_lambda\", 0, 1, 0.2),\n",
    "#     \"reg_alpha\": hp.quniform(\"reg_alpha\", 0, 1, 0.2),\n",
    "#     \"max_delta_step\": hp.quniform(\"max_delta_step\", 1, 5, 1),\n",
    "#     \"min_child_weight\": hp.quniform(\"min_child_weight\", 1, 6, 1),\n",
    "#     \"num_rounds\":  hp.choice(\"num_rounds\", [100,250,500,1000])\n",
    "#     }\n",
    "\n",
    "# rstate = np.random.default_rng(42)\n",
    "# trials = Trials()\n",
    "# best = fmin(fn = cross_validation_mse_gradient_boosting, space = space_gradient_boosting,\n",
    "#             algo=rand.suggest, max_evals = 20, trials=trials, return_argmin=False, rstate=rstate)\n",
    "\n",
    "# print(best)\n",
    "# param = best\n",
    "# param[\"random_state\"] = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if split == \"full\":\n",
    "    param = {\"random_state\": 42, 'learning_rate': 0.1, 'max_delta_step': 3.0, 'max_depth': 9.0, 'min_child_weight': 2.0, 'num_rounds': 100, 'reg_alpha': 0.8, 'reg_lambda': 0.2}\n",
    "elif split == \"Arabidopsis\":\n",
    "    param = {\"random_state\": 42, 'learning_rate': 0.01, 'max_delta_step': 5.0, 'max_depth': 5.0, 'min_child_weight': 3.0, 'num_rounds': 1000, 'reg_alpha': 0.6, 'reg_lambda': 0.8}\n",
    "elif split == \"Brassicaceae\":\n",
    "    param = {\"random_state\": 42, 'learning_rate': 0.01, 'max_delta_step': 2.0, 'max_depth': 8.0, 'min_child_weight': 5.0, 'num_rounds': 500, 'reg_alpha': 0.4, 'reg_lambda': 0.0}\n",
    "elif split == \"wildtype\":\n",
    "    param = {\"random_state\": 42, 'learning_rate': 0.1, 'max_delta_step': 3.0, 'max_depth': 9.0, 'min_child_weight': 2.0, 'num_rounds': 100, 'reg_alpha': 0.8, 'reg_lambda': 0.2}\n",
    "else:\n",
    "    param = {\"random_state\": 42, 'learning_rate': 0.01, 'max_delta_step': 2.0, 'max_depth': 8.0, 'min_child_weight': 5.0, 'num_rounds': 500, 'reg_alpha': 0.4, 'reg_lambda': 0.0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c) Training and validating model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_round = param[\"num_rounds\"]\n",
    "param[\"max_depth\"] = int(np.round(param[\"max_depth\"]))\n",
    "param[\"tree_method\"] = \"gpu_hist\"\n",
    "param[\"sampling_method\"] = \"gradient_based\"\n",
    "del param[\"num_rounds\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R2 = []\n",
    "MSE = []\n",
    "Pearson = []\n",
    "MAE = []\n",
    "MedAE = []\n",
    "\n",
    "for i in range(5):\n",
    "    train_index, test_index  = train_indices[i], test_indices[i]\n",
    "    dtrain = xgb.DMatrix(train_X[train_index], label = train_Y[train_index])\n",
    "    dvalid = xgb.DMatrix(train_X[test_index])\n",
    "    \n",
    "    bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "    \n",
    "    y_valid_pred = bst.predict(dvalid)\n",
    "    MSE.append(np.sqrt(np.mean(abs(np.array([10**x for x in np.reshape(train_Y[test_index], (-1))]) - np.array([10**x for x in y_valid_pred]))**2)))\n",
    "    R2.append(r2_score([10**x for x in np.reshape(train_Y[test_index], (-1))], [10**x for x in y_valid_pred]))\n",
    "    Pearson.append(stats.pearsonr([10**x for x in np.reshape(train_Y[test_index], (-1))], [10**x for x in y_valid_pred])[0])\n",
    "    MAE.append(np.mean(abs(np.array([10**x for x in train_Y[test_index]]) - np.array([10**x for x in y_valid_pred]))))\n",
    "    MedAE.append(np.median(abs(np.array([10**x for x in train_Y[test_index]]) - np.array([10**x for x in y_valid_pred]))))\n",
    "\n",
    "print(Pearson)\n",
    "print(MSE)\n",
    "print(R2)\n",
    "print(MAE)\n",
    "print(MedAE)\n",
    "\n",
    "np.save(join(\"..\", \"..\", \"results\", split, \"Pearson_CV_xgboost_diff_fp.npy\"), np.array(Pearson))\n",
    "np.save(join(\"..\", \"..\", \"results\", split, \"MSE_CV_xgboost_diff_fp.npy\"), np.array(MSE))\n",
    "np.save(join(\"..\", \"..\", \"results\", split, \"R2_CV_xgboost_diff_fp.npy\"), np.array(R2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(train_X, label = train_Y)\n",
    "dtest = xgb.DMatrix(test_X, label = test_Y)\n",
    "\n",
    "bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "\n",
    "y_test_pred = bst.predict(dtest)\n",
    "\n",
    "MSE_dif_fp_test = np.sqrt(np.mean(abs(np.array([10**x for x in test_Y]) - np.array([10**x for x in y_test_pred]))**2))\n",
    "R2_dif_fp_test = r2_score(np.array([10**x for x in test_Y]), np.array([10**x for x in y_test_pred]))\n",
    "Pearson = stats.pearsonr(np.array([10**x for x in test_Y]), np.array([10**x for x in y_test_pred]))\n",
    "MAE = np.mean(abs(np.array([10**x for x in test_Y]) - np.array([10**x for x in y_test_pred])))\n",
    "MedAE = np.median(abs(np.array([10**x for x in test_Y]) - np.array([10**x for x in y_test_pred])))\n",
    "\n",
    "print(np.round(Pearson[0],3) , np.round(MSE_dif_fp_test, 10), np.round(R2_dif_fp_test,3), np.round(MAE, 10), np.round(MedAE, 10))\n",
    "\n",
    "np.save(join(\"..\", \"..\", \"results\", split, \"y_test_pred_xgboost_diff_fp.npy\"), bst.predict(dtest))\n",
    "np.save(join(\"..\", \"..\", \"results\", split,  \"y_test_true_xgboost_diff_fp.npy\"), test_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (d) Training model with test and train data for production mode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = np.array(list(data_train[\"difference_fp\"]))\n",
    "train_X = np.concatenate([train_X, np.array(list(data_train[\"Temperature\"]))[:, np.newaxis], np.array(list(data_train[\"pH\"]))[:, np.newaxis]], axis = 1)\n",
    "train_Y = np.array(list(data_train[\"log10_Km\"]))\n",
    "\n",
    "test_X = np.array(list(data_test[\"difference_fp\"]))\n",
    "test_X = np.concatenate([test_X, np.array(list(data_test[\"Temperature\"]))[:, np.newaxis], np.array(list(data_test[\"pH\"]))[:, np.newaxis]], axis = 1)\n",
    "test_Y = np.array(list(data_test[\"log10_Km\"]))\n",
    "\n",
    "train_X = np.concatenate([train_X, test_X])\n",
    "train_Y = np.concatenate([train_Y, test_Y])\n",
    "\n",
    "train_X = train_X.astype(float)\n",
    "test_X = test_X.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(train_X, label = train_Y)\n",
    "dtest = xgb.DMatrix(test_X)\n",
    "\n",
    "bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "\n",
    "y_test_pred = bst.predict(dtest)\n",
    "\n",
    "MSE_dif_fp_test = np.sqrt(np.mean(abs(np.array([10**x for x in np.reshape(test_Y, (-1))]) - np.array([10**x for x in y_test_pred]))**2))\n",
    "R2_dif_fp_test = r2_score(np.array([10**x for x in np.reshape(test_Y, (-1))]), np.array([10**x for x in y_test_pred]))\n",
    "Pearson = stats.pearsonr(np.array([10**x for x in np.reshape(test_Y, (-1))]), np.array([10**x for x in y_test_pred]))\n",
    "\n",
    "print(Pearson, MSE_dif_fp_test, R2_dif_fp_test)\n",
    "\n",
    "pickle.dump(bst, open(join(\"..\", \"..\", \"results\", split, \"xgboost_diff_fp.pkl\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (e) Validating model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_X = np.array(list(data_val[\"difference_fp\"]))\n",
    "val_X = np.concatenate([val_X, np.array(list(data_val[\"Temperature\"]))[:, np.newaxis], np.array(list(data_val[\"pH\"]))[:, np.newaxis]], axis = 1)\n",
    "val_Y = np.array(list(data_val[\"log10_Km\"]))\n",
    "\n",
    "val_X = val_X.astype(float)\n",
    "\n",
    "dval = xgb.DMatrix(val_X)\n",
    "\n",
    "y_val_pred = bst.predict(dval)\n",
    "\n",
    "MSE_dif_fp_val = np.sqrt(np.mean(abs(np.array([10**x for x in np.reshape(val_Y, (-1))]) - np.array([10**x for x in y_val_pred]))**2))\n",
    "R2_dif_fp_val = r2_score(np.array([10**x for x in np.reshape(val_Y, (-1))]), np.array([10**x for x in y_val_pred]))\n",
    "Pearson = stats.pearsonr(np.array([10**x for x in np.reshape(val_Y, (-1))]), np.array([10**x for x in y_val_pred]))\n",
    "MAE = np.mean(abs(np.array([10**x for x in val_Y]) - np.array([10**x for x in y_val_pred])))\n",
    "MedAE = np.median(abs(np.array([10**x for x in val_Y]) - np.array([10**x for x in y_val_pred])))\n",
    "\n",
    "print(Pearson, MSE_dif_fp_val, R2_dif_fp_val, MAE, MedAE)\n",
    "\n",
    "np.save(join(\"..\", \"..\", \"results\", split, \"y_val_true_xgboost_diff_fp.npy\"), val_Y)\n",
    "np.save(join(\"..\", \"..\", \"results\", split, \"y_val_pred_xgboost_diff_fp.npy\"), y_val_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training a model with reaction and main substrate information (diff-fp/GNN) + Temperature + pH:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Creating input matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = np.array(list(data_train[\"difference_fp\"]))\n",
    "train_X = np.concatenate([train_X, np.array(list(data_train[\"GNN FP\"])), np.array(list(data_train[\"Temperature\"]))[:, np.newaxis], np.array(list(data_train[\"pH\"]))[:, np.newaxis]], axis = 1)\n",
    "train_Y = np.array(list(data_train[\"log10_Km\"]))\n",
    "\n",
    "test_X = np.array(list(data_test[\"difference_fp\"]))\n",
    "test_X = np.concatenate([test_X, np.array(list(data_test[\"GNN FP\"])), np.array(list(data_test[\"Temperature\"]))[:, np.newaxis], np.array(list(data_test[\"pH\"]))[:, np.newaxis]], axis = 1)\n",
    "test_Y = np.array(list(data_test[\"log10_Km\"]))\n",
    "\n",
    "train_X = train_X.astype(float)\n",
    "test_X = test_X.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Hyperparameter optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def cross_validation_mse_gradient_boosting(param):\n",
    "#     num_round = param[\"num_rounds\"]\n",
    "#     del param[\"num_rounds\"]\n",
    "#     param[\"max_depth\"] = int(np.round(param[\"max_depth\"]))\n",
    "#     # param[\"device\"] = \"cuda\"\n",
    "#     param[\"tree_method\"] = \"hist\"\n",
    "#     param[\"sampling_method\"] = \"gradient_based\"\n",
    "    \n",
    "#     MSE = []\n",
    "#     R2 = []\n",
    "#     for i in range(5):\n",
    "#         train_index, test_index  = train_indices[i], test_indices[i]\n",
    "#         dtrain = xgb.DMatrix(train_X[train_index], label = train_Y[train_index])\n",
    "#         dvalid = xgb.DMatrix(train_X[test_index])\n",
    "#         bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "#         y_valid_pred = bst.predict(dvalid)\n",
    "#         MSE.append(np.mean(abs(np.reshape(train_Y[test_index], (-1)) - y_valid_pred)**2))\n",
    "#         R2.append(r2_score(np.reshape(train_Y[test_index], (-1)),  y_valid_pred))\n",
    "#     return(-np.mean(R2))\n",
    "\n",
    "# space_gradient_boosting = {\n",
    "#     \"learning_rate\": hp.choice(\"learning_rate\", [0.01,0.1,0.2]),\n",
    "#     \"max_depth\": hp.quniform(\"max_depth\", 3, 10, 1),\n",
    "#     # \"subsample\": hp.quniform(\"subsample\", 0.5, 1, 0.5),\n",
    "#     \"reg_lambda\": hp.quniform(\"reg_lambda\", 0, 1, 0.2),\n",
    "#     \"reg_alpha\": hp.quniform(\"reg_alpha\", 0, 1, 0.2),\n",
    "#     \"max_delta_step\": hp.quniform(\"max_delta_step\", 1, 5, 1),\n",
    "#     \"min_child_weight\": hp.quniform(\"min_child_weight\", 1, 6, 1),\n",
    "#     \"num_rounds\":  hp.choice(\"num_rounds\", [100,250,500,1000])\n",
    "#     }\n",
    "\n",
    "# rstate = np.random.default_rng(42)\n",
    "# trials = Trials()\n",
    "# best = fmin(fn = cross_validation_mse_gradient_boosting, space = space_gradient_boosting,\n",
    "#             algo=rand.suggest, max_evals = 20, trials=trials, return_argmin=False, rstate=rstate)\n",
    "\n",
    "# print(best)\n",
    "# param = best\n",
    "# param[\"random_state\"] = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "if split == \"full\":\n",
    "    param = {\"random_state\": 42, 'learning_rate': 0.01, 'max_delta_step': 2.0, 'max_depth': 8.0, 'min_child_weight': 5.0, 'num_rounds': 500, 'reg_alpha': 0.4, 'reg_lambda': 0.0}\n",
    "elif split == \"Arabidopsis\":\n",
    "    param = {\"random_state\": 42, 'learning_rate': 0.01, 'max_delta_step': 5.0, 'max_depth': 5.0, 'min_child_weight': 3.0, 'num_rounds': 1000, 'reg_alpha': 0.6, 'reg_lambda': 0.8}\n",
    "elif split == \"Brassicaceae\":\n",
    "    param = {\"random_state\": 42, 'learning_rate': 0.2, 'max_delta_step': 3.0, 'max_depth': 5.0, 'min_child_weight': 2.0, 'num_rounds': 500, 'reg_alpha': 0.8, 'reg_lambda': 0.4}\n",
    "elif split == \"wildtype\":\n",
    "    param = {\"random_state\": 42, 'learning_rate': 0.01, 'max_delta_step': 5.0, 'max_depth': 11.0, 'min_child_weight': 3.0, 'num_rounds': 1000, 'reg_alpha': 0.6, 'reg_lambda': 0.8}\n",
    "else:\n",
    "    param = {\"random_state\": 42, 'learning_rate': 0.1, 'max_delta_step': 3.0, 'max_depth': 4.0, 'min_child_weight': 5.0, 'num_rounds': 500, 'reg_alpha': 0.2, 'reg_lambda': 0.4}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c) Training and validating model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_round = param[\"num_rounds\"]\n",
    "param[\"max_depth\"] = int(np.round(param[\"max_depth\"]))\n",
    "param[\"tree_method\"] = \"gpu_hist\"\n",
    "param[\"sampling_method\"] = \"gradient_based\"\n",
    "del param[\"num_rounds\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R2 = []\n",
    "MSE = []\n",
    "Pearson = []\n",
    "MAE = []\n",
    "MedAE = []\n",
    "\n",
    "for i in range(5):\n",
    "    train_index, test_index  = train_indices[i], test_indices[i]\n",
    "    dtrain = xgb.DMatrix(train_X[train_index], label = train_Y[train_index])\n",
    "    dvalid = xgb.DMatrix(train_X[test_index])\n",
    "    \n",
    "    bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "    \n",
    "    y_valid_pred = bst.predict(dvalid)\n",
    "    MSE.append(np.sqrt(np.mean(abs(np.array([10**x for x in np.reshape(train_Y[test_index], (-1))]) - np.array([10**x for x in y_valid_pred]))**2)))\n",
    "    R2.append(r2_score([10**x for x in np.reshape(train_Y[test_index], (-1))], [10**x for x in y_valid_pred]))\n",
    "    Pearson.append(stats.pearsonr([10**x for x in np.reshape(train_Y[test_index], (-1))], [10**x for x in y_valid_pred])[0])\n",
    "    MAE.append(np.mean(abs(np.array([10**x for x in train_Y[test_index]]) - np.array([10**x for x in y_valid_pred]))))\n",
    "    MedAE.append(np.median(abs(np.array([10**x for x in train_Y[test_index]]) - np.array([10**x for x in y_valid_pred]))))\n",
    "\n",
    "print(Pearson)\n",
    "print(MSE)\n",
    "print(R2)\n",
    "print(MAE)\n",
    "print(MedAE)\n",
    "\n",
    "np.save(join(\"..\", \"..\", \"results\", split, \"Pearson_CV_xgboost_gnn_fp_diff_fp.npy\"), np.array(Pearson))\n",
    "np.save(join(\"..\", \"..\", \"results\", split, \"MSE_CV_xgboost_gnn_fp_diff_fp.npy\"), np.array(MSE))\n",
    "np.save(join(\"..\", \"..\", \"results\", split, \"R2_CV_xgboost_gnn_fp_diff_fp.npy\"), np.array(R2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(train_X, label = train_Y)\n",
    "dtest = xgb.DMatrix(test_X, label = test_Y)\n",
    "\n",
    "bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "\n",
    "y_test_pred = bst.predict(dtest)\n",
    "\n",
    "MSE_dif_fp_test = np.sqrt(np.mean(abs(np.array([10**x for x in test_Y]) - np.array([10**x for x in y_test_pred]))**2))\n",
    "R2_dif_fp_test = r2_score(np.array([10**x for x in test_Y]), np.array([10**x for x in y_test_pred]))\n",
    "Pearson = stats.pearsonr(np.array([10**x for x in test_Y]), np.array([10**x for x in y_test_pred]))\n",
    "MAE = np.mean(abs(np.array([10**x for x in test_Y]) - np.array([10**x for x in y_test_pred])))\n",
    "MedAE = np.median(abs(np.array([10**x for x in test_Y]) - np.array([10**x for x in y_test_pred])))\n",
    "\n",
    "print(np.round(Pearson[0],20) , np.round(MSE_dif_fp_test, 10), np.round(R2_dif_fp_test,3), np.round(MAE, 10), np.round(MedAE, 10))\n",
    "\n",
    "\n",
    "np.save(join(\"..\", \"..\", \"results\", split, \"y_test_pred_xgboost_gnn_fp_diff_fp.npy\"), bst.predict(dtest))\n",
    "np.save(join(\"..\", \"..\", \"results\", split,  \"y_test_true_xgboost_gnn_fp_diff_fp.npy\"), test_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (d) Training model with test and train data for production mode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = np.array(list(data_train[\"difference_fp\"]))\n",
    "train_X = np.concatenate([train_X, np.array(list(data_train[\"GNN FP\"])), np.array(list(data_train[\"Temperature\"]))[:, np.newaxis], np.array(list(data_train[\"pH\"]))[:, np.newaxis]], axis = 1)\n",
    "train_Y = np.array(list(data_train[\"log10_Km\"]))\n",
    "\n",
    "test_X = np.array(list(data_test[\"difference_fp\"]))\n",
    "test_X = np.concatenate([test_X, np.array(list(data_test[\"GNN FP\"])), np.array(list(data_test[\"Temperature\"]))[:, np.newaxis], np.array(list(data_test[\"pH\"]))[:, np.newaxis]], axis = 1)\n",
    "test_Y = np.array(list(data_test[\"log10_Km\"]))\n",
    "\n",
    "train_X = np.concatenate([train_X, test_X])\n",
    "train_Y = np.concatenate([train_Y, test_Y])\n",
    "\n",
    "train_X = train_X.astype(float)\n",
    "test_X = test_X.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.9518546955559481, 9.681199131833892e-25) 8.248612992253333e-05 0.8976388650347142\n"
     ]
    }
   ],
   "source": [
    "dtrain = xgb.DMatrix(train_X, label = train_Y)\n",
    "dtest = xgb.DMatrix(test_X)\n",
    "\n",
    "bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "\n",
    "y_test_pred = bst.predict(dtest)\n",
    "\n",
    "MSE_dif_fp_test = np.sqrt(np.mean(abs(np.array([10**x for x in np.reshape(test_Y, (-1))]) - np.array([10**x for x in y_test_pred]))**2))\n",
    "R2_dif_fp_test = r2_score(np.array([10**x for x in np.reshape(test_Y, (-1))]), np.array([10**x for x in y_test_pred]))\n",
    "Pearson = stats.pearsonr(np.array([10**x for x in np.reshape(test_Y, (-1))]), np.array([10**x for x in y_test_pred]))\n",
    "\n",
    "print(Pearson, MSE_dif_fp_test, R2_dif_fp_test)\n",
    "\n",
    "pickle.dump(bst, open(join(\"..\", \"..\", \"results\", split, \"xgboost_gnn_fp_diff_fp.pkl\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = bst.get_score(importance_type=\"total_gain\")\n",
    "list = list(importances.items())\n",
    "sorted_items = sorted(list, key=lambda x: x[1], reverse=True)[:30]\n",
    "print(sorted_items)\n",
    "dicti = dict(sorted_items)\n",
    "importances = pd.DataFrame({'Feature' : dicti.keys(), \"Importance\" : dicti.values()})\n",
    "def condition(x):\n",
    "    if int(x[1:]) <= 2047:\n",
    "        return \"Reaction\"\n",
    "    elif 2048 <= int(x[1:]) <= 2097:\n",
    "        return \"Substrate\"\n",
    "    elif int(x[1:]) == 2098:\n",
    "        return 'Temperature'\n",
    "    else : \n",
    "        return 'pH'\n",
    "importances[\"Feature category\"] = importances[\"Feature\"].apply(condition)\n",
    "importances_order = importances.sort_values(by='Importance', ascending=False)[\"Feature\"]\n",
    "(ggplot(importances, aes(x=\"Feature\", y=\"Importance\", fill=\"Feature category\"))\n",
    "+ geom_bar(stat=\"identity\", width=0.9, color='black')\n",
    "+ scale_x_discrete(limits= importances_order)\n",
    "+ labs(title=\"Feature importance analysis on reaction+substrate model (Km)\", y = \"Total gain\")\n",
    "+ theme_classic(base_size=38)\n",
    "+ theme(figure_size=(20, 10), axis_text_x=element_text(angle=-90), plot_title = element_text(ha = \"center\"))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (e) Validating model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.6818551493917264, 0.0006632871608159583) 0.0018432566972590853 0.12316334523915462 0.00072963039708445 0.00010633489999653051\n"
     ]
    }
   ],
   "source": [
    "val_X = np.array(list(data_val[\"difference_fp\"]))\n",
    "val_X = np.concatenate([val_X, np.array(list(data_val[\"GNN FP\"])), np.array(list(data_val[\"Temperature\"]))[:, np.newaxis], np.array(list(data_val[\"pH\"]))[:, np.newaxis]], axis = 1)\n",
    "val_Y = np.array(list(data_val[\"log10_Km\"]))\n",
    "\n",
    "val_X = val_X.astype(float)\n",
    "\n",
    "dval = xgb.DMatrix(val_X)\n",
    "\n",
    "y_val_pred = bst.predict(dval)\n",
    "\n",
    "MSE_dif_fp_val = np.sqrt(np.mean(abs(np.array([10**x for x in np.reshape(val_Y, (-1))]) - np.array([10**x for x in y_val_pred]))**2))\n",
    "R2_dif_fp_val = r2_score(np.array([10**x for x in np.reshape(val_Y, (-1))]), np.array([10**x for x in y_val_pred]))\n",
    "Pearson = stats.pearsonr(np.array([10**x for x in np.reshape(val_Y, (-1))]), np.array([10**x for x in y_val_pred]))\n",
    "MAE = np.mean(abs(np.array([10**x for x in val_Y]) - np.array([10**x for x in y_val_pred])))\n",
    "MedAE = np.median(abs(np.array([10**x for x in val_Y]) - np.array([10**x for x in y_val_pred])))\n",
    "\n",
    "print(Pearson, MSE_dif_fp_val, R2_dif_fp_val, MAE, MedAE)\n",
    "np.save(join(\"..\", \"..\", \"results\", split, \"y_val_true_xgboost_diff_fp_gnn_fp.npy\"), val_Y)\n",
    "np.save(join(\"..\", \"..\", \"results\", split, \"y_val_pred_xgboost_diff_fp_gnn_fp.npy\"), y_val_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.TreeExplainer(bst)\n",
    "explanation = explainer(val_X)\n",
    "\n",
    "shap_values = explanation.values\n",
    "\n",
    "shap.plots.waterfall(explanation[19], max_display=15, show=False)\n",
    "plt.title(\"SHAP Waterfall Plot for glucosinolates' worst prediction (Km)\", fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = list()\n",
    "for i in range(0, 2047):\n",
    "    names.append(\"Reaction_%s\" %str(i))\n",
    "\n",
    "for i in range(0, 49):\n",
    "    names.append(\"Substrate_%s\" %str(i))\n",
    "\n",
    "names.append([\"Temperature\", \"pH\"])\n",
    "\n",
    "shap.force_plot(explainer.expected_value, shap_values[19], val_X[19], matplotlib=True, feature_names=names, text_rotation=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.DataFrame({\"Actual Km\": [10**x for x in val_Y], \"Predicted Km\" : [10**x for x in y_val_pred], \"Difference\" : abs(np.array([10**x for x in val_Y])-np.array([10**x for x in y_val_pred]))})\n",
    "\n",
    "(\n",
    "    ggplot(predictions, aes(x = \"Actual Km\", y=\"Predicted Km\", fill=\"Difference\"))\n",
    "    + geom_point(color=\"black\", size=5) \n",
    "    + geom_abline(slope = 1, intercept = 0, color = \"grey\", linetype=\"dashed\")\n",
    "    + scale_fill_gradient2(low = \"green\", mid=\"yellow\", high = \"red\", midpoint=0.004)\n",
    "    + labs(title=\"Predictions for glucosinolate set (Km)\", x=\"Actual Km (M)\", y=\"Predicted Km (M)\", fill=\"Difference\")\n",
    "    + theme_classic(base_size=38)\n",
    "    + xlim(min(predictions[\"Actual Km\"]), max(predictions[\"Actual Km\"]))\n",
    "    + ylim(min(predictions[\"Predicted Km\"]), max(predictions[\"Predicted Km\"]))\n",
    "    + theme(figure_size=(10, 10))\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
