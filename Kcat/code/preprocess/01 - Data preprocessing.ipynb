{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os.path import join\n",
    "import os\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import MACCSkeys\n",
    "from ete3 import NCBITaxa\n",
    "import random\n",
    "random.seed(10)\n",
    "import torch\n",
    "import esm\n",
    "from bioservices import *\n",
    "from data_preprocessing import *\n",
    "from functions_and_dicts_data_preprocessing_GNN import *\n",
    "from build_GNN import *\n",
    "import warnings\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
    "warnings.filterwarnings('ignore')\n",
    "datasets_dir = \"../../data\"\n",
    "\n",
    "CURRENT_DIR = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading in Sabio data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Sabio data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data points: 1344\n",
      "Number of UniProt IDs: 370\n"
     ]
    }
   ],
   "source": [
    "organism = \"Seed plants\"\n",
    "\n",
    "df_Sabio = pd.read_table(join(datasets_dir, \"kcat_model_\" + organism + \".tsv\"))\n",
    "\n",
    "df_Sabio[\"kcat\"] = df_Sabio[\"kcat\"].astype('float')\n",
    "df_Sabio[\"PMID\"] = df_Sabio[\"PMID\"].astype('Int64')\n",
    "\n",
    "df_Sabio[\"substrate_IDs\"] = df_Sabio[\"substrate_IDs\"].str.split('#')\n",
    "df_Sabio[\"product_IDs\"] = df_Sabio[\"product_IDs\"].str.split('#')\n",
    "\n",
    "df_Sabio[\"Type\"][df_Sabio['Type'].str.contains(\"wildtype\")] = \"wildtype\"\n",
    "df_Sabio[\"Type\"][df_Sabio['Type'].str.contains(\"mutant\")] = \"mutant\"\n",
    "\n",
    "print(\"Number of data points: %s\" % len(df_Sabio))\n",
    "print(\"Number of UniProt IDs: %s\" % len(set(df_Sabio[\"Uniprot IDs\"])))\n",
    "\n",
    "df_kcat = df_Sabio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "droplist = []\n",
    "\n",
    "for ind in df_kcat.index:\n",
    "    UID, kcat = df_kcat[\"Uniprot IDs\"][ind], df_kcat[\"kcat\"][ind]\n",
    "    help_df = df_kcat.loc[df_kcat[\"Uniprot IDs\"] == UID].loc[df_kcat[\"kcat\"] == kcat]\n",
    "    \n",
    "    if len(help_df) > 1:\n",
    "        droplist = droplist + list(help_df.index)[1:]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kcat.drop(list(set(droplist)), inplace = True)\n",
    "print(\"Dropping %s data points, because they are duplicated.\" % len(set(droplist)))\n",
    "df_kcat.reset_index(inplace = True, drop = True)\n",
    "df_kcat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing top and bottom 3% of kcat values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_outliers_IQR(df):\n",
    "\n",
    "   q1=df.quantile(0.25)\n",
    "\n",
    "   q3=df.quantile(0.75)\n",
    "\n",
    "   IQR=q3-q1\n",
    "\n",
    "   outliers = df[((df<(q1-1.5*IQR)) | (df>(q3+1.5*IQR)))]\n",
    "\n",
    "   return outliers\n",
    "\n",
    "find_outliers_IQR(df_kcat[\"kcat\"])\n",
    "\n",
    "print(df_kcat['kcat'].quantile(0.03),  df_kcat['kcat'].quantile(0.97))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kcat = df_kcat[(df_kcat['kcat'] > df_kcat['kcat'].quantile(0.03)) & (df_kcat['kcat'] < df_kcat['kcat'].quantile(0.97))]\n",
    "df_kcat.reset_index(inplace = True, drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "todrop= []\n",
    "\n",
    "for ind in df_kcat.index:\n",
    "    UID = df_kcat[\"Uniprot IDs\"][ind]\n",
    "    if len(UID.split(';')) > 1:\n",
    "        todrop.append(ind)\n",
    "        print(df_kcat[\"Uniprot IDs\"][ind])\n",
    "        print(todrop)\n",
    "        \n",
    "df_kcat.drop(todrop, inplace=True)\n",
    "df_kcat.reset_index(inplace = True, drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kcat[\"substrate_IDs\"] = df_kcat[\"substrate_IDs\"].apply(lambda x: (set(x)))\n",
    "df_kcat[\"product_IDs\"] = df_kcat[\"product_IDs\"].apply(lambda x: (set(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kcat.to_pickle(join(datasets_dir, \"kcat_data_merged.pkl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Assigning IDs to every unique sequence and to every unique reaction in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating DataFrames for all sequences and for all reactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reactions = pd.DataFrame({\"substrates\": df_kcat[\"substrate_IDs\"],\n",
    "                            \"products\" : df_kcat[\"product_IDs\"]})\n",
    "\n",
    "df_reactions = df_reactions.loc[df_reactions[\"substrates\"] != set([])]\n",
    "df_reactions = df_reactions.loc[df_reactions[\"products\"] != set([])]\n",
    "\n",
    "\n",
    "droplist = []\n",
    "for ind in df_reactions.index:\n",
    "    sub_IDs, pro_IDs = df_reactions[\"substrates\"][ind], df_reactions[\"products\"][ind]\n",
    "    help_df = df_reactions.loc[df_reactions[\"substrates\"] == sub_IDs].loc[df_reactions[\"products\"] == pro_IDs]\n",
    "    if len(help_df):\n",
    "        for ind in list(help_df.index)[1:]:\n",
    "            droplist.append(ind)\n",
    "            \n",
    "df_reactions.drop(list(set(droplist)), inplace = True)\n",
    "df_reactions.reset_index(inplace = True, drop =True)\n",
    "\n",
    "df_reactions[\"Reaction ID\"] = [\"Reaction_\" + str(ind) for ind in df_reactions.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sequence</th>\n",
       "      <th>Sequence ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MTTGKGKILILGATGYLGKYMVKASISLGHPTYAYVMPLKKNSDDS...</td>\n",
       "      <td>Sequence_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MEENGMKSKILIFGGTGYIGNHMVKGSLKLGHPTYVFTRPNSSKTT...</td>\n",
       "      <td>Sequence_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MANLRESSRDKSRWSLEGMTALVTGGSKGIGEAVVEELAMLGARVH...</td>\n",
       "      <td>Sequence_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MAKEGGLGENSRWSLGGMTALVTGGSKGIGEAVVEELAMLGAKVHT...</td>\n",
       "      <td>Sequence_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MAKAGENSRDKSRWSLEGMTALVTGGSKGLGEAVVEELAMLGARVH...</td>\n",
       "      <td>Sequence_4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>455</th>\n",
       "      <td>MSSLADLINLDLSDSTDQIIAEYIWIGGSGLDMRSKARTLPGPVTD...</td>\n",
       "      <td>Sequence_455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456</th>\n",
       "      <td>MSSLADLINLDLSDSTDQIIAEYIWIGGSGLDMRSKARTLPGPVTD...</td>\n",
       "      <td>Sequence_456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457</th>\n",
       "      <td>MALLSQAGGSYTVVPSGVCSKAGTKAVVSGGVRNLDVLRMKEAFGS...</td>\n",
       "      <td>Sequence_457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458</th>\n",
       "      <td>MLPKFDPTNQKACLSLLEDLTTNVKQIQDSVLEAILSRNAQTEYLR...</td>\n",
       "      <td>Sequence_458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459</th>\n",
       "      <td>MSLSTLSHPAAAAAGSGKSLFPAGPAAQSVHFPKARLPVPAAVSAA...</td>\n",
       "      <td>Sequence_459</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>460 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Sequence   Sequence ID\n",
       "0    MTTGKGKILILGATGYLGKYMVKASISLGHPTYAYVMPLKKNSDDS...    Sequence_0\n",
       "1    MEENGMKSKILIFGGTGYIGNHMVKGSLKLGHPTYVFTRPNSSKTT...    Sequence_1\n",
       "2    MANLRESSRDKSRWSLEGMTALVTGGSKGIGEAVVEELAMLGARVH...    Sequence_2\n",
       "3    MAKEGGLGENSRWSLGGMTALVTGGSKGIGEAVVEELAMLGAKVHT...    Sequence_3\n",
       "4    MAKAGENSRDKSRWSLEGMTALVTGGSKGLGEAVVEELAMLGARVH...    Sequence_4\n",
       "..                                                 ...           ...\n",
       "455  MSSLADLINLDLSDSTDQIIAEYIWIGGSGLDMRSKARTLPGPVTD...  Sequence_455\n",
       "456  MSSLADLINLDLSDSTDQIIAEYIWIGGSGLDMRSKARTLPGPVTD...  Sequence_456\n",
       "457  MALLSQAGGSYTVVPSGVCSKAGTKAVVSGGVRNLDVLRMKEAFGS...  Sequence_457\n",
       "458  MLPKFDPTNQKACLSLLEDLTTNVKQIQDSVLEAILSRNAQTEYLR...  Sequence_458\n",
       "459  MSLSTLSHPAAAAAGSGKSLFPAGPAAQSVHFPKARLPVPAAVSAA...  Sequence_459\n",
       "\n",
       "[460 rows x 2 columns]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sequences = pd.DataFrame(data = {\"Sequence\" : df_kcat[\"Sequence\"].unique()})\n",
    "df_sequences = df_sequences.loc[~pd.isnull(df_sequences[\"Sequence\"])]\n",
    "df_sequences.reset_index(inplace = True, drop = True)\n",
    "df_sequences[\"Sequence ID\"] = [\"Sequence_\" + str(ind) for ind in df_sequences.index]\n",
    "\n",
    "df_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating maximal kcat value for each reaction and sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reactions[\"max_kcat_for_RID\"] = np.nan\n",
    "for ind in df_reactions.index:\n",
    "    df_reactions[\"max_kcat_for_RID\"][ind] = max(df_kcat.loc[df_kcat[\"substrate_IDs\"] == df_reactions[\"substrates\"][ind]].loc[df_kcat[\"product_IDs\"] == df_reactions[\"products\"][ind]][\"kcat\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sequences[\"max_kcat_for_UID\"] = np.nan\n",
    "for ind in df_sequences.index:\n",
    "    df_sequences[\"max_kcat_for_UID\"][ind] = max(df_kcat.loc[df_kcat[\"Sequence\"] == df_sequences['Sequence'][ind]][\"kcat\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating the sum of the molecular weights of all substrates and of all products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reactions[\"MW_frac\"] = np.nan\n",
    "\n",
    "for ind in df_reactions.index:\n",
    "    substrates = list(df_reactions[\"substrates\"][ind])\n",
    "    products = list(df_reactions[\"products\"][ind])\n",
    "    \n",
    "    mw_subs = mw_mets(metabolites = substrates)\n",
    "    mw_pros = mw_mets(metabolites = products)\n",
    "    \n",
    "    if mw_subs == np.nan or mw_pros == np.nan:\n",
    "        df_reactions[\"MW_frac\"][ind] = np.inf\n",
    "    if mw_pros != 0:\n",
    "        df_reactions[\"MW_frac\"][ind] = mw_subs/mw_pros\n",
    "    else:\n",
    "        df_reactions[\"MW_frac\"][ind] = np.inf\n",
    "        \n",
    "df_reactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating enzyme, reaction and substrate features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\marle/.cache\\torch\\hub\\facebookresearch_esm_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.decay\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.rho\n"
     ]
    }
   ],
   "source": [
    "model, alphabet = torch.hub.load(\"facebookresearch/esm:main\", \"esm2_t33_650M_UR50D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".....2(b) Calculating enzyme representations.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sequence</th>\n",
       "      <th>Sequence ID</th>\n",
       "      <th>model_input</th>\n",
       "      <th>Enzyme rep</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MTTGKGKILILGATGYLGKYMVKASISLGHPTYAYVMPLKKNSDDS...</td>\n",
       "      <td>Sequence_0</td>\n",
       "      <td>MTTGKGKILILGATGYLGKYMVKASISLGHPTYAYVMPLKKNSDDS...</td>\n",
       "      <td>[-0.032205846, -0.031796537, -0.051493276, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MEENGMKSKILIFGGTGYIGNHMVKGSLKLGHPTYVFTRPNSSKTT...</td>\n",
       "      <td>Sequence_1</td>\n",
       "      <td>MEENGMKSKILIFGGTGYIGNHMVKGSLKLGHPTYVFTRPNSSKTT...</td>\n",
       "      <td>[-0.016749386, -0.048214775, -0.049711384, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MANLRESSRDKSRWSLEGMTALVTGGSKGIGEAVVEELAMLGARVH...</td>\n",
       "      <td>Sequence_2</td>\n",
       "      <td>MANLRESSRDKSRWSLEGMTALVTGGSKGIGEAVVEELAMLGARVH...</td>\n",
       "      <td>[-0.0007728094, -0.061243154, 0.041369718, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MAKEGGLGENSRWSLGGMTALVTGGSKGIGEAVVEELAMLGAKVHT...</td>\n",
       "      <td>Sequence_3</td>\n",
       "      <td>MAKEGGLGENSRWSLGGMTALVTGGSKGIGEAVVEELAMLGAKVHT...</td>\n",
       "      <td>[-0.011831572, -0.06318858, 0.038726494, 0.021...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MAKAGENSRDKSRWSLEGMTALVTGGSKGLGEAVVEELAMLGARVH...</td>\n",
       "      <td>Sequence_4</td>\n",
       "      <td>MAKAGENSRDKSRWSLEGMTALVTGGSKGLGEAVVEELAMLGARVH...</td>\n",
       "      <td>[-0.0063330494, -0.07236029, 0.040614996, 0.02...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sequence Sequence ID  \\\n",
       "0  MTTGKGKILILGATGYLGKYMVKASISLGHPTYAYVMPLKKNSDDS...  Sequence_0   \n",
       "1  MEENGMKSKILIFGGTGYIGNHMVKGSLKLGHPTYVFTRPNSSKTT...  Sequence_1   \n",
       "2  MANLRESSRDKSRWSLEGMTALVTGGSKGIGEAVVEELAMLGARVH...  Sequence_2   \n",
       "3  MAKEGGLGENSRWSLGGMTALVTGGSKGIGEAVVEELAMLGAKVHT...  Sequence_3   \n",
       "4  MAKAGENSRDKSRWSLEGMTALVTGGSKGLGEAVVEELAMLGARVH...  Sequence_4   \n",
       "\n",
       "                                         model_input  \\\n",
       "0  MTTGKGKILILGATGYLGKYMVKASISLGHPTYAYVMPLKKNSDDS...   \n",
       "1  MEENGMKSKILIFGGTGYIGNHMVKGSLKLGHPTYVFTRPNSSKTT...   \n",
       "2  MANLRESSRDKSRWSLEGMTALVTGGSKGIGEAVVEELAMLGARVH...   \n",
       "3  MAKEGGLGENSRWSLGGMTALVTGGSKGIGEAVVEELAMLGAKVHT...   \n",
       "4  MAKAGENSRDKSRWSLEGMTALVTGGSKGLGEAVVEELAMLGARVH...   \n",
       "\n",
       "                                          Enzyme rep  \n",
       "0  [-0.032205846, -0.031796537, -0.051493276, 0.0...  \n",
       "1  [-0.016749386, -0.048214775, -0.049711384, 0.0...  \n",
       "2  [-0.0007728094, -0.061243154, 0.041369718, 0.0...  \n",
       "3  [-0.011831572, -0.06318858, 0.038726494, 0.021...  \n",
       "4  [-0.0063330494, -0.07236029, 0.040614996, 0.02...  "
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating model input:\n",
    "df_sequences[\"model_input\"] = [seq[:1022] for seq in df_sequences[\"Sequence\"]]\n",
    "model_input = [(df_sequences[\"Sequence ID\"][ind], df_sequences[\"model_input\"][ind]) for ind in df_sequences.index]\n",
    "seqs = [model_input[i][1] for i in range(len(model_input))]\n",
    "#loading ESM-2 model:\n",
    "print(\".....2(a) Loading ESM-2 model.\")\n",
    "model, alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n",
    "batch_converter = alphabet.get_batch_converter()\n",
    "#convert input into batches:\n",
    "\n",
    "#Calculate ESM-2 representations\n",
    "print(\".....2(b) Calculating enzyme representations.\")\n",
    "df_sequences[\"Enzyme rep\"] = \"\"\n",
    "\n",
    "for ind in df_sequences.index:\n",
    "    print(ind,\"/\",len(df_sequences))    \n",
    "    batch_labels, batch_strs, batch_tokens = batch_converter([(df_sequences[\"Sequence ID\"][ind], df_sequences[\"model_input\"][ind])])\n",
    "    with torch.no_grad():\n",
    "        results = model(batch_tokens, repr_layers=[33])\n",
    "    df_sequences[\"Enzyme rep\"][ind] = results[\"representations\"][33][0, 1 : len(df_sequences[\"model_input\"][ind]) + 1].mean(0).numpy()\n",
    "    \n",
    "df_sequences.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metabolite_type(met):\n",
    "    if is_KEGG_ID(met):\n",
    "        return(\"KEGG\")\n",
    "    elif is_InChI(met):\n",
    "        return(\"InChI\")\n",
    "    else:\n",
    "        return(\"invalid\")\n",
    "\n",
    "def get_reaction_site_smarts(metabolites):\n",
    "    reaction_site = \"\"\n",
    "    for met in metabolites:\n",
    "        met_type = get_metabolite_type(met)\n",
    "        if met_type == \"KEGG\":\n",
    "            try:\n",
    "                Smarts = Chem.MolToSmarts(Chem.MolFromMolFile(join(\"\", \"\", \"data\", \"mol-files\",  met + \".mol\")))\n",
    "            except OSError:\n",
    "                return(np.nan)\n",
    "        elif met_type == \"InChI\":\n",
    "            Smarts = Chem.MolToSmarts(Chem.inchi.MolFromInchi(met))\n",
    "        else:\n",
    "            Smarts = \"invalid\"\n",
    "        reaction_site = reaction_site + \".\" + Smarts\n",
    "    return(reaction_site[1:])\n",
    "\n",
    "\n",
    "def is_KEGG_ID(met):\n",
    "    #a valid KEGG ID starts with a \"C\" or \"D\" followed by a 5 digit number:\n",
    "    if len(met) == 6 and met[0] in [\"C\", \"D\"]:\n",
    "        try:\n",
    "            int(met[1:])\n",
    "            return(True)\n",
    "        except: \n",
    "            pass\n",
    "    return(False)\n",
    "\n",
    "def is_InChI(met):\n",
    "    m = Chem.inchi.MolFromInchi(met,sanitize=False)\n",
    "    if m is None:\n",
    "      return(False)\n",
    "    else:\n",
    "      try:\n",
    "        Chem.SanitizeMol(m)\n",
    "      except:\n",
    "        print('.......Metabolite string \"%s\" is in InChI format but has invalid chemistry' % met)\n",
    "        return(False)\n",
    "    return(True)\n",
    "\n",
    "def convert_fp_to_array(difference_fp_dict):\n",
    "    fp = np.zeros(2048)\n",
    "    for key in difference_fp_dict.keys():\n",
    "        fp[key] = difference_fp_dict[key]\n",
    "    return(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reactions[\"difference_fp\"], df_reactions[\"structural_fp\"],  = \"\", \"\"\n",
    "#each metabolite should be either a KEGG ID, InChI string, or a SMILES:\n",
    "for ind in df_reactions.index:\n",
    "    left_site = get_reaction_site_smarts(df_reactions[\"substrates\"][ind])\n",
    "    right_site = get_reaction_site_smarts(df_reactions[\"products\"][ind])\n",
    "    if not pd.isnull(left_site) and not pd.isnull(right_site):\n",
    "        rxn_forward = AllChem.ReactionFromSmarts(left_site + \">>\" + right_site)\n",
    "        difference_fp = Chem.rdChemReactions.CreateDifferenceFingerprintForReaction(rxn_forward)\n",
    "        difference_fp = convert_fp_to_array(difference_fp.GetNonzeroElements())\n",
    "        df_reactions[\"difference_fp\"][ind] = difference_fp\n",
    "        df_reactions[\"structural_fp\"][ind] = Chem.rdChemReactions.CreateStructuralFingerprintForReaction(rxn_forward).ToBitString()\n",
    "\n",
    "df_reactions.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sequences.to_pickle(join(datasets_dir, \"all_sequences_with_IDs.pkl\"))\n",
    "df_reactions.to_pickle(join(datasets_dir, \"all_reactions_with_IDs.pkl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mapping Sequence and Reaction IDs to kcat_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kcat = df_kcat.merge(df_sequences, on = \"Sequence\", how = \"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reactions.rename(columns = {\"substrates\" : \"substrate_IDs\",\n",
    "                              \"products\" : \"product_IDs\"}, inplace = True)\n",
    "\n",
    "df_kcat[\"Reaction ID\"] = np.nan\n",
    "df_kcat[\"MW_frac\"] = np.nan\n",
    "df_kcat[\"max_kcat_for_RID\"] = np.nan\n",
    "df_kcat[\"difference_fp\"] = \"\"\n",
    "df_kcat[\"structural_fp\"] = \"\"\n",
    "\n",
    "for ind in df_kcat.index:\n",
    "    sub_set, pro_set = df_kcat[\"substrate_IDs\"][ind], df_kcat[\"product_IDs\"][ind]\n",
    "    \n",
    "    help_df = df_reactions.loc[df_reactions[\"substrate_IDs\"] == sub_set].loc[df_reactions[\"product_IDs\"] == pro_set]\n",
    "    if len(help_df) == 1:\n",
    "        df_kcat[\"Reaction ID\"][ind] = list(help_df[\"Reaction ID\"])[0]\n",
    "        df_kcat[\"max_kcat_for_RID\"][ind] = list(help_df[\"max_kcat_for_RID\"])[0]\n",
    "        df_kcat[\"MW_frac\"][ind] = list(help_df[\"MW_frac\"])[0]\n",
    "        df_kcat[\"difference_fp\"][ind] = list(help_df[\"difference_fp\"])[0]\n",
    "        df_kcat[\"structural_fp\"][ind] = list(help_df[\"structural_fp\"])[0]\n",
    "df_kcat.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kcat[\"MACCS FP\"] = \"\"\n",
    "\n",
    "for ind in df_kcat.index:\n",
    "    id = df_kcat[\"Main Substrate\"][ind]\n",
    "    # try:\n",
    "    #     id = df_kcat['Substrate_IDs'][ind][df_kcat[\"Substrates\"][ind].split(';').index(substrate)]\n",
    "    # except:\n",
    "    #     for i,s in enumerate(df_kcat[\"Substrates\"][ind].split(';')[:-1]):\n",
    "    #         if substrate in s or s in substrate:\n",
    "    #             id = list(df_kcat['Substrate_IDs'][ind])[i]\n",
    "    if id[0] == \"C\":\n",
    "        try:\n",
    "            mol = Chem.MolFromMolFile(join(datasets_dir,\"mol-files\", id + '.mol'))\n",
    "        except OSError:\n",
    "            None\n",
    "    else:\n",
    "        try:\n",
    "            mol = Chem.inchi.MolFromInchi(id,sanitize=False)\n",
    "        except OSError:\n",
    "            None\n",
    "    if mol is not None:\n",
    "        maccs_fp = MACCSkeys.GenMACCSKeys(mol).ToBitString()\n",
    "        df_kcat[\"MACCS FP\"][ind] = maccs_fp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating the maximal kcat value for every EC number in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_EC_kcat = pd.read_csv(join(datasets_dir, \"max_EC_\" + organism + \".tsv\"), sep = \"\\t\", header=0)\n",
    "# df_EC_kcat = df_EC_kcat.rename(columns={0: \"EC\", 1: \"max_kcat\"})\n",
    "\n",
    "for ind in df_EC_kcat.index:\n",
    "    try:\n",
    "        kcat_max = df_EC_kcat[df_EC_kcat[\"EC\"] == df_kcat[\"ECs\"]][\"max_kcat\"]\n",
    "        df_EC_kcat[\"max_kcat\"][ind] = kcat_max\n",
    "        print(ind, kcat_max)\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "df_EC_kcat.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_EC_kcat = pd.read_csv(join(datasets_dir, \"max_EC_\" + organism + \".tsv\"), sep = \"\\t\", header=0)\n",
    "\n",
    "df_EC_kcat.head(5)\n",
    "df_kcat[\"max_kcat_for_EC\"] = np.nan\n",
    "\n",
    "for ind in df_kcat.index:\n",
    "    EC = df_kcat[\"ECs\"][ind]\n",
    "    max_kcat = 0\n",
    "    try:\n",
    "        print(EC)\n",
    "        max_kcat = df_EC_kcat.loc[df_EC_kcat[\"EC\"] == EC, \"max_kcat\"].iloc[0]\n",
    "        print(max_kcat)\n",
    "    except:\n",
    "        pass\n",
    "    if max_kcat != 0:\n",
    "        df_kcat[\"max_kcat_for_EC\"][ind] = max_kcat\n",
    "df_kcat.to_pickle(join(datasets_dir, \"merged_and_grouped_kcat_dataset.pkl\"))     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Removing outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing non-optimally measured values\n",
    "\n",
    "To ignore $kcat$ values that were obtained under non-optimal conditions, we exclude values lower than 0.1\\% than the maximal $kcat$ value for the same enzyme, reaction or EC number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kcat[\"frac_of_max_UID\"] = np.nan\n",
    "df_kcat[\"frac_of_max_RID\"] = np.nan\n",
    "df_kcat[\"frac_of_max_EC\"] = np.nan\n",
    "\n",
    "for ind in df_kcat.index:\n",
    "    df_kcat[\"frac_of_max_UID\"][ind] =  df_kcat[\"kcat\"][ind]/df_kcat[\"max_kcat_for_UID\"][ind]\n",
    "    df_kcat[\"frac_of_max_RID\"][ind] =  df_kcat[\"kcat\"][ind]/df_kcat[\"max_kcat_for_RID\"][ind]\n",
    "    df_kcat[\"frac_of_max_EC\"][ind] = df_kcat[\"kcat\"][ind]/df_kcat[\"max_kcat_for_EC\"][ind]\n",
    "\n",
    "len(df_kcat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(df_kcat)\n",
    "\n",
    "df_kcat = df_kcat.loc[df_kcat[\"frac_of_max_UID\"] >= 0.01]\n",
    "df_kcat = df_kcat.loc[df_kcat[\"frac_of_max_RID\"] >= 0.01]\n",
    "\n",
    "df_kcat[\"frac_of_max_EC\"].loc[pd.isnull(df_kcat[\"frac_of_max_EC\"])] = 1\n",
    "df_kcat = df_kcat.loc[df_kcat[\"frac_of_max_EC\"] <= 10]\n",
    "df_kcat = df_kcat.loc[df_kcat[\"frac_of_max_EC\"] >= 0.01]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"We remove %s data points, because we suspect that these kcat values were not measure for the natural reaction \" \\\n",
    "    \"of an enzyme or under non-optimal conditions.\" % (n-len(df_kcat)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing data points with reaction queations with uneven fraction of molecular weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(df_kcat)\n",
    "\n",
    "df_kcat = df_kcat.loc[df_kcat[\"MW_frac\"] < 3]\n",
    "df_kcat = df_kcat.loc[df_kcat[\"MW_frac\"] > 1/3]\n",
    "\n",
    "print(\"We remove %s data points because the sum of molecular weights of substrates does not match the sum of molecular\" \\\n",
    "      \"weights of the products.\" % (n-len(df_kcat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Size of final kcat dataset: %s\" % len(df_kcat))\n",
    "df_kcat.to_pickle(join(datasets_dir, \"final_kcat_dataset_\" + organism + \".pkl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Preparing dataset and splitting into train-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kcat = pd.read_pickle(join(datasets_dir, \"final_kcat_dataset_\" + organism + \".pkl\"))\n",
    "# df_kcat[\"log10_kcat\"] = [np.log10(x) for x in df_kcat[\"kcat\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import plotnine\n",
    "# from plotnine import ggplot, geom_point, aes, theme_matplotlib, theme_set, geom_bin_2d\n",
    "# # df_kcat['Temperature'] = df_kcat['Temperature'].replace('-', np.nan)\n",
    "# # df_kcat['pH'] = df_kcat['pH'].replace('-', np.nan)\n",
    "# # df_kcat['pH'] = df_kcat['pH'].astype('float')\n",
    "# # df_kcat['Temperature'] = df_kcat['Temperature'].astype('float')\n",
    "# # df_kcat[\"log10_kcat\"] = np.log10(df_kcat[\"kcat\"])\n",
    "# theme_set(theme_matplotlib())\n",
    "# (\n",
    "#     ggplot(df_kcat) +\n",
    "#     aes(x=\"Temperature\",y=\"kcat\") +\n",
    "#     geom_bin_2d()\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(7, 4))\n",
    "# hb = ax.hexbin(df_kcat[\"Temperature\"], df_kcat[\"kcat\"], gridsize=50, cmap='inferno')\n",
    "\n",
    "\n",
    "# # ax.set_xlim(x.min(), x.max())\n",
    "# # ax.set_ylim(y.min(), y.max())\n",
    "\n",
    "# # Add a title and colorbar\n",
    "# ax.set_title(\"Hexagon binning\")\n",
    "# fig.colorbar(hb, ax=ax, label='counts')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making input for GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for ind in df_kcat.index:\n",
    "#     substrate = df_kcat[\"Main Substrate\"][ind]\n",
    "#     try:\n",
    "#         id = list(df_kcat['substrate_IDs'][ind])[df_kcat[\"Substrates\"][ind].split(';').index(substrate)]\n",
    "#     except:\n",
    "#         for i,s in enumerate(df_kcat[\"Substrates\"][ind].split(';')[:-1]):\n",
    "#             if substrate in s or s in substrate:\n",
    "#                 id = list(df_kcat['substrate_IDs'][ind])[i]\n",
    "#     df_kcat[\"Main Substrate\"][ind] = id\n",
    "\n",
    "\n",
    "inchi_ids = {}\n",
    "for i, element in enumerate(df_kcat[\"Main Substrate\"]):\n",
    "    if element[0] != 'C' and element not in inchi_ids.keys():\n",
    "        inchi_ids[element] = str(i)\n",
    "        # mol = Chem.inchi.MolFromInchi(element)\n",
    "        # if not mol is None:\n",
    "        #     calculate_atom_and_bond_feature_vectors(mol, str(i))\n",
    "        # Chem.rdmolfiles.MolToMolFile(Chem.inchi.MolFromInchi(element), join(datasets_dir,\"mol-files\", str(i) + \".mol\")  )  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting glucosinolates into validation dataset\n",
    "\n",
    "Search UniProt for GO term related to glucosionalte metabolic process, download file as .tsv and filter dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "glucosinolates = pd.read_table(join(datasets_dir,\"glucosinolates.tsv\"))[\"Entry\"].tolist()\n",
    "df_validation = df_kcat[df_kcat[\"Uniprot IDs\"].isin(glucosinolates)]\n",
    "df_validation.reset_index(inplace=True, drop = True)\n",
    "df_kcat = df_kcat[~df_kcat[\"Uniprot IDs\"].isin(glucosinolates)]\n",
    "df_kcat.reset_index(inplace=True, drop = True)\n",
    "split = \"full\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If training-testing with only Arabidopsis data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_kcat = df_kcat[df_kcat[\"Organism\"] == 'Arabidopsis thaliana']\n",
    "# df_kcat.reset_index(inplace=True, drop = True)\n",
    "# split = \"Arabidopsis\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If training-testing with only Brassicaceae data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ncbi = NCBITaxa()\n",
    "\n",
    "# organisms = {}\n",
    "\n",
    "# def is_brassicaceae(org):\n",
    "#     try:\n",
    "#         tax_id = ncbi.get_name_translator([org])[org][0]\n",
    "#         lineage = ncbi.get_lineage(tax_id)\n",
    "#         if 3700 not in lineage:\n",
    "#             return(False)\n",
    "#         else:\n",
    "#             return(True)\n",
    "#     except KeyError:\n",
    "#         return(False)\n",
    "    \n",
    "# for org in df_kcat[\"Organism\"].tolist():\n",
    "#     if org not in organisms.keys():\n",
    "#         organisms[org] = is_brassicaceae(org)\n",
    "\n",
    "# df_kcat = df_kcat[df_kcat[\"Organism\"].isin([key for key, value in organisms.items() if value is True])]\n",
    "# df_kcat.reset_index(inplace=True, drop = True)\n",
    "# split = \"Brassicaceae\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If training-testing only with wildtype data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_kcat = df_kcat[df_kcat[\"Type\"].str.contains(\"wildtype\")]\n",
    "# df_kcat.reset_index(inplace=True, drop = True)\n",
    "# split = \"wildtype\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If training-testing only with secondary metabolite data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# secondary = pd.read_table(join(datasets_dir,\"secondary_metabolites.tsv\"))[\"Entry\"].tolist()\n",
    "# df_kcat = df_kcat[df_kcat[\"Uniprot IDs\"].isin(secondary)]\n",
    "# df_kcat.reset_index(inplace=True, drop = True)\n",
    "# split = \"secondary\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.mkdir(join(datasets_dir, \"splits\", split))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating arithmetic mean for kcat values of same enzyme-reaction-substrate combination-pH-temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_new = pd.DataFrame(data = {\"Reaction ID\" : df_kcat[\"Reaction ID\"],\n",
    "#                                   \"Sequence ID\" : df_kcat[\"Sequence ID\"],\n",
    "#                                   \"Temperature\" : df_kcat[\"Temperature\"],\n",
    "#                                     \"pH\" : df_kcat[\"pH\"],\n",
    "#                                  \"Type\": df_kcat[\"Type\"],\n",
    "#                              \"MACCS FP\" : df_kcat[\"MACCS FP\"]})\n",
    "\n",
    "# df_new.drop_duplicates(inplace = True)\n",
    "# df_new.reset_index(inplace = True, drop = True)\n",
    "\n",
    "# df_new[\"kcat_values\"], df_new[\"Uniprot IDs\"], df_new[\"ECs\"], df_new[\"Substrates\"], df_new[\"Products\"], df_new[\"ESM2\"], df_new[\"Sequence\"], df_new[\"difference_fp\"], df_new[\"structural_fp\"] = \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"\n",
    "\n",
    "# for ind in df_new.index:\n",
    "#     RID, SID, Temp, pH, Type, MSubstrate = df_new[\"Reaction ID\"][ind], df_new[\"Sequence ID\"][ind], df_new[\"Temperature\"][ind], df_new[\"pH\"][ind], df_new[\"Type\"][ind], df_new[\"MACCS FP\"][ind]\n",
    "#     help_df = df_kcat.loc[df_kcat[\"Reaction ID\"] \n",
    "#                                  == RID].loc[df_kcat[\"Sequence ID\"] \n",
    "#                                              == SID].loc[df_kcat[\"Temperature\"] \n",
    "#                                                          == Temp].loc[df_kcat[\"pH\"] \n",
    "#                                                                       == pH].loc[df_kcat[\"Type\"] \n",
    "#                                                                                  == Type].loc[df_kcat[\"MACCS FP\"] \n",
    "#                                                                                               == MSubstrate]\n",
    "#     print(help_df)\n",
    "#     df_new[\"ECs\"][ind] = list(help_df[\"ECs\"])\n",
    "#     df_new[\"kcat_values\"][ind] = list(help_df[\"kcat\"])\n",
    "#     df_new[\"Uniprot IDs\"][ind] = list(help_df[\"Uniprot IDs\"])\n",
    "#     df_new[\"Sequence\"][ind] = help_df[\"Sequence\"].values[0]\n",
    "#     df_new[\"ESM2\"][ind] = help_df[\"Enzyme rep\"].values[0]\n",
    "#     df_new[\"difference_fp\"][ind], df_new[\"structural_fp\"][ind] = help_df[\"difference_fp\"].values[0], help_df[\"structural_fp\"].values[0]\n",
    "#     df_new[\"Substrates\"][ind], df_new[\"Products\"][ind] = help_df[\"Substrates\"].values[0], help_df[\"Products\"].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_new2 = pd.DataFrame(data = {\"Reaction ID\" : df_validation[\"Reaction ID\"],\n",
    "#                                   \"Sequence ID\" : df_validation[\"Sequence ID\"],\n",
    "#                                   \"Temperature\" : df_validation[\"Temperature\"],\n",
    "#                                     \"pH\" : df_validation[\"pH\"],\n",
    "#                                   \"Type\" : df_validation[\"Type\"],\n",
    "#                                   \"MACCS FP\" : df_validation[\"MACCS FP\"]})\n",
    "\n",
    "# df_new2.drop_duplicates(inplace = True)\n",
    "# df_new2.reset_index(inplace = True, drop = True)\n",
    "\n",
    "# df_new2[\"kcat_values\"], df_new2[\"Uniprot IDs\"], df_new2[\"ECs\"], df_new2[\"Organisms\"], df_new2[\"Substrates\"], df_new2[\"Products\"], df_new2[\"ESM2\"], df_new2[\"Sequence\"], df_new2[\"difference_fp\"], df_new2[\"structural_fp\"] = \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"\n",
    "\n",
    "# for ind in df_new2.index:\n",
    "#     RID, SID, Temp, pH, Type, MSubstrate = df_new2[\"Reaction ID\"][ind], df_new2[\"Sequence ID\"][ind], df_new2[\"Temperature\"][ind], df_new2[\"pH\"][ind], df_new2[\"Type\"][ind], df_new2[\"MACCS FP\"][ind]\n",
    "#     help_df = df_validation.loc[df_validation[\"Reaction ID\"] \n",
    "#                               == RID].loc[df_validation[\"Sequence ID\"] \n",
    "#                                           == SID].loc[df_validation[\"Temperature\"] \n",
    "#                                                       == Temp].loc[df_validation[\"pH\"] \n",
    "#                                                                     == pH].loc[df_validation[\"Type\"] \n",
    "#                                                                               == Type].loc[df_validation[\"MACCS FP\"] \n",
    "#                                                                                                             == MSubstrate]\n",
    "#     df_new2[\"ECs\"][ind] = list(help_df[\"ECs\"])\n",
    "#     df_new2[\"kcat_values\"][ind] = list(help_df[\"kcat\"])\n",
    "#     df_new2[\"Uniprot IDs\"][ind] = list(help_df[\"Uniprot IDs\"])\n",
    "#     df_new2[\"Organisms\"][ind] = list(help_df[\"Organism\"])\n",
    "#     df_new2[\"Type\"][ind]\n",
    "#     df_new2[\"Sequence\"][ind] = help_df[\"Sequence\"].values[0]\n",
    "#     df_new2[\"ESM2\"][ind] = help_df[\"Enzyme rep\"].values[0]\n",
    "#     df_new2[\"difference_fp\"][ind], df_new2[\"structural_fp\"][ind] = help_df[\"difference_fp\"].values[0], help_df[\"structural_fp\"].values[0]\n",
    "#     df_new2[\"Substrates\"][ind], df_new2[\"Products\"][ind] = help_df[\"Substrates\"].values[0], help_df[\"Products\"].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_kcat = df_new\n",
    "# df_validation = df_new2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_kcat[\"geomean_kcat\"] = np.nan\n",
    "# for ind in df_kcat.index:\n",
    "#     all_kcat = np.array(df_kcat[\"kcat_values\"][ind]).astype(float)\n",
    "#     max_kcat = max(all_kcat)\n",
    "#     all_kcat_top = [kcat for kcat in all_kcat  if kcat/max_kcat >= 0.01]\n",
    "#     df_kcat[\"geomean_kcat\"][ind] = np.mean((all_kcat_top))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_validation[\"geomean_kcat\"] = np.nan\n",
    "# for ind in df_validation.index:\n",
    "#     all_kcat = np.array(df_validation[\"kcat_values\"][ind]).astype(float)\n",
    "#     max_kcat = max(all_kcat)\n",
    "#     all_kcat_top = [kcat for kcat in all_kcat  if kcat/max_kcat >= 0.01]\n",
    "#     df_validation[\"geomean_kcat\"][ind] = np.mean((all_kcat_top))\n",
    "\n",
    "# df_validation.to_pickle(join(datasets_dir,\"splits\", split, \"validation_%s.pkl\" %organism))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting into train-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set size: 36\n",
      "Training set size: 105\n",
      "Size of test set in percent: 26.0\n"
     ]
    }
   ],
   "source": [
    "df = df_kcat.copy()\n",
    "df = df.sample(frac = 1, random_state=123)\n",
    "df.reset_index(drop= True, inplace = True)\n",
    "\n",
    "train_df, test_df = split_dataframe_enzyme(frac = 5, df = df.copy())\n",
    "print(\"Test set size: %s\" % len(test_df))\n",
    "print(\"Training set size: %s\" % len(train_df))\n",
    "print(\"Size of test set in percent: %s\" % np.round(100*len(test_df)/ (len(test_df) + len(train_df))))\n",
    "\n",
    "train_df.reset_index(inplace = True, drop = True)\n",
    "test_df.reset_index(inplace = True, drop = True)\n",
    "\n",
    "train_df.to_pickle(join(datasets_dir, \"splits\", split, \"train_df_kcat_%s.pkl\" %organism))\n",
    "test_df.to_pickle(join(datasets_dir, \"splits\", split, \"test_df_kcat_%s.pkl\" %organism))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting CV folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80 25\n",
      "61 19\n",
      "41 20\n",
      "23 18\n"
     ]
    }
   ],
   "source": [
    "data_train2 = train_df.copy()\n",
    "data_train2[\"index\"] = list(data_train2.index)\n",
    "\n",
    "data_train2, df_fold = split_dataframe_enzyme(df = data_train2, frac=5)\n",
    "indices_fold1 = list(df_fold[\"index\"])\n",
    "print(len(data_train2), len(indices_fold1))#\n",
    "\n",
    "data_train2, df_fold = split_dataframe_enzyme(df = data_train2, frac=4)\n",
    "indices_fold2 = list(df_fold[\"index\"])\n",
    "print(len(data_train2), len(indices_fold2))\n",
    "\n",
    "data_train2, df_fold = split_dataframe_enzyme(df = data_train2, frac=3)\n",
    "indices_fold3 = list(df_fold[\"index\"])\n",
    "print(len(data_train2), len(indices_fold3))\n",
    "\n",
    "data_train2, df_fold = split_dataframe_enzyme(df = data_train2, frac=2)\n",
    "indices_fold4 = list(df_fold[\"index\"])\n",
    "indices_fold5 = list(data_train2[\"index\"])\n",
    "print(len(data_train2), len(indices_fold4))\n",
    "\n",
    "\n",
    "fold_indices = [indices_fold1, indices_fold2, indices_fold3, indices_fold4, indices_fold5]\n",
    "\n",
    "CV_train_indices = [[], [], [], [], []]\n",
    "CV_test_indices = [[], [], [], [], []]\n",
    "\n",
    "for i in range(5):\n",
    "    for j in range(5):\n",
    "        if i != j:\n",
    "            CV_train_indices[i] = CV_train_indices[i] + fold_indices[j]\n",
    "    CV_test_indices[i] = fold_indices[i]\n",
    "    \n",
    "    \n",
    "np.save(join(datasets_dir, \"splits\", split, \"CV_train_indices_%s\" %organism), CV_train_indices)\n",
    "np.save(join(datasets_dir, \"splits\", split, \"CV_test_indices_%s\" %organism), CV_test_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Building GNN for substrate representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.mkdir(join(datasets_dir, \"GNN_input_data\", split))\n",
    "\n",
    "for ind in train_df.index:\n",
    "    calculate_and_save_input_matrixes(inchi_ids, sample_ID = \"train_\" + str(ind), df = train_df,\n",
    "                                      save_folder = join(datasets_dir, \"GNN_input_data\", split))\n",
    "    \n",
    "for ind in test_df.index:\n",
    "    calculate_and_save_input_matrixes(inchi_ids, sample_ID = \"test_\" + str(ind), df = test_df,\n",
    "                                      save_folder = join(datasets_dir, \"GNN_input_data\", split))\n",
    "    \n",
    "for ind in df_validation.index:\n",
    "    calculate_and_save_input_matrixes(inchi_ids, sample_ID = \"val_\" + str(ind), df = df_validation,\n",
    "                                    save_folder = join(datasets_dir, \"GNN_input_data\", split))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices = os.listdir(join(datasets_dir, \"GNN_input_data\", split))\n",
    "train_indices = [index[:index.rfind(\"_\")] for index in train_indices]\n",
    "train_indices = list(set([index for index in train_indices if \"train\" in index]))\n",
    "\n",
    "test_indices = os.listdir(join(datasets_dir, \"GNN_input_data\", split))\n",
    "test_indices = [index[:index.rfind(\"_\")] for index in test_indices]\n",
    "test_indices = list(set([index for index in test_indices if \"test\" in index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyper-parameter optimization with CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'batch_size': [32,64,96],\n",
    "                'D': [50,100],\n",
    "                'learning_rate': [0.01, 0.1],\n",
    "                'epochs': [30,50,80],\n",
    "                'l2_reg_fc' : [0.01, 0.1, 1],\n",
    "                'l2_reg_conv': [0.01, 0.1, 1],\n",
    "                'rho': [0.9, 0.95, 0.99]}\n",
    "\n",
    "params_list = [(batch_size, D, learning_rate, epochs, l2_reg_fc, l2_reg_conv, rho) for batch_size in param_grid['batch_size'] for D in param_grid[\"D\"] for learning_rate in param_grid['learning_rate']\n",
    "                for epochs in param_grid['epochs'] for l2_reg_fc in param_grid['l2_reg_fc'] for l2_reg_conv in param_grid['l2_reg_conv'] for rho in param_grid[\"rho\"]]\n",
    "\n",
    "params_list = random.sample(params_list, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count = 0\n",
    "# results=[]\n",
    "\n",
    "# for params in params_list:\n",
    "\n",
    "#     batch_size, D, learning_rate, epochs, l2_reg_fc, l2_reg_conv, rho = params\n",
    "#     count +=1\n",
    "#     MAE = []\n",
    "\n",
    "#     for i in range(5):\n",
    "#         train_index, test_index  = CV_train_indices[i], CV_test_indices[i]\n",
    "#         train_index = [ind for ind in train_indices if int(ind.split(\"_\")[1]) in train_index]\n",
    "#         test_index = [ind for ind in train_indices if int(ind.split(\"_\")[1]) in test_index]\n",
    "\n",
    "#         train_params = {'batch_size': batch_size,\n",
    "#                     'folder' :join(datasets_dir, \"GNN_input_data/full\"),\n",
    "#                     'list_IDs' : np.array(train_index),\n",
    "#                     'shuffle': True}\n",
    "\n",
    "#         test_params = {'batch_size': len(test_index),\n",
    "#                     'folder' : join(datasets_dir, \"GNN_input_data/full\"),\n",
    "#                     'list_IDs' : np.array(test_index),\n",
    "#                     'shuffle': False}\n",
    "\n",
    "#         training_generator = DataGenerator(**train_params)\n",
    "#         test_generator = DataGenerator(**test_params)\n",
    "\n",
    "\n",
    "#         model = DMPNN_without_extra_features(l2_reg_conv = l2_reg_conv, l2_reg_fc = l2_reg_fc, learning_rate = learning_rate,\n",
    "#                         D = D, N = N, F1 = F1, F2 = F2, F= F, drop_rate = 0.0, ada_rho = rho)\n",
    "#         model.fit(training_generator, epochs= epochs, shuffle = True, verbose = 1)\n",
    "\n",
    "#         #get test_y:\n",
    "#         test_indices_y = [int(ind.split(\"_\")[1]) for ind in train_indices if ind in test_index]\n",
    "#         test_y = np.array([train_df[\"kcat\"][ind] for ind in test_indices_y])\n",
    "\n",
    "#         pred_test = model.predict(test_generator)\n",
    "#         mae = np.median(abs(np.array([10**x for x in pred_test]) - np.reshape(test_y[:len(pred_test)], (-1,1))))\n",
    "#         print(mae)\n",
    "#         MAE.append(mae)\n",
    "\n",
    "#     results.append({\"batch_size\" : batch_size, \"D\" : D , \"learning_rate\" : learning_rate, \"epochs\" : epochs,\n",
    "#                     \"l2_reg_fc\" : l2_reg_fc, \"l2_reg_conv\" : l2_reg_conv, \"rho\" : rho, \"cv_mae\" : np.mean(MAE)})\n",
    "\n",
    "# params = min(results, key=lambda d: d['cv_mae'])\n",
    "# print(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{'batch_size': 32, 'D': 50, 'learning_rate': 0.01, 'epochs': 30, 'l2_reg_fc': 0.1, 'l2_reg_conv': 1, 'rho': 0.9, 'cv_mae': 2.4853503725624084}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the model with the best set of hyperparmeters on the whole training set and validate it on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "D = 50\n",
    "learning_rate = 0.01\n",
    "epochs = 30\n",
    "l2_reg_fc = 0.1\n",
    "l2_reg_conv = 1\n",
    "rho = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_indices = os.listdir(join(datasets_dir, \"GNN_input_data/full\"))\n",
    "# train_indices = [index[:index.rfind(\"_\")] for index in train_indices]\n",
    "# train_indices = list(set([index for index in train_indices if \"train\" in index]))\n",
    "\n",
    "# test_indices = os.listdir(join(datasets_dir, \"GNN_input_data/full\"))\n",
    "# test_indices = [index[:index.rfind(\"_\")] for index in test_indices]\n",
    "# test_indices = list(set([index for index in test_indices if \"test\" in index]))\n",
    "\n",
    "# train_params = {'batch_size': batch_size,\n",
    "#               'folder' :join(datasets_dir, \"GNN_input_data/full\"),\n",
    "#               'list_IDs' : train_indices,\n",
    "#               'shuffle': True}\n",
    "\n",
    "# test_params = {'batch_size': batch_size,\n",
    "#               'folder' :join(datasets_dir, \"GNN_input_data/full\"),\n",
    "#               'list_IDs' : test_indices,\n",
    "#               'shuffle': False}\n",
    "\n",
    "# training_generator = DataGenerator(**train_params)\n",
    "# test_generator = DataGenerator(**test_params)\n",
    "\n",
    "# model = DMPNN_without_extra_features(l2_reg_conv = l2_reg_conv, l2_reg_fc = l2_reg_fc, learning_rate = learning_rate,\n",
    "#                   D = D, N = N, F1 = F1, F2 = F2, F= F, drop_rate = 0.0, ada_rho = rho)\n",
    "\n",
    "# model.fit(training_generator, epochs= epochs, shuffle = True, verbose = 1)\n",
    "# model.save_weights(join(datasets_dir, \"model_weights\", \"saved_model_GNN_best_hyperparameters\"))\n",
    "\n",
    "# pred_test = model.predict(test_generator)\n",
    "# test_indices_y = [int(ind.split(\"_\")[1]) for ind in np.array(test_indices)]\n",
    "# test_y = np.array([test_df[\"kcat\"][ind] for ind in test_indices_y])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating substrate representation for every data point in training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.decay\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.rho\n"
     ]
    }
   ],
   "source": [
    "model = DMPNN_without_extra_features(l2_reg_conv = l2_reg_conv, l2_reg_fc = l2_reg_fc, learning_rate = learning_rate,\n",
    "                  D = D, N = N, F1 = F1, F2 = F2, F= F, drop_rate = 0.0, ada_rho = rho)\n",
    "model.load_weights(join(datasets_dir, \"model_weights\", \"saved_model_GNN_best_hyperparameters\"))\n",
    "\n",
    "get_fingerprint_fct = K.function([model.layers[0].input, model.layers[26].input,\n",
    "                                  model.layers[3].input],\n",
    "                                  [model.layers[-10].output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_folder = join(datasets_dir, \"GNN_input_data\", split)   \n",
    "\n",
    "def get_representation_input(cid_list):\n",
    "    XE = ();\n",
    "    X = ();\n",
    "    A = ();\n",
    "    # Generate data\n",
    "    for cid in cid_list:\n",
    "        try:\n",
    "            X = X + (np.load(join(input_data_folder, cid + '_X.npy')), );\n",
    "            XE = XE + (np.load(join(input_data_folder, cid + '_XE.npy')), );\n",
    "            A = A + (np.load(join(input_data_folder, cid + '_A.npy')), );\n",
    "        except FileNotFoundError: #return zero arrays:\n",
    "            X = X + (np.zeros((N,32)), );\n",
    "            XE = XE + (np.zeros((N,N,F)), );\n",
    "            A = A + (np.zeros((N,N,1)), );\n",
    "    return(XE, X, A)\n",
    "\n",
    "input_data_folder = join(datasets_dir, \"GNN_input_data\", split)   \n",
    "def get_substrate_representations(df, training_set, testing_set, get_fingerprint_fct):\n",
    "    df[\"GNN FP\"] = \"\"\n",
    "    i = 0\n",
    "    n = len(df)\n",
    "    \n",
    "    cid_all = list(df.index)\n",
    "    if training_set == True:\n",
    "        prefix = \"train_\"\n",
    "    elif testing_set == True:\n",
    "        prefix = \"test_\"\n",
    "    else:\n",
    "        prefix = \"val_\"\n",
    "    cid_all = [prefix + str(cid) for cid in cid_all]\n",
    "    \n",
    "    while i*32 <= n:\n",
    "        if (i+1)*32  <= n:\n",
    "            XE, X, A = get_representation_input(cid_all[i*32:(i+1)*32])\n",
    "            representations = get_fingerprint_fct([np.array(XE), np.array(X),np.array(A)])[0]\n",
    "            df[\"GNN FP\"][i*32:(i+1)*32] = list(representations[:, :52])\n",
    "        else:\n",
    "            print(i)\n",
    "            XE, X, A = get_representation_input(cid_all[-min(32,n):])\n",
    "            representations = get_fingerprint_fct([np.array(XE), np.array(X),np.array(A)])[0]\n",
    "            df[\"GNN FP\"][-min(32,n):] = list(representations[:, :52])\n",
    "        i += 1\n",
    "        \n",
    "    ### set all GNN FP-entries with no input matrices to np.nan:\n",
    "    all_X_matrices = os.listdir(input_data_folder)\n",
    "    for ind in df.index:\n",
    "        if prefix +str(ind) +\"_X.npy\" not in all_X_matrices:\n",
    "            df[\"GNN FP\"][ind] = np.nan\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "#Calculating the GNN representations\n",
    "train_with_rep = get_substrate_representations(df = train_df, training_set = True, testing_set = False,\n",
    "                                                      get_fingerprint_fct = get_fingerprint_fct)\n",
    "test_with_rep = get_substrate_representations(df = test_df, training_set = False, testing_set = True,\n",
    "                                                     get_fingerprint_fct = get_fingerprint_fct)\n",
    "val_with_rep = get_substrate_representations(df = df_validation, training_set = False, testing_set = False,\n",
    "                                                     get_fingerprint_fct = get_fingerprint_fct)\n",
    "\n",
    "#Saving the DataFrames:\n",
    "train_with_rep.to_pickle(join(datasets_dir, \"splits\", split, \"training_data.pkl\"))\n",
    "test_with_rep.to_pickle(join(datasets_dir, \"splits\", split, \"test_data.pkl\"))\n",
    "val_with_rep.to_pickle(join(datasets_dir, \"splits\", split, \"val_data.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_sequences.drop(\"model_input\", axis=1, inplace=True)\n",
    "\n",
    "# for split in [\"full\", \"Arabidopsis\", \"Brassicaceae\", \"wildtype\", \"secondary\"]:\n",
    "#     train_with_rep = pd.read_pickle(join(datasets_dir, \"splits\", split, \"training_data.pkl\"))\n",
    "#     test_with_rep = pd.read_pickle(join(datasets_dir, \"splits\", split, \"test_data.pkl\"))\n",
    "#     val_with_rep = pd.read_pickle(join(datasets_dir, \"splits\", split, \"val_data.pkl\"))\n",
    "\n",
    "#     train_with_rep = train_with_rep.merge(df_sequences, on = \"Sequence\", how = \"left\")\n",
    "#     test_with_rep = test_with_rep.merge(df_sequences, on = \"Sequence\", how = \"left\")\n",
    "#     val_with_rep = val_with_rep.merge(df_sequences, on = \"Sequence\", how = \"left\")\n",
    "\n",
    "#     train_with_rep.to_pickle(join(datasets_dir, \"splits\", split, \"training_data.pkl\"))\n",
    "#     test_with_rep.to_pickle(join(datasets_dir, \"splits\", split, \"test_data.pkl\"))\n",
    "#     val_with_rep.to_pickle(join(datasets_dir, \"splits\", split, \"val_data.pkl\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
