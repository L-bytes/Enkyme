{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import os\n",
    "from os.path import join\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy import stats\n",
    "import xgboost as xgb\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import matplotlib as mpl\n",
    "# plt.style.use('CCB_plot_style_0v4.mplstyle')\n",
    "# c_styles      = mpl.rcParams['axes.prop_cycle'].by_key()['color']   # fetch the defined color styles\n",
    "# high_contrast = ['#004488', '#DDAA33', '#BB5566', '#000000']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading training and test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(334, 69)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train = pd.read_pickle(join(\"..\", \"..\", \"data\", \"train_df_kcat_Seed plants.pkl\"))\n",
    "data_test = pd.read_pickle(join(\"..\", \"..\", \"data\", \"test_df_kcat_Seed plants.pkl\"))\n",
    "\n",
    "data_train[\"geomean_kcat\"] = np.log10(data_train[\"geomean_kcat\"])\n",
    "data_test[\"geomean_kcat\"] = np.log10(data_test[\"geomean_kcat\"])\n",
    "\n",
    "data_train.rename(columns = {\"geomean_kcat\" :\"log10_kcat\"}, inplace = True)\n",
    "data_test.rename(columns = {\"geomean_kcat\" :\"log10_kcat\"}, inplace = True)\n",
    "len(data_train), len(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices = list(np.load(join(\"..\", \"..\", \"data\", \"CV_train_indices_Seed plants.npy\"), allow_pickle = True))\n",
    "test_indices = list(np.load(join(\"..\", \"..\", \"data\", \"CV_test_indices_Seed plants.npy\"), allow_pickle = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Training a model with only sequence information (ESM-1b):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Creating input matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ESM2 = np.array(list(data_train[\"ESM2\"]))\n",
    "train_X = train_ESM2\n",
    "train_Y = np.array(list(data_train[\"log10_kcat\"]))\n",
    "\n",
    "test_ESM2 = np.array(list(data_test[\"ESM2\"]))\n",
    "test_X = test_ESM2\n",
    "test_Y = np.array(list(data_test[\"log10_kcat\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Hyperparameter optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def cross_validation_mse_gradient_boosting(param):\n",
    "    num_round = param[\"num_rounds\"]\n",
    "    del param[\"num_rounds\"]\n",
    "    param[\"max_depth\"] = int(np.round(param[\"max_depth\"]))\n",
    "    param[\"tree_method\"] = \"gpu_hist\"\n",
    "    param[\"sampling_method\"] = \"gradient_based\"\n",
    "    \n",
    "    MSE = []\n",
    "    R2 = []\n",
    "    for i in range(5):\n",
    "        train_index, test_index  = train_indices[i], test_indices[i]\n",
    "        dtrain = xgb.DMatrix(train_X[train_index], label = train_Y[train_index])\n",
    "        dvalid = xgb.DMatrix(train_X[test_index])\n",
    "        bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "        y_valid_pred = bst.predict(dvalid)\n",
    "        MSE.append(np.mean(abs(np.reshape(train_Y[test_index], (-1)) - y_valid_pred)**2))\n",
    "        R2.append(r2_score(np.reshape(train_Y[test_index], (-1)),  y_valid_pred))\n",
    "    return(-np.mean(R2))\n",
    "\n",
    "\n",
    "from hyperopt import fmin, tpe, rand, hp, Trials\n",
    "\n",
    "space_gradient_boosting = {\n",
    "    \"learning_rate\": hp.uniform(\"learning_rate\", 0.01, 1),\n",
    "    \"max_depth\": hp.uniform(\"max_depth\", 4,12),\n",
    "    #\"subsample\": hp.uniform(\"subsample\", 0.7, 1),\n",
    "    \"reg_lambda\": hp.uniform(\"reg_lambda\", 0, 5),\n",
    "    \"reg_alpha\": hp.uniform(\"reg_alpha\", 0, 5),\n",
    "    \"max_delta_step\": hp.uniform(\"max_delta_step\", 0, 5),\n",
    "    \"min_child_weight\": hp.uniform(\"min_child_weight\", 0.1, 15),\n",
    "    \"num_rounds\":  hp.uniform(\"num_rounds\", 20, 200)}\n",
    "\n",
    "\n",
    "trials = Trials()\n",
    "best = fmin(fn = cross_validation_mse_gradient_boosting, space = space_gradient_boosting,\n",
    "            algo=rand.suggest, max_evals = 200, trials=trials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c) Training and validating model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {'learning_rate': 0.051447544749765035,\n",
    "         'max_delta_step': 2.956459783615207,\n",
    "         'max_depth': 5.034202474908222,\n",
    "         'min_child_weight': 7.457989829577018,\n",
    "         'num_rounds': 297.50601395689256,\n",
    "         'reg_alpha': 1.0858835704466614, \n",
    "         'reg_lambda': 1.1385559144302175}\n",
    "\n",
    "num_round = param[\"num_rounds\"]\n",
    "param[\"max_depth\"] = int(np.round(param[\"max_depth\"]))\n",
    "\n",
    "del param[\"num_rounds\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6156521084065656, 0.592768921573751, 0.5450373027741603, 0.6635468722921141, 0.5465250301214234]\n",
      "[0.8251322874089426, 0.8206497271502332, 0.9401031382230501, 0.9269849644397952, 1.061793264104686]\n",
      "[0.3720653400563064, 0.34881598167570627, 0.2894002515317964, 0.4219034383575203, 0.2977699878192147]\n"
     ]
    }
   ],
   "source": [
    "R2 = []\n",
    "MSE = []\n",
    "Pearson = []\n",
    "\n",
    "for i in range(5):\n",
    "    train_index, test_index  = train_indices[i], test_indices[i]\n",
    "    dtrain = xgb.DMatrix(train_X[train_index], label = train_Y[train_index])\n",
    "    dvalid = xgb.DMatrix(train_X[test_index])\n",
    "    \n",
    "    bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "    \n",
    "    y_valid_pred = bst.predict(dvalid)\n",
    "    MSE.append(np.mean(abs(np.reshape(train_Y[test_index], (-1)) - y_valid_pred)**2))\n",
    "    R2.append(r2_score(np.reshape(train_Y[test_index], (-1)), y_valid_pred))\n",
    "    Pearson.append(stats.pearsonr(np.reshape(train_Y[test_index], (-1)), y_valid_pred)[0])\n",
    "\n",
    "print(Pearson)\n",
    "print(MSE)\n",
    "print(R2)\n",
    "\n",
    "np.save(join(\"..\", \"..\", \"data\", \"training_results\", \"Pearson_CV_xgboost_ESM1b.npy\"), np.array(Pearson))\n",
    "np.save(join(\"..\", \"..\", \"data\",  \"training_results\", \"MSE_CV_xgboost_ESM1b.npy\"), np.array(MSE))\n",
    "np.save(join(\"..\", \"..\", \"data\", \"training_results\", \"R2_CV_xgboost_ESM1b.npy\"), np.array(R2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.59 0.939 0.345\n"
     ]
    }
   ],
   "source": [
    "dtrain = xgb.DMatrix(train_X, label = train_Y)\n",
    "dtest = xgb.DMatrix(test_X)\n",
    "\n",
    "bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "\n",
    "y_test_pred = bst.predict(dtest)\n",
    "MSE_dif_fp_test = np.mean(abs(np.reshape(test_Y, (-1)) - y_test_pred)**2)\n",
    "R2_dif_fp_test = r2_score(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "Pearson = stats.pearsonr(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "\n",
    "print(np.round(Pearson[0],3) ,np.round(MSE_dif_fp_test,3), np.round(R2_dif_fp_test,3))\n",
    "\n",
    "np.save(join(\"..\", \"..\", \"data\", \"training_results\", \"y_test_pred_xgboost_ESM1b.npy\"), bst.predict(dtest))\n",
    "np.save(join(\"..\", \"..\", \"data\", \"training_results\", \"y_test_true_xgboost_ESM1b.npy\"), test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred_esm1b = y_test_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training a model with only sequence information (ESM-1b_ts):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Creating input matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ESM1b = np.array(list(data_train[\"ESM1b_ts\"]))\n",
    "train_X = train_ESM1b\n",
    "train_Y = np.array(list(data_train[\"log10_kcat\"]))\n",
    "\n",
    "test_ESM1b = np.array(list(data_test[\"ESM1b_ts\"]))\n",
    "test_X = test_ESM1b\n",
    "test_Y = np.array(list(data_test[\"log10_kcat\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Hyperparameter optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''def cross_validation_mse_gradient_boosting(param):\n",
    "    num_round = param[\"num_rounds\"]\n",
    "    del param[\"num_rounds\"]\n",
    "    param[\"max_depth\"] = int(np.round(param[\"max_depth\"]))\n",
    "    param[\"tree_method\"] = \"gpu_hist\"\n",
    "    param[\"sampling_method\"] = \"gradient_based\"\n",
    "    \n",
    "    MSE = []\n",
    "    R2 = []\n",
    "    for i in range(5):\n",
    "        train_index, test_index  = train_indices[i], test_indices[i]\n",
    "        dtrain = xgb.DMatrix(train_X[train_index], label = train_Y[train_index])\n",
    "        dvalid = xgb.DMatrix(train_X[test_index])\n",
    "        bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "        y_valid_pred = bst.predict(dvalid)\n",
    "        MSE.append(np.mean(abs(np.reshape(train_Y[test_index], (-1)) - y_valid_pred)**2))\n",
    "        R2.append(r2_score(np.reshape(train_Y[test_index], (-1)),  y_valid_pred))\n",
    "    return(-np.mean(R2))\n",
    "\n",
    "\n",
    "from hyperopt import fmin, tpe, rand, hp, Trials\n",
    "\n",
    "space_gradient_boosting = {\n",
    "    \"learning_rate\": hp.uniform(\"learning_rate\", 0.01, 1),\n",
    "    \"max_depth\": hp.uniform(\"max_depth\", 4,12),\n",
    "    #\"subsample\": hp.uniform(\"subsample\", 0.7, 1),\n",
    "    \"reg_lambda\": hp.uniform(\"reg_lambda\", 0, 5),\n",
    "    \"reg_alpha\": hp.uniform(\"reg_alpha\", 0, 5),\n",
    "    \"max_delta_step\": hp.uniform(\"max_delta_step\", 0, 5),\n",
    "    \"min_child_weight\": hp.uniform(\"min_child_weight\", 0.1, 15),\n",
    "    \"num_rounds\":  hp.uniform(\"num_rounds\", 20, 200)}\n",
    "\n",
    "\n",
    "trials = Trials()\n",
    "best = fmin(fn = cross_validation_mse_gradient_boosting, space = space_gradient_boosting,\n",
    "            algo=rand.suggest, max_evals = 200, trials=trials)''';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c) Training and validating model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {'learning_rate': 0.2831145406836757,\n",
    "         'max_delta_step': 0.07686715986169101, \n",
    "         'max_depth': 4.96836783761305,\n",
    "          'min_child_weight': 6.905400087083855,\n",
    "           'num_rounds': 313.1498988074061,\n",
    "            'reg_alpha': 1.717314107718892,\n",
    "             'reg_lambda': 2.470354543039016}\n",
    "\n",
    "num_round = param[\"num_rounds\"]\n",
    "param[\"max_depth\"] = int(np.round(param[\"max_depth\"]))\n",
    "\n",
    "del param[\"num_rounds\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6089419355661412, 0.5895091873624032, 0.5917286045189503, 0.6428033240152629, 0.5233641744058668]\n",
      "[0.830309790220053, 0.8254878887860261, 0.8642504226487573, 0.9497311132104741, 1.1100478926967097]\n",
      "[0.3681252040118652, 0.3449769094978131, 0.34673536553812156, 0.4077182348220084, 0.26585619671739524]\n"
     ]
    }
   ],
   "source": [
    "R2 = []\n",
    "MSE = []\n",
    "Pearson = []\n",
    "y_valid_pred_esm1b_ts = []\n",
    "\n",
    "for i in range(5):\n",
    "    train_index, test_index  = train_indices[i], test_indices[i]\n",
    "    dtrain = xgb.DMatrix(train_X[train_index], label = train_Y[train_index])\n",
    "    dvalid = xgb.DMatrix(train_X[test_index])\n",
    "    \n",
    "    bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "    \n",
    "    y_valid_pred = bst.predict(dvalid)\n",
    "    y_valid_pred_esm1b_ts.append(y_valid_pred)\n",
    "    MSE.append(np.mean(abs(np.reshape(train_Y[test_index], (-1)) - y_valid_pred)**2))\n",
    "    R2.append(r2_score(np.reshape(train_Y[test_index], (-1)), y_valid_pred))\n",
    "    Pearson.append(stats.pearsonr(np.reshape(train_Y[test_index], (-1)), y_valid_pred)[0])\n",
    "\n",
    "print(Pearson)\n",
    "print(MSE)\n",
    "print(R2)\n",
    "\n",
    "np.save(join(\"..\", \"..\", \"data\", \"training_results\", \"Pearson_CV_xgboost_ESM1b_ts.npy\"), np.array(Pearson))\n",
    "np.save(join(\"..\", \"..\", \"data\",  \"training_results\", \"MSE_CV_xgboost_ESM1b_ts.npy\"), np.array(MSE))\n",
    "np.save(join(\"..\", \"..\", \"data\", \"training_results\", \"R2_CV_xgboost_ESM1b_ts.npy\"), np.array(R2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.608 0.905 0.369\n"
     ]
    }
   ],
   "source": [
    "dtrain = xgb.DMatrix(train_X, label = train_Y)\n",
    "dtest = xgb.DMatrix(test_X)\n",
    "\n",
    "bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "\n",
    "y_test_pred = bst.predict(dtest)\n",
    "MSE_dif_fp_test = np.mean(abs(np.reshape(test_Y, (-1)) - y_test_pred)**2)\n",
    "R2_dif_fp_test = r2_score(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "Pearson = stats.pearsonr(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "\n",
    "print(np.round(Pearson[0],3) ,np.round(MSE_dif_fp_test,3), np.round(R2_dif_fp_test,3))\n",
    "\n",
    "np.save(join(\"..\", \"..\", \"data\", \"training_results\", \"y_test_pred_xgboost_ESM1b_ts.npy\"), bst.predict(dtest))\n",
    "np.save(join(\"..\", \"..\", \"data\", \"training_results\", \"y_test_true_xgboost_ESM1b_ts.npy\"), test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred_esm1b_ts = y_test_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (d) Training model with test and train data for production mode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ESM1b = np.array(list(data_train[\"ESM1b_ts\"]))\n",
    "train_Y = np.array(list(data_train[\"log10_kcat\"]))\n",
    "\n",
    "test_ESM1b = np.array(list(data_test[\"ESM1b_ts\"]))\n",
    "test_Y = np.array(list(data_test[\"log10_kcat\"]))\n",
    "\n",
    "train_X = np.concatenate([train_ESM1b, test_ESM1b])\n",
    "train_Y = np.concatenate([train_Y, test_Y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.9839122286376514, 0.0) 0.05006636413883383 0.9650757482138885\n"
     ]
    }
   ],
   "source": [
    "param = {'learning_rate': 0.2831145406836757,\n",
    "         'max_delta_step': 0.07686715986169101, \n",
    "         'max_depth': 4.96836783761305,\n",
    "          'min_child_weight': 6.905400087083855,\n",
    "           'num_rounds': 313.1498988074061,\n",
    "            'reg_alpha': 1.717314107718892,\n",
    "             'reg_lambda': 2.470354543039016}\n",
    "\n",
    "num_round = param[\"num_rounds\"]\n",
    "param[\"max_depth\"] = int(np.round(param[\"max_depth\"]))\n",
    "\n",
    "del param[\"num_rounds\"]\n",
    "\n",
    "dtrain = xgb.DMatrix(train_X, label = train_Y)\n",
    "dtest = xgb.DMatrix(test_X)\n",
    "\n",
    "bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "\n",
    "y_test_pred = bst.predict(dtest)\n",
    "MSE_dif_fp_test = np.mean(abs(np.reshape(test_Y, (-1)) - y_test_pred)**2)\n",
    "R2_dif_fp_test = r2_score(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "Pearson = stats.pearsonr(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "\n",
    "print(Pearson, MSE_dif_fp_test, R2_dif_fp_test)\n",
    "\n",
    "pickle.dump(bst, open(join(\"..\", \"..\", \"data\", \"training_results\", \"saved_models\",\n",
    "                          \"xgboost_sequence_only_train_and_test.pkl\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training a model with only reaction information (DRFP):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = np.array(list(data_train[\"DRFP\"]))\n",
    "train_Y = np.array(list(data_train[\"log10_kcat\"]))\n",
    "\n",
    "test_X = np.array(list(data_test[\"DRFP\"]))\n",
    "test_Y = np.array(list(data_test[\"log10_kcat\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Hyperparameter optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''def cross_validation_mse_gradient_boosting(param):\n",
    "    num_round = param[\"num_rounds\"]\n",
    "    del param[\"num_rounds\"]\n",
    "    param[\"max_depth\"] = int(np.round(param[\"max_depth\"]))\n",
    "    param[\"tree_method\"] = \"gpu_hist\"\n",
    "    param[\"sampling_method\"] = \"gradient_based\"\n",
    "    \n",
    "    MSE = []\n",
    "    R2 = []\n",
    "    for i in range(5):\n",
    "        train_index, test_index  = train_indices[i], test_indices[i]\n",
    "        dtrain = xgb.DMatrix(train_X[train_index], label = train_Y[train_index])\n",
    "        dvalid = xgb.DMatrix(train_X[test_index])\n",
    "        bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "        y_valid_pred = bst.predict(dvalid)\n",
    "        MSE.append(np.mean(abs(np.reshape(train_Y[test_index], (-1)) - y_valid_pred)**2))\n",
    "        R2.append(r2_score(np.reshape(train_Y[test_index], (-1)),  y_valid_pred))\n",
    "    return(-np.mean(R2))\n",
    "\n",
    "\n",
    "from hyperopt import fmin, tpe, rand, hp, Trials\n",
    "\n",
    "space_gradient_boosting = {\n",
    "    \"learning_rate\": hp.uniform(\"learning_rate\", 0.01, 1),\n",
    "    \"max_depth\": hp.uniform(\"max_depth\", 4,12),\n",
    "    #\"subsample\": hp.uniform(\"subsample\", 0.7, 1),\n",
    "    \"reg_lambda\": hp.uniform(\"reg_lambda\", 0, 5),\n",
    "    \"reg_alpha\": hp.uniform(\"reg_alpha\", 0, 5),\n",
    "    \"max_delta_step\": hp.uniform(\"max_delta_step\", 0, 5),\n",
    "    \"min_child_weight\": hp.uniform(\"min_child_weight\", 0.1, 15),\n",
    "    \"num_rounds\":  hp.uniform(\"num_rounds\", 20, 200)}\n",
    "\n",
    "\n",
    "trials = Trials()\n",
    "best = fmin(fn = cross_validation_mse_gradient_boosting, space = space_gradient_boosting,\n",
    "            algo=rand.suggest, max_evals = 200, trials=trials)''';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c) Training and validating model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {'learning_rate': 0.08987247189322463,\n",
    "         'max_delta_step': 1.1939737318908727,\n",
    "         'max_depth': 11.268531225242574,\n",
    "         'min_child_weight': 2.8172720953826302,\n",
    "         'num_rounds': 109.03643430746544,\n",
    "         'reg_alpha': 1.9412226989868904,\n",
    "         'reg_lambda': 4.950543905603358}\n",
    "\n",
    "\n",
    "num_round = param[\"num_rounds\"]\n",
    "param[\"max_depth\"] = int(np.round(param[\"max_depth\"]))\n",
    "\n",
    "del param[\"num_rounds\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5703243058726097, 0.5534056710833766, 0.583291365495366, 0.5706660736746292, 0.5608173001699261]\n",
      "[0.8877274220430679, 0.8796146121877297, 0.8815364701552285, 1.082140247084615, 1.0382516796274228]\n",
      "[0.3244297606705625, 0.3020274560618814, 0.3336692874551086, 0.32514379407155003, 0.313339503762757]\n"
     ]
    }
   ],
   "source": [
    "R2 = []\n",
    "MSE = []\n",
    "Pearson = []\n",
    "y_valid_pred_DRFP = []\n",
    "\n",
    "for i in range(5):\n",
    "    train_index, test_index  = train_indices[i], test_indices[i]\n",
    "    dtrain = xgb.DMatrix(train_X[train_index], label = train_Y[train_index])\n",
    "    dvalid = xgb.DMatrix(train_X[test_index])\n",
    "    \n",
    "    bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "    \n",
    "    y_valid_pred = bst.predict(dvalid)\n",
    "    y_valid_pred_DRFP.append(y_valid_pred)\n",
    "    MSE.append(np.mean(abs(np.reshape(train_Y[test_index], (-1)) - y_valid_pred)**2))\n",
    "    R2.append(r2_score(np.reshape(train_Y[test_index], (-1)), y_valid_pred))\n",
    "    Pearson.append(stats.pearsonr(np.reshape(train_Y[test_index], (-1)), y_valid_pred)[0])\n",
    "\n",
    "print(Pearson)\n",
    "print(MSE)\n",
    "print(R2)\n",
    "\n",
    "np.save(join(\"..\", \"..\", \"data\", \"training_results\", \"Pearson_CV_xgboost_DRFP.npy\"), np.array(Pearson))\n",
    "np.save(join(\"..\", \"..\", \"data\",  \"training_results\", \"MSE_CV_xgboost_DRFP.npy\"), np.array(MSE))\n",
    "np.save(join(\"..\", \"..\", \"data\", \"training_results\", \"R2_CV_xgboost_DRFP.npy\"), np.array(R2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.619 0.886 0.382\n"
     ]
    }
   ],
   "source": [
    "dtrain = xgb.DMatrix(train_X, label = train_Y)\n",
    "dtest = xgb.DMatrix(test_X)\n",
    "\n",
    "bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "\n",
    "y_test_pred = bst.predict(dtest)\n",
    "MSE_dif_fp_test = np.mean(abs(np.reshape(test_Y, (-1)) - y_test_pred)**2)\n",
    "R2_dif_fp_test = r2_score(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "Pearson = stats.pearsonr(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "\n",
    "print(np.round(Pearson[0],3) ,np.round(MSE_dif_fp_test,3), np.round(R2_dif_fp_test,3))\n",
    "\n",
    "np.save(join(\"..\", \"..\", \"data\", \"training_results\", \"y_test_pred_xgboost_DRFP.npy\"), bst.predict(dtest))\n",
    "np.save(join(\"..\", \"..\", \"data\", \"training_results\", \"y_test_true_xgboost_DRFP.npy\"), test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred_drfp = y_test_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (d) Training model with test and train data for production mode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_DRFP = np.array(list(data_train[\"DRFP\"]))\n",
    "train_Y = np.array(list(data_train[\"log10_kcat\"]))\n",
    "\n",
    "test_DRFP = np.array(list(data_test[\"DRFP\"]))\n",
    "test_Y = np.array(list(data_test[\"log10_kcat\"]))\n",
    "\n",
    "train_X = np.concatenate([train_DRFP, test_DRFP])\n",
    "train_Y = np.concatenate([train_Y, test_Y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8681560564738647, 3.0446983463533428e-260) 0.4027527000636768 0.7190561578317212\n"
     ]
    }
   ],
   "source": [
    "param = {'learning_rate': 0.08987247189322463,\n",
    "         'max_delta_step': 1.1939737318908727,\n",
    "         'max_depth': 11.268531225242574,\n",
    "         'min_child_weight': 2.8172720953826302,\n",
    "         'num_rounds': 109.03643430746544,\n",
    "         'reg_alpha': 1.9412226989868904,\n",
    "         'reg_lambda': 4.950543905603358}\n",
    "\n",
    "\n",
    "num_round = param[\"num_rounds\"]\n",
    "param[\"max_depth\"] = int(np.round(param[\"max_depth\"]))\n",
    "\n",
    "del param[\"num_rounds\"]\n",
    "\n",
    "dtrain = xgb.DMatrix(train_X, label = train_Y)\n",
    "dtest = xgb.DMatrix(test_X)\n",
    "\n",
    "bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "\n",
    "y_test_pred = bst.predict(dtest)\n",
    "MSE_dif_fp_test = np.mean(abs(np.reshape(test_Y, (-1)) - y_test_pred)**2)\n",
    "R2_dif_fp_test = r2_score(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "Pearson = stats.pearsonr(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "\n",
    "print(Pearson, MSE_dif_fp_test, R2_dif_fp_test)\n",
    "\n",
    "pickle.dump(bst, open(join(\"..\", \"..\", \"data\", \"training_results\", \"saved_models\",\n",
    "                          \"xgboost_reaction_only_train_and_test.pkl\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training a model with only reaction information (difference fingerprint):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Creating input matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = np.array(list(data_train[\"difference_fp\"]))\n",
    "train_Y = np.array(list(data_train[\"log10_kcat\"]))\n",
    "\n",
    "test_X = np.array(list(data_test[\"difference_fp\"]))\n",
    "test_Y = np.array(list(data_test[\"log10_kcat\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Hyperparameter optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''def cross_validation_mse_gradient_boosting(param):\n",
    "    num_round = param[\"num_rounds\"]\n",
    "    del param[\"num_rounds\"]\n",
    "    param[\"max_depth\"] = int(np.round(param[\"max_depth\"]))\n",
    "    param[\"tree_method\"] = \"gpu_hist\"\n",
    "    param[\"sampling_method\"] = \"gradient_based\"\n",
    "    \n",
    "    MSE = []\n",
    "    R2 = []\n",
    "    for i in range(5):\n",
    "        train_index, test_index  = train_indices[i], test_indices[i]\n",
    "        dtrain = xgb.DMatrix(train_X[train_index], label = train_Y[train_index])\n",
    "        dvalid = xgb.DMatrix(train_X[test_index])\n",
    "        bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "        y_valid_pred = bst.predict(dvalid)\n",
    "        MSE.append(np.mean(abs(np.reshape(train_Y[test_index], (-1)) - y_valid_pred)**2))\n",
    "        R2.append(r2_score(np.reshape(train_Y[test_index], (-1)),  y_valid_pred))\n",
    "    return(-np.mean(R2))\n",
    "\n",
    "\n",
    "from hyperopt import fmin, tpe, rand, hp, Trials\n",
    "\n",
    "space_gradient_boosting = {\n",
    "    \"learning_rate\": hp.uniform(\"learning_rate\", 0.01, 1),\n",
    "    \"max_depth\": hp.uniform(\"max_depth\", 4,12),\n",
    "    #\"subsample\": hp.uniform(\"subsample\", 0.7, 1),\n",
    "    \"reg_lambda\": hp.uniform(\"reg_lambda\", 0, 5),\n",
    "    \"reg_alpha\": hp.uniform(\"reg_alpha\", 0, 5),\n",
    "    \"max_delta_step\": hp.uniform(\"max_delta_step\", 0, 5),\n",
    "    \"min_child_weight\": hp.uniform(\"min_child_weight\", 0.1, 15),\n",
    "    \"num_rounds\":  hp.uniform(\"num_rounds\", 20, 200)}\n",
    "\n",
    "\n",
    "trials = Trials()\n",
    "best = fmin(fn = cross_validation_mse_gradient_boosting, space = space_gradient_boosting,\n",
    "            algo=rand.suggest, max_evals = 200, trials=trials)''';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c) Training and validating model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {'learning_rate': 0.14154883958006167,\n",
    "         'max_delta_step': 0.02234358170535966,\n",
    "         'max_depth': 10.869653004093198,\n",
    "         'min_child_weight': 1.7936882442746056,\n",
    "         'num_rounds': 361.6168542774665,\n",
    "         'reg_alpha': 4.825525325323308, \n",
    "         'reg_lambda': 2.74944090578774}\n",
    "\n",
    "\n",
    "num_round = param[\"num_rounds\"]\n",
    "param[\"max_depth\"] = int(np.round(param[\"max_depth\"]))\n",
    "\n",
    "del param[\"num_rounds\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5933818458131599, 0.5290768248822283, 0.5426074027598811, 0.5889341652634298, 0.5248561000592911]\n",
      "[0.8733503195891379, 0.9112072460648096, 0.9525515259896388, 1.0924558329257095, 1.1054594918609029]\n",
      "[0.3353708922662406, 0.27695876037247324, 0.27999083584524465, 0.31871067494359473, 0.2688907919476974]\n"
     ]
    }
   ],
   "source": [
    "R2 = []\n",
    "MSE = []\n",
    "Pearson = []\n",
    "\n",
    "for i in range(5):\n",
    "    train_index, test_index  = train_indices[i], test_indices[i]\n",
    "    dtrain = xgb.DMatrix(train_X[train_index], label = train_Y[train_index])\n",
    "    dvalid = xgb.DMatrix(train_X[test_index])\n",
    "    \n",
    "    bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "    \n",
    "    y_valid_pred = bst.predict(dvalid)\n",
    "    MSE.append(np.mean(abs(np.reshape(train_Y[test_index], (-1)) - y_valid_pred)**2))\n",
    "    R2.append(r2_score(np.reshape(train_Y[test_index], (-1)), y_valid_pred))\n",
    "    Pearson.append(stats.pearsonr(np.reshape(train_Y[test_index], (-1)), y_valid_pred)[0])\n",
    "\n",
    "print(Pearson)\n",
    "print(MSE)\n",
    "print(R2)\n",
    "\n",
    "np.save(join(\"..\", \"..\", \"data\", \"training_results\", \"Pearson_CV_xgboost_diff_fp.npy\"), np.array(Pearson))\n",
    "np.save(join(\"..\", \"..\", \"data\", \"training_results\", \"MSE_CV_xgboost_diff_fp.npy\"), np.array(MSE))\n",
    "np.save(join(\"..\", \"..\", \"data\", \"training_results\", \"R2_CV_xgboost_diff_fp.npy\"), np.array(R2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6 0.948 0.339\n"
     ]
    }
   ],
   "source": [
    "dtrain = xgb.DMatrix(train_X, label = train_Y)\n",
    "dtest = xgb.DMatrix(test_X)\n",
    "\n",
    "bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "\n",
    "y_test_pred = bst.predict(dtest)\n",
    "MSE_dif_fp_test = np.mean(abs(np.reshape(test_Y, (-1)) - y_test_pred)**2)\n",
    "R2_dif_fp_test = r2_score(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "Pearson = stats.pearsonr(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "\n",
    "print(np.round(Pearson[0],3) ,np.round(MSE_dif_fp_test,3), np.round(R2_dif_fp_test,3))\n",
    "\n",
    "\n",
    "np.save(join(\"..\", \"..\", \"data\", \"training_results\", \"y_test_pred_xgboost_diff_fp.npy\"), bst.predict(dtest))\n",
    "np.save(join(\"..\", \"..\", \"data\", \"training_results\", \"y_test_true_xgboost_diff_fp.npy\"), test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred_diff_fp = y_test_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training a model with only reaction information (structural fingerprint):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Creating input matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = ();\n",
    "for ind in data_train.index:\n",
    "    train_X = train_X + (np.array(list(data_train[\"structural_fp\"][ind])).astype(int), )\n",
    "train_X = np.array(train_X)\n",
    "train_Y = np.array(list(data_train[\"log10_kcat\"]))\n",
    "\n",
    "\n",
    "test_X = ();\n",
    "for ind in data_test.index:\n",
    "    test_X = test_X + (np.array(list(data_test[\"structural_fp\"][ind])).astype(int), )\n",
    "test_X = np.array(test_X)\n",
    "test_Y = np.array(list(data_test[\"log10_kcat\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Hyperparameter optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''def cross_validation_mse_gradient_boosting(param):\n",
    "    num_round = param[\"num_rounds\"]\n",
    "    del param[\"num_rounds\"]\n",
    "    param[\"max_depth\"] = int(np.round(param[\"max_depth\"]))\n",
    "    param[\"tree_method\"] = \"gpu_hist\"\n",
    "    param[\"sampling_method\"] = \"gradient_based\"\n",
    "    \n",
    "    MSE = []\n",
    "    R2 = []\n",
    "    for i in range(5):\n",
    "        train_index, test_index  = train_indices[i], test_indices[i]\n",
    "        dtrain = xgb.DMatrix(train_X[train_index], label = train_Y[train_index])\n",
    "        dvalid = xgb.DMatrix(train_X[test_index])\n",
    "        bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "        y_valid_pred = bst.predict(dvalid)\n",
    "        MSE.append(np.mean(abs(np.reshape(train_Y[test_index], (-1)) - y_valid_pred)**2))\n",
    "        R2.append(r2_score(np.reshape(train_Y[test_index], (-1)),  y_valid_pred))\n",
    "    return(-np.mean(R2))\n",
    "\n",
    "\n",
    "from hyperopt import fmin, tpe, rand, hp, Trials\n",
    "\n",
    "space_gradient_boosting = {\n",
    "    \"learning_rate\": hp.uniform(\"learning_rate\", 0.01, 1),\n",
    "    \"max_depth\": hp.uniform(\"max_depth\", 4,12),\n",
    "    #\"subsample\": hp.uniform(\"subsample\", 0.7, 1),\n",
    "    \"reg_lambda\": hp.uniform(\"reg_lambda\", 0, 5),\n",
    "    \"reg_alpha\": hp.uniform(\"reg_alpha\", 0, 5),\n",
    "    \"max_delta_step\": hp.uniform(\"max_delta_step\", 0, 5),\n",
    "    \"min_child_weight\": hp.uniform(\"min_child_weight\", 0.1, 15),\n",
    "    \"num_rounds\":  hp.uniform(\"num_rounds\", 20, 200)}\n",
    "\n",
    "\n",
    "trials = Trials()\n",
    "best = fmin(fn = cross_validation_mse_gradient_boosting, space = space_gradient_boosting,\n",
    "            algo=rand.suggest, max_evals = 200, trials=trials)''';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c) Training and validating model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {'learning_rate': 0.01126910440903659,\n",
    "         'max_delta_step': 0.5777120839605732,\n",
    "         'max_depth': 5.486901609313889,\n",
    "         'min_child_weight': 6.14467742389769,\n",
    "         'num_rounds': 488.943459090126,\n",
    "         'reg_alpha': 4.629840853377147,\n",
    "         'reg_lambda': 2.1047561335691745}\n",
    "\n",
    "num_round = param[\"num_rounds\"]\n",
    "param[\"max_depth\"] = int(np.round(param[\"max_depth\"]))\n",
    "\n",
    "del param[\"num_rounds\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5536292775258076, 0.5323143816237441, 0.4889091394899996, 0.6104056516948199, 0.5039256159780712]\n",
      "[0.917613544024189, 0.9056644419444148, 1.015607786275439, 1.064522657133946, 1.1289411204418558]\n",
      "[0.3016860962550283, 0.2813569650394473, 0.23232823280029657, 0.3361306693344771, 0.2533609285723336]\n"
     ]
    }
   ],
   "source": [
    "R2 = []\n",
    "MSE = []\n",
    "Pearson = []\n",
    "\n",
    "for i in range(5):\n",
    "    train_index, test_index  = train_indices[i], test_indices[i]\n",
    "    dtrain = xgb.DMatrix(train_X[train_index], label = train_Y[train_index])\n",
    "    dvalid = xgb.DMatrix(train_X[test_index])\n",
    "    \n",
    "    bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "    \n",
    "    y_valid_pred = bst.predict(dvalid)\n",
    "    MSE.append(np.mean(abs(np.reshape(train_Y[test_index], (-1)) - y_valid_pred)**2))\n",
    "    R2.append(r2_score(np.reshape(train_Y[test_index], (-1)), y_valid_pred))\n",
    "    Pearson.append(stats.pearsonr(np.reshape(train_Y[test_index], (-1)), y_valid_pred)[0])\n",
    "\n",
    "print(Pearson)\n",
    "print(MSE)\n",
    "print(R2)\n",
    "\n",
    "np.save(join(\"..\", \"..\", \"data\", \"training_results\", \"Pearson_CV_xgboost_str_fp.npy\"), np.array(Pearson))\n",
    "np.save(join(\"..\", \"..\", \"data\", \"training_results\", \"MSE_CV_xgboost_str_fp.npy\"), np.array(MSE))\n",
    "np.save(join(\"..\", \"..\", \"data\", \"training_results\", \"R2_CV_xgboost_str_fp.npy\"), np.array(R2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.561 0.994 0.307\n"
     ]
    }
   ],
   "source": [
    "dtrain = xgb.DMatrix(train_X, label = train_Y)\n",
    "dtest = xgb.DMatrix(test_X)\n",
    "\n",
    "bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "\n",
    "y_test_pred = bst.predict(dtest)\n",
    "MSE_dif_fp_test = np.mean(abs(np.reshape(test_Y, (-1)) - y_test_pred)**2)\n",
    "R2_dif_fp_test = r2_score(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "Pearson = stats.pearsonr(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "\n",
    "print(np.round(Pearson[0],3) ,np.round(MSE_dif_fp_test,3), np.round(R2_dif_fp_test,3))\n",
    "\n",
    "\n",
    "np.save(join(\"..\", \"..\", \"data\", \"training_results\", \"y_test_pred_xgboost_str_fp.npy\"), bst.predict(dtest))\n",
    "np.save(join(\"..\", \"..\", \"data\", \"training_results\", \"y_test_true_xgboost_str_fp.npy\"), test_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training a model with enzyme and reaction information (ESM1b_ts/DRFP):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = np.array(list(data_train[\"DRFP\"]))\n",
    "train_X = np.concatenate([train_X, np.array(list(data_train[\"ESM1b_ts\"]))], axis = 1)\n",
    "train_Y = np.array(list(data_train[\"log10_kcat\"]))\n",
    "\n",
    "test_X = np.array(list(data_test[\"DRFP\"]))\n",
    "test_X = np.concatenate([test_X, np.array(list(data_test[\"ESM1b_ts\"]))], axis = 1)\n",
    "test_Y = np.array(list(data_test[\"log10_kcat\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Hyperparameter optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''def cross_validation_mse_gradient_boosting(param):\n",
    "    num_round = param[\"num_rounds\"]\n",
    "    del param[\"num_rounds\"]\n",
    "    param[\"max_depth\"] = int(np.round(param[\"max_depth\"]))\n",
    "    param[\"tree_method\"] = \"gpu_hist\"\n",
    "    param[\"sampling_method\"] = \"gradient_based\"\n",
    "    \n",
    "    MSE = []\n",
    "    R2 = []\n",
    "    for i in range(5):\n",
    "        train_index, test_index  = train_indices[i], test_indices[i]\n",
    "        dtrain = xgb.DMatrix(train_X[train_index], label = train_Y[train_index])\n",
    "        dvalid = xgb.DMatrix(train_X[test_index])\n",
    "        bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "        y_valid_pred = bst.predict(dvalid)\n",
    "        MSE.append(np.mean(abs(np.reshape(train_Y[test_index], (-1)) - y_valid_pred)**2))\n",
    "        R2.append(r2_score(np.reshape(train_Y[test_index], (-1)),  y_valid_pred))\n",
    "    return(-np.mean(R2))\n",
    "\n",
    "\n",
    "from hyperopt import fmin, tpe, rand, hp, Trials\n",
    "\n",
    "space_gradient_boosting = {\n",
    "    \"learning_rate\": hp.uniform(\"learning_rate\", 0.01, 1),\n",
    "    \"max_depth\": hp.uniform(\"max_depth\", 6,14),\n",
    "    #\"subsample\": hp.uniform(\"subsample\", 0.7, 1),\n",
    "    \"reg_lambda\": hp.uniform(\"reg_lambda\", 0, 5),\n",
    "    \"reg_alpha\": hp.uniform(\"reg_alpha\", 0, 5),\n",
    "    \"max_delta_step\": hp.uniform(\"max_delta_step\", 0, 5),\n",
    "    \"min_child_weight\": hp.uniform(\"min_child_weight\", 0.1, 15),\n",
    "    \"num_rounds\":  hp.uniform(\"num_rounds\", 20, 200)}\n",
    "\n",
    "\n",
    "trials = Trials()\n",
    "best = fmin(fn = cross_validation_mse_gradient_boosting, space = space_gradient_boosting,\n",
    "            algo=rand.suggest, max_evals = 200, trials=trials)''';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c) Training and validating model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {'learning_rate': 0.05221672412884108,\n",
    "         'max_delta_step': 1.0767235463496743,\n",
    "         'max_depth': 11.329014411591299,\n",
    "         'min_child_weight': 14.724796449973605,\n",
    "         'num_rounds': 298.9598325756988,\n",
    "         'reg_alpha': 2.8295816318634452,\n",
    "         'reg_lambda': 0.6528469146574993}\n",
    "\n",
    "num_round = param[\"num_rounds\"]\n",
    "param[\"max_depth\"] = int(np.round(param[\"max_depth\"]))\n",
    "\n",
    "del param[\"num_rounds\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6353444924580318, 0.5880301926572151, 0.5241899073773033, 0.6588493325838651, 0.5473715375037416]\n",
      "[0.7895750755455662, 0.8261521353734187, 0.9646511784030429, 0.9310396416328481, 1.0594944425073949]\n",
      "[0.3991247656547007, 0.34444983107735405, 0.270845020229982, 0.4193748159593236, 0.2992903417080939]\n"
     ]
    }
   ],
   "source": [
    "R2 = []\n",
    "MSE = []\n",
    "Pearson = []\n",
    "\n",
    "for i in range(5):\n",
    "    train_index, test_index  = train_indices[i], test_indices[i]\n",
    "    dtrain = xgb.DMatrix(train_X[train_index], label = train_Y[train_index])\n",
    "    dvalid = xgb.DMatrix(train_X[test_index])\n",
    "    \n",
    "    bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "    \n",
    "    y_valid_pred = bst.predict(dvalid)\n",
    "    MSE.append(np.mean(abs(np.reshape(train_Y[test_index], (-1)) - y_valid_pred)**2))\n",
    "    R2.append(r2_score(np.reshape(train_Y[test_index], (-1)), y_valid_pred))\n",
    "    Pearson.append(stats.pearsonr(np.reshape(train_Y[test_index], (-1)), y_valid_pred)[0])\n",
    "\n",
    "print(Pearson)\n",
    "print(MSE)\n",
    "print(R2)\n",
    "\n",
    "np.save(join(\"..\", \"..\", \"data\", \"training_results\", \"Pearson_CV_xgboost_ESM1b_ts_DRFP.npy\"), np.array(Pearson))\n",
    "np.save(join(\"..\", \"..\", \"data\", \"training_results\", \"MSE_CV_xgboost_ESM1b_ts_DRFP.npy\"), np.array(MSE))\n",
    "np.save(join(\"..\", \"..\", \"data\", \"training_results\", \"R2_CV_xgboost_ESM1b_ts_DRFP.npy\"), np.array(R2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.636 0.856 0.403\n"
     ]
    }
   ],
   "source": [
    "dtrain = xgb.DMatrix(train_X, label = train_Y)\n",
    "dtest = xgb.DMatrix(test_X, label = test_Y)\n",
    "\n",
    "\n",
    "bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "\n",
    "y_test_pred = bst.predict(dtest)\n",
    "MSE_dif_fp_test = np.mean(abs(np.reshape(test_Y, (-1)) - y_test_pred)**2)\n",
    "R2_dif_fp_test = r2_score(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "Pearson = stats.pearsonr(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "\n",
    "print(np.round(Pearson[0],3) ,np.round(MSE_dif_fp_test,3), np.round(R2_dif_fp_test,3))\n",
    "\n",
    "\n",
    "np.save(join(\"..\", \"..\", \"data\", \"training_results\", \"y_test_pred_xgboost_ESM1b_ts_DRFP.npy\"), bst.predict(dtest))\n",
    "np.save(join(\"..\", \"..\", \"data\", \"training_results\", \"y_test_true_xgboost_ESM1b_ts_DRFP.npy\"), test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred_esm1b_ts_drfp = y_test_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training a model with enzyme and reaction information (ESM1b_ts/diff_fp):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Creating input matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = np.array(list(data_train[\"difference_fp\"]))\n",
    "train_X = np.concatenate([train_X, np.array(list(data_train[\"ESM2\"]))], axis = 1)\n",
    "train_Y = np.array(list(data_train[\"log10_kcat\"]))\n",
    "\n",
    "test_X = np.array(list(data_test[\"difference_fp\"]))\n",
    "test_X = np.concatenate([test_X, np.array(list(data_test[\"ESM2\"]))], axis = 1)\n",
    "test_Y = np.array(list(data_test[\"log10_kcat\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Hyperparameter optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                          | 0/200 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "job exception: [18:09:31] c:\\users\\administrator\\workspace\\xgboost-win64_release_1.6.0\\src\\data\\../common/device_helpers.cuh:428: Memory allocation error on worker 0: [18:09:31] c:\\users\\administrator\\workspace\\xgboost-win64_release_1.6.0\\src\\common\\common.h:46: c:\\users\\administrator\\workspace\\xgboost-win64_release_1.6.0\\src\\common\\device_helpers.cuh: 447: cudaErrorNoKernelImageForDevice: no kernel image is available for execution on the device\n",
      "- Free memory: 3582548379\n",
      "- Requested memory: 12\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|                                                                          | 0/200 [00:01<?, ?trial/s, best loss=?]\n"
     ]
    },
    {
     "ename": "XGBoostError",
     "evalue": "[18:09:31] c:\\users\\administrator\\workspace\\xgboost-win64_release_1.6.0\\src\\data\\../common/device_helpers.cuh:428: Memory allocation error on worker 0: [18:09:31] c:\\users\\administrator\\workspace\\xgboost-win64_release_1.6.0\\src\\common\\common.h:46: c:\\users\\administrator\\workspace\\xgboost-win64_release_1.6.0\\src\\common\\device_helpers.cuh: 447: cudaErrorNoKernelImageForDevice: no kernel image is available for execution on the device\n- Free memory: 3582548379\n- Requested memory: 12\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mXGBoostError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17328\\3286195955.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[0mtrials\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrials\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m best = fmin(fn = cross_validation_mse_gradient_boosting, space = space_gradient_boosting,\n\u001b[1;32m---> 36\u001b[1;33m             algo=rand.suggest, max_evals = 200, trials=trials)\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\py37\\lib\\site-packages\\hyperopt\\fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[1;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[0;32m    553\u001b[0m             \u001b[0mshow_progressbar\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshow_progressbar\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m             \u001b[0mearly_stop_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mearly_stop_fn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 555\u001b[1;33m             \u001b[0mtrials_save_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrials_save_file\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    556\u001b[0m         )\n\u001b[0;32m    557\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\py37\\lib\\site-packages\\hyperopt\\base.py\u001b[0m in \u001b[0;36mfmin\u001b[1;34m(self, fn, space, algo, max_evals, timeout, loss_threshold, max_queue_len, rstate, verbose, pass_expr_memo_ctrl, catch_eval_exceptions, return_argmin, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[0;32m    686\u001b[0m             \u001b[0mshow_progressbar\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshow_progressbar\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    687\u001b[0m             \u001b[0mearly_stop_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mearly_stop_fn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 688\u001b[1;33m             \u001b[0mtrials_save_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrials_save_file\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    689\u001b[0m         )\n\u001b[0;32m    690\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\py37\\lib\\site-packages\\hyperopt\\fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[1;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[0;32m    584\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m     \u001b[1;31m# next line is where the fmin is actually executed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 586\u001b[1;33m     \u001b[0mrval\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexhaust\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mreturn_argmin\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\py37\\lib\\site-packages\\hyperopt\\fmin.py\u001b[0m in \u001b[0;36mexhaust\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    362\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mexhaust\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    363\u001b[0m         \u001b[0mn_done\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 364\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_evals\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mn_done\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mblock_until_done\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masynchronous\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    365\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    366\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\py37\\lib\\site-packages\\hyperopt\\fmin.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, N, block_until_done)\u001b[0m\n\u001b[0;32m    298\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    299\u001b[0m                     \u001b[1;31m# -- loop over trials and do the jobs directly\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 300\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mserial_evaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    301\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\py37\\lib\\site-packages\\hyperopt\\fmin.py\u001b[0m in \u001b[0;36mserial_evaluate\u001b[1;34m(self, N)\u001b[0m\n\u001b[0;32m    176\u001b[0m                 \u001b[0mctrl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCtrl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurrent_trial\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 178\u001b[1;33m                     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdomain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctrl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    179\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"job exception: %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\py37\\lib\\site-packages\\hyperopt\\base.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[0;32m    890\u001b[0m                 \u001b[0mprint_node_on_error\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrec_eval_print_node_on_error\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m             )\n\u001b[1;32m--> 892\u001b[1;33m             \u001b[0mrval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpyll_rval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    893\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumber\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17328\\3286195955.py\u001b[0m in \u001b[0;36mcross_validation_mse_gradient_boosting\u001b[1;34m(param)\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mdtrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDMatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_Y\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mdvalid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDMatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[0mbst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_round\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose_eval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m         \u001b[0my_valid_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdvalid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mMSE\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_Y\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0my_valid_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\training.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[0;32m    179\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcb_container\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbefore_iteration\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m             \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 181\u001b[1;33m         \u001b[0mbst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    182\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcb_container\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mafter_iteration\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m             \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[0;32m   1733\u001b[0m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,\n\u001b[0;32m   1734\u001b[0m                                                     \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1735\u001b[1;33m                                                     dtrain.handle))\n\u001b[0m\u001b[0;32m   1736\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1737\u001b[0m             \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_margin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\py37\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36m_check_call\u001b[1;34m(ret)\u001b[0m\n\u001b[0;32m    201\u001b[0m     \"\"\"\n\u001b[0;32m    202\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mXGBoostError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpy_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mXGBGetLastError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    204\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mXGBoostError\u001b[0m: [18:09:31] c:\\users\\administrator\\workspace\\xgboost-win64_release_1.6.0\\src\\data\\../common/device_helpers.cuh:428: Memory allocation error on worker 0: [18:09:31] c:\\users\\administrator\\workspace\\xgboost-win64_release_1.6.0\\src\\common\\common.h:46: c:\\users\\administrator\\workspace\\xgboost-win64_release_1.6.0\\src\\common\\device_helpers.cuh: 447: cudaErrorNoKernelImageForDevice: no kernel image is available for execution on the device\n- Free memory: 3582548379\n- Requested memory: 12\n"
     ]
    }
   ],
   "source": [
    "def cross_validation_mse_gradient_boosting(param):\n",
    "    num_round = param[\"num_rounds\"]\n",
    "    del param[\"num_rounds\"]\n",
    "    param[\"max_depth\"] = int(np.round(param[\"max_depth\"]))\n",
    "    param[\"tree_method\"] = \"gpu_hist\"\n",
    "    param[\"sampling_method\"] = \"gradient_based\"\n",
    "    \n",
    "    MSE = []\n",
    "    R2 = []\n",
    "    for i in range(5):\n",
    "        train_index, test_index  = train_indices[i], test_indices[i]\n",
    "        dtrain = xgb.DMatrix(train_X[train_index], label = train_Y[train_index])\n",
    "        dvalid = xgb.DMatrix(train_X[test_index])\n",
    "        bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "        y_valid_pred = bst.predict(dvalid)\n",
    "        MSE.append(np.mean(abs(np.reshape(train_Y[test_index], (-1)) - y_valid_pred)**2))\n",
    "        R2.append(r2_score(np.reshape(train_Y[test_index], (-1)),  y_valid_pred))\n",
    "    return(-np.mean(R2))\n",
    "\n",
    "\n",
    "from hyperopt import fmin, tpe, rand, hp, Trials\n",
    "\n",
    "space_gradient_boosting = {\n",
    "    \"learning_rate\": hp.uniform(\"learning_rate\", 0.01, 1),\n",
    "    \"max_depth\": hp.uniform(\"max_depth\", 4,12),\n",
    "    #\"subsample\": hp.uniform(\"subsample\", 0.7, 1),\n",
    "    \"reg_lambda\": hp.uniform(\"reg_lambda\", 0, 5),\n",
    "    \"reg_alpha\": hp.uniform(\"reg_alpha\", 0, 5),\n",
    "    \"max_delta_step\": hp.uniform(\"max_delta_step\", 0, 5),\n",
    "    \"min_child_weight\": hp.uniform(\"min_child_weight\", 0.1, 15),\n",
    "    \"num_rounds\":  hp.uniform(\"num_rounds\", 20, 200)}\n",
    "\n",
    "\n",
    "trials = Trials()\n",
    "best = fmin(fn = cross_validation_mse_gradient_boosting, space = space_gradient_boosting,\n",
    "            algo=rand.suggest, max_evals = 200, trials=trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c) Training and validating model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {'learning_rate': 0.5727401435817077, \n",
    "         'max_delta_step': 0.022572978136953803,\n",
    "         'max_depth': 9.734956573895278,\n",
    "         'min_child_weight': 2.026404280518698,\n",
    "         'num_rounds': 259.69265795096726,\n",
    "         'reg_alpha': 7.333074414515098,\n",
    "         'reg_lambda': 0.8545111451043885}\n",
    "\n",
    "\n",
    "num_round = param[\"num_rounds\"]\n",
    "param[\"max_depth\"] = int(np.round(param[\"max_depth\"]))\n",
    "\n",
    "del param[\"num_rounds\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6445379171956691, 0.5663538267118355, 0.5813292687854972, 0.660367102217668, 0.5437710214323066]\n",
      "[0.7778549566615786, 0.8621287188783706, 0.8811802952099859, 0.9212592601438562, 1.0674061317478523]\n",
      "[0.40804390380771904, 0.31590247958588724, 0.33393851092241733, 0.4254741650612315, 0.2940578488872041]\n"
     ]
    }
   ],
   "source": [
    "R2 = []\n",
    "MSE = []\n",
    "Pearson = []\n",
    "\n",
    "for i in range(5):\n",
    "    train_index, test_index  = train_indices[i], test_indices[i]\n",
    "    dtrain = xgb.DMatrix(train_X[train_index], label = train_Y[train_index])\n",
    "    dvalid = xgb.DMatrix(train_X[test_index])\n",
    "    \n",
    "    bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "    \n",
    "    y_valid_pred = bst.predict(dvalid)\n",
    "    MSE.append(np.mean(abs(np.reshape(train_Y[test_index], (-1)) - y_valid_pred)**2))\n",
    "    R2.append(r2_score(np.reshape(train_Y[test_index], (-1)), y_valid_pred))\n",
    "    Pearson.append(stats.pearsonr(np.reshape(train_Y[test_index], (-1)), y_valid_pred)[0])\n",
    "\n",
    "print(Pearson)\n",
    "print(MSE)\n",
    "print(R2)\n",
    "\n",
    "np.save(join(\"..\", \"..\", \"data\", \"training_results\", \"Pearson_CV_xgboost_ESM1b_ts_diff_fp.npy\"), np.array(Pearson))\n",
    "np.save(join(\"..\", \"..\", \"data\", \"training_results\", \"MSE_CV_xgboost_ESM1b_ts_diff_fp.npy\"), np.array(MSE))\n",
    "np.save(join(\"..\", \"..\", \"data\", \"training_results\", \"R2_CV_xgboost_ESM1b_ts_diff_fp.npy\"), np.array(R2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.629 0.868 0.394\n"
     ]
    }
   ],
   "source": [
    "dtrain = xgb.DMatrix(train_X, label = train_Y)\n",
    "dtest = xgb.DMatrix(test_X, label = test_Y)\n",
    "\n",
    "\n",
    "bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "\n",
    "y_test_pred = bst.predict(dtest)\n",
    "MSE_dif_fp_test = np.mean(abs(np.reshape(test_Y, (-1)) - y_test_pred)**2)\n",
    "R2_dif_fp_test = r2_score(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "Pearson = stats.pearsonr(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "\n",
    "print(np.round(Pearson[0],3) ,np.round(MSE_dif_fp_test,3), np.round(R2_dif_fp_test,3))\n",
    "\n",
    "\n",
    "np.save(join(\"..\", \"..\", \"data\", \"training_results\", \"y_test_pred_xgboost_ESM1b_ts_diff_fp.npy\"), bst.predict(dtest))\n",
    "np.save(join(\"..\", \"..\", \"data\", \"training_results\", \"y_test_true_xgboost_ESM1b_ts_diff_fp.npy\"), test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred_esm1b_ts_drfp = y_test_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (d) Training model with test and train data for production mode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = np.array(list(data_train[\"difference_fp\"]))\n",
    "train_X = np.concatenate([train_X, np.array(list(data_train[\"ESM1b_ts\"]))], axis = 1)\n",
    "train_Y = np.array(list(data_train[\"log10_kcat\"]))\n",
    "\n",
    "test_X = np.array(list(data_test[\"difference_fp\"]))\n",
    "test_X = np.concatenate([test_X, np.array(list(data_test[\"ESM1b_ts\"]))], axis = 1)\n",
    "test_Y = np.array(list(data_test[\"log10_kcat\"]))\n",
    "\n",
    "train_X = np.concatenate([train_X, test_X])\n",
    "train_Y = np.concatenate([train_Y, test_Y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.9906766451895581, 0.0) 0.02970620593719598 0.9792781634215774\n"
     ]
    }
   ],
   "source": [
    "param = {'learning_rate': 0.5727401435817077, \n",
    "         'max_delta_step': 0.022572978136953803,\n",
    "         'max_depth': 9.734956573895278,\n",
    "         'min_child_weight': 2.026404280518698,\n",
    "         'num_rounds': 259.69265795096726,\n",
    "         'reg_alpha': 7.333074414515098,\n",
    "         'reg_lambda': 0.8545111451043885}\n",
    "\n",
    "num_round = param[\"num_rounds\"]\n",
    "param[\"max_depth\"] = int(np.round(param[\"max_depth\"]))\n",
    "\n",
    "del param[\"num_rounds\"]\n",
    "\n",
    "dtrain = xgb.DMatrix(train_X, label = train_Y)\n",
    "dtest = xgb.DMatrix(test_X)\n",
    "\n",
    "bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "\n",
    "y_test_pred = bst.predict(dtest)\n",
    "MSE_dif_fp_test = np.mean(abs(np.reshape(test_Y, (-1)) - y_test_pred)**2)\n",
    "R2_dif_fp_test = r2_score(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "Pearson = stats.pearsonr(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "\n",
    "print(Pearson, MSE_dif_fp_test, R2_dif_fp_test)\n",
    "\n",
    "pickle.dump(bst, open(join(\"..\", \"..\", \"data\", \"training_results\", \"saved_models\",\n",
    "                          \"xgboost_train_and_test.pkl\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model with enzyme and reaction information (ESM1b_ts/DRFP [mean]):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6427432566631278, 0.6226108619000825, 0.6371590251884234, 0.6630733248247811, 0.5920048794950177]\n",
      "[0.7840422477361155, 0.7768955691409662, 0.8045088804939567, 0.9293225562214953, 0.9845290313461734]\n",
      "[0.40333530789389016, 0.38353482393964156, 0.39189245852320176, 0.42044564365386916, 0.3488696368237689]\n"
     ]
    }
   ],
   "source": [
    "R2 = []\n",
    "MSE = []\n",
    "Pearson = []\n",
    "\n",
    "for i in range(5):\n",
    "    train_index, test_index  = train_indices[i], test_indices[i]\n",
    "    y_valid_pred = np.mean([y_valid_pred_DRFP[i], y_valid_pred_esm1b_ts[i]], axis =0)\n",
    "    MSE.append(np.mean(abs(np.reshape(train_Y[test_index], (-1)) - y_valid_pred)**2))\n",
    "    R2.append(r2_score(np.reshape(train_Y[test_index], (-1)), y_valid_pred))\n",
    "    Pearson.append(stats.pearsonr(np.reshape(train_Y[test_index], (-1)), y_valid_pred)[0])\n",
    "\n",
    "print(Pearson)\n",
    "print(MSE)\n",
    "print(R2)\n",
    "\n",
    "np.save(join(\"..\", \"..\", \"data\", \"training_results\", \"Pearson_CV_xgboost_ESM1b_ts_DRFP_mean.npy\"), np.array(Pearson))\n",
    "np.save(join(\"..\", \"..\", \"data\", \"training_results\", \"MSE_CV_xgboost_ESM1b_ts_DRFP_mean.npy\"), np.array(MSE))\n",
    "np.save(join(\"..\", \"..\", \"data\", \"training_results\", \"R2_CV_xgboost_ESM1b_ts_DRFP_mean.npy\"), np.array(R2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation on test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.671 0.808 0.436\n"
     ]
    }
   ],
   "source": [
    "y_test_pred = np.mean([y_test_pred_drfp, y_test_pred_esm1b_ts], axis =0)\n",
    "\n",
    "MSE_dif_fp_test = np.mean(abs(np.reshape(test_Y, (-1)) - y_test_pred)**2)\n",
    "R2_dif_fp_test = r2_score(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "Pearson = stats.pearsonr(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "print(np.round(Pearson[0],3) ,np.round(MSE_dif_fp_test,3), np.round(R2_dif_fp_test,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(join(\"..\", \"..\", \"data\", \"training_results\", \"y_test_pred_xgboost_ESM1b_ts_DRFP_mean.npy\"), y_test_pred)\n",
    "np.save(join(\"..\", \"..\", \"data\", \"training_results\", \"y_test_true_xgboost_ESM1b_ts_DRFP_mean.npy\"), test_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Training a model with enzyme rep., reaction information, and additional features (ESM1b_ts/diff_fp/flux/KM):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading dataset with additonal features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3421, 850)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train = pd.read_pickle(join(\"..\", \"..\", \"data\", \"kcat_data\", \"splits\", \"train_df_kcat_with_KM_and_flux.pkl\"))\n",
    "data_test = pd.read_pickle(join(\"..\", \"..\", \"data\", \"kcat_data\", \"splits\", \"test_df_kcat_with_KM_and_flux.pkl\"))\n",
    "\n",
    "data_train.rename(columns = {\"geomean_kcat\" :\"log10_kcat\"}, inplace = True)\n",
    "data_test.rename(columns = {\"geomean_kcat\" :\"log10_kcat\"}, inplace = True)\n",
    "len(data_train), len(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = ();\n",
    "train_X = np.array(list(data_train[\"DRFP\"]))\n",
    "train_X = np.concatenate([train_X, np.array(list(data_train[\"ESM1b_ts\"])),\n",
    "                          np.reshape(np.array(list(data_train[\"KM\"])), (-1,1)),\n",
    "                          np.reshape(np.array(list(data_train[\"flux\"])), (-1,1))], axis = 1)\n",
    "train_Y = np.array(list(data_train[\"log10_kcat\"]))\n",
    "\n",
    "test_X = ();\n",
    "test_X = np.array(list(data_test[\"DRFP\"]))\n",
    "test_X = np.concatenate([test_X, np.array(list(data_test[\"ESM1b_ts\"])),\n",
    "                          np.reshape(np.array(list(data_test[\"KM\"])), (-1,1)),\n",
    "                          np.reshape(np.array(list(data_test[\"flux\"])), (-1,1))], axis = 1)\n",
    "test_Y = np.array(list(data_test[\"log10_kcat\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''def cross_validation_mse_gradient_boosting(param):\n",
    "    num_round = param[\"num_rounds\"]\n",
    "    del param[\"num_rounds\"]\n",
    "    param[\"max_depth\"] = int(np.round(param[\"max_depth\"]))\n",
    "    param[\"tree_method\"] = \"gpu_hist\"\n",
    "    param[\"sampling_method\"] = \"gradient_based\"\n",
    "    \n",
    "    MSE = []\n",
    "    R2 = []\n",
    "    for i in range(5):\n",
    "        train_index, test_index  = train_indices[i], test_indices[i]\n",
    "        dtrain = xgb.DMatrix(train_X[train_index], label = train_Y[train_index])\n",
    "        dvalid = xgb.DMatrix(train_X[test_index])\n",
    "        bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "        y_valid_pred = bst.predict(dvalid)\n",
    "        MSE.append(np.mean(abs(np.reshape(train_Y[test_index], (-1)) - y_valid_pred)**2))\n",
    "        R2.append(r2_score(np.reshape(train_Y[test_index], (-1)),  y_valid_pred))\n",
    "    return(-np.mean(R2))\n",
    "\n",
    "\n",
    "from hyperopt import fmin, tpe, rand, hp, Trials\n",
    "\n",
    "space_gradient_boosting = {\n",
    "    \"learning_rate\": hp.uniform(\"learning_rate\", 0.01, 1),\n",
    "    \"max_depth\": hp.uniform(\"max_depth\", 4,12),\n",
    "    #\"subsample\": hp.uniform(\"subsample\", 0.7, 1),\n",
    "    \"reg_lambda\": hp.uniform(\"reg_lambda\", 0, 5),\n",
    "    \"reg_alpha\": hp.uniform(\"reg_alpha\", 0, 5),\n",
    "    \"max_delta_step\": hp.uniform(\"max_delta_step\", 0, 5),\n",
    "    \"min_child_weight\": hp.uniform(\"min_child_weight\", 0.1, 15),\n",
    "    \"num_rounds\":  hp.uniform(\"num_rounds\", 20, 200)}\n",
    "\n",
    "\n",
    "trials = Trials()\n",
    "best = fmin(fn = cross_validation_mse_gradient_boosting, space = space_gradient_boosting,\n",
    "            algo=rand.suggest, max_evals = 200, trials=trials)''';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {'learning_rate': 0.15055870206296312,\n",
    "         'max_delta_step': 0.13821534396910307,\n",
    "         'max_depth': 5.338955142881738,\n",
    "         'min_child_weight': 14.84613730467497,\n",
    "         'num_rounds': 294.13028718637383,\n",
    "         'reg_alpha': 2.6752278199969153,\n",
    "         'reg_lambda': 0.6063171152564584}\n",
    "\n",
    "num_round = param[\"num_rounds\"]\n",
    "param[\"max_depth\"] = int(np.round(param[\"max_depth\"]))\n",
    "\n",
    "del param[\"num_rounds\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R2 = []\n",
    "MSE = []\n",
    "Pearson = []\n",
    "\n",
    "for i in range(5):\n",
    "    train_index, test_index  = train_indices[i], test_indices[i]\n",
    "    dtrain = xgb.DMatrix(train_X[train_index], label = train_Y[train_index])\n",
    "    dvalid = xgb.DMatrix(train_X[test_index])\n",
    "    \n",
    "    bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "    \n",
    "    y_valid_pred = bst.predict(dvalid)\n",
    "    MSE.append(np.mean(abs(np.reshape(train_Y[test_index], (-1)) - y_valid_pred)**2))\n",
    "    R2.append(r2_score(np.reshape(train_Y[test_index], (-1)), y_valid_pred))\n",
    "    Pearson.append(stats.pearsonr(np.reshape(train_Y[test_index], (-1)), y_valid_pred)[0])\n",
    "\n",
    "print(Pearson)\n",
    "print(MSE)\n",
    "print(R2)\n",
    "\n",
    "np.save(join(\"..\", \"..\", \"data\", \"training_results\", \"Pearson_CV_xgboost_ESM1b_diff_fp_flux_KM.npy\"), np.array(Pearson))\n",
    "np.save(join(\"..\", \"..\", \"data\", \"training_results\", \"MSE_CV_xgboost_ESM1b_diff_fp_flux_KM.npy\"), np.array(MSE))\n",
    "np.save(join(\"..\", \"..\", \"data\", \"training_results\", \"R2_CV_xgboost_ESM1b_diff_fp_flux_KM.npy\"), np.array(R2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(train_X, label = train_Y)\n",
    "dtest = xgb.DMatrix(test_X)\n",
    "\n",
    "bst = xgb.train(param, dtrain, int(num_round), verbose_eval=False)\n",
    "\n",
    "y_test_pred = bst.predict(dtest)\n",
    "MSE_dif_fp_test = np.mean(abs(np.reshape(test_Y, (-1)) - y_test_pred)**2)\n",
    "R2_dif_fp_test = r2_score(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "Pearson = stats.pearsonr(np.reshape(test_Y, (-1)), y_test_pred)\n",
    "\n",
    "print(Pearson[0], MSE_dif_fp_test, R2_dif_fp_test)\n",
    "\n",
    "\n",
    "np.save(join(\"..\", \"..\", \"data\", \"training_results\", \"y_test_pred_xgboost_ESM1b_diff_fp_flux_KM.npy\"), bst.predict(dtest))\n",
    "np.save(join(\"..\", \"..\", \"data\", \"training_results\", \"y_test_true_xgboost_ESM1b_diff_fp_flux_KM.npy\"), test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
